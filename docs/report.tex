% XELATEX
\documentclass[12pt]{report}
\usepackage{multicol}

\usepackage{geometry}

\let\keptmaketitle\maketitle %<------------ADDED THIS LINE
\usepackage[hyphens]{url}
% \usepackage[ocgcolorlinks]{hyperref}
\usepackage{hyperref}
\hypersetup{
  colorlinks,
  citecolor=blue,
  filecolor=blue,
  linkcolor=blue,
  urlcolor=blue
}

\usepackage[numbers,sort&compress]{natbib}
\usepackage{geometry}
\usepackage{subcaption}
%\geometry{left=0.5in, right=0.5in, top=0.5in, bottom=0.5in}
\geometry{left=1.5in, right=1.5in, top=1.5in, bottom=1.5in}
\usepackage{fourier}
\DeclareMathAlphabet{\mathcal}{OT1}{pzc}{m}{it}
\DeclareSymbolFont{letters}{OML}{cmm}{m}{it}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\usepackage{amsfonts, amssymb, bm, amsmath}
\usepackage{booktabs}  % for \toprule ...
\usepackage{caption}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage{graphicx, type1cm, lettrine}
\usepackage{qtree}
\usepackage[dvipsnames, table]{xcolor}
\usepackage{collcell}
\usepackage{pgf}
\usepackage{forest}
\usepackage{setspace}
\usepackage{fontspec}
\usepackage{polyglossia} % <3
\setmainlanguage{english}
\setotherlanguage{arabic}


%\usepackage{times} % for Sans Serif
% for IPA
\usepackage{tipa}
\usepackage{pgfplots}
\definecolor{myBlue}{HTML}{7982db}
\definecolor{myGreen}{HTML}{ABDDA4}





% Arabic Fonts
%\newfontfamily\arabicfont[Script=Arabic,Scale=1.3]{Scheherazade}
\newfontfamily\arabicfont[Script=Arabic]{Amiri}

% This font is not found on my machine
%\setmainfont[Scale=0.925]{Heuristica}

\usepackage{bidipoem}

\tikzset{
  Above/.style={
    midway,
    above,
    font=\scriptsize,
    text width=1.5cm,
    align=center,
  },
  Below/.style={
    midway,
    below,
    font=\scriptsize,
    text width=1.5cm,
    align=center
  }
}

%% Confusion Matrix Settings

\newcommand\gray{gray}

\newcommand*{\woB}[1]{\multicolumn{1}{c}{#1}}
\newcommand\ColCell[1]{%
  \pgfmathparse{#1 <.8 ? 1 : 0}%
    \ifnum\pgfmathresult=0\relax\color{white}\fi
  \pgfmathparse{1-#1}%
  \expandafter\cellcolor\expandafter[%
    \expandafter\gray\expandafter]\expandafter{\pgfmathresult}#1}

\newcolumntype{E}{>{\collectcell\ColCell}c<{\endcollectcell}}





%% Drawing
\usepackage{tikz}
\usetikzlibrary{arrows} % For arrows :"D



%\usepackage{float}

% \newfontfamily\arabicfont[Script=Arabic,Scale=1.1]{Amiri}


%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Citation Style
% \bibliographystyle{IEEEtran}
\bibliographystyle{plainnat}
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\renewcommand{\thesection}{\Roman{section}} 
\renewcommand{\thesubsection}{\thesection.\Roman{subsection}}

\let\maketitle\keptmaketitle %<------------ADDED THIS LINE
\definecolor{new_blue}{HTML}{517EB9}
%%%%%%%%%%%%%%%%%% 
\begin{document}

%\title{Learning meters of Arabic and English poems with Recurrent Neural Networks}
%\author{Yousef. W.A}
%
%%%%%%% 
%\maketitle 




\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%-----------------------
%	HEADING SECTIONS
%-----------------------

\textsc{\Large \textarabic{لا غالب إلا الله}}\\[1.9cm] % Major heading such as course name
\textsc{\LARGE Helwan University}\\[1.5cm] % Name of your university/college

%-----------------------
%	TITLE SECTION
%-----------------------

\HRule \\[.4cm]
{ \LARGE \bfseries Learning meters of Arabic and English poems with recurrent
neural networks}\\[0.4cm] % Title of your document
\HRule \\[3cm]
 
%-----------------------
%	AUTHOR SECTION
%-----------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\textit{\Large Members,} \textit{\small alphabetically}:\\

Abdaullah Ramzy\\
Ali Abdemoniem\\
Ali Osama\\
Taha Magdy\\
Umar Mohamed\\
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\textit{\Large Supervisor:} \\
Prof. Waleed A.\textsc{Yousuf} % Supervisor's Name
\end{flushright}
\end{minipage}\\[4cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%-----------------------
%	DATE SECTION
%-----------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

%-----------------------
%	LOGO SECTION
%-----------------------
%\includegraphics[width=30mm,scale=0.5]{logo.png}\\[0.5cm] % Include a department/university logo - this will require the graphicx package

\vfill % Fill the rest of the page with whitespace

\end{titlepage}









%\renewcommand\IEEEkeywordsname{Keywords}
%\begin{IEEEkeywords}
%\ Poetry, Meters, Arabic, English, RNN, Deep Learning, Classification,
%\ Text Mining, Arud.
%\end{IEEEkeywords}

\section*{Abstract}
%% Should be revised after finishing this papers.

% 1. Opening appetite 
People can easily determine whether a piece of writing is
a poem or prose, but only specialists can determine which 
meter a poem is belonged to. 
% What we want to build
In the present paper, we build a model that can classify poems
according to their meters; a forward step towards machine understanding of Arabic language.

% 2. comprehensive introduction
A number of different \textit{deep learning} models are proposed for poem meter
classification. As poems are sequence data, then \textit{recurrent neural
networks} are suitable for the task.  We have trained three variants of them,
\textit{LSTM}, \textit{GRU} and \textit{Bi-LSTM} with different architectures.
% 3. character-level inputs, Why!
Because meters are nothing but sequences of characters,
then we have encoded the input text at the \textit{character-level},
% Question; what is thep?!
so that we preserve the information provided by the letters succession directly fed to thep models. In addition,
we introduce a comparative study on the difference between binary and one-hot encoding
in terms of their effect on the learning curve. 
We also introduce a new encoding technique called \textit{Two-Hot} which 
merges the advantages of both \textit{Binary} and \textit{One-Hot} techniques.


% 4. Motivating Questions
% Now, we are trying to answer this question, \textit{can
% computers learn to classify poems according to their meters?}

% Resaults

% \makebox[\textwidth][s]{This is a text that almost fills a complete printed line.}




\section{Introduction and Problem Statement}
Detecting the meter of poems is not an easy task for ordinary people, but how
computers will perform? Our task is to train a model so that it can detect the
meter of the input verse/text.
We have worked on Arabic and English in parallel, everything thing is applied to
Arabic is applied also in English, as possible as we can.

To be clearer, the model's input is a verse/text \textarabic{بيت شعر} and the
output is a class which is the verse's meter \textarabic{البحر}, as shown in the
figure below.


% FIGURE
\begin{center}
\begin{tikzpicture}
\centering

%\draw[step=0.5, gray, very thin] (0,0) grid (6,2);
\draw[rounded corners=2pt, thick] (2,0) rectangle (2+2,2);
\node at (3, 1.5) {Deep};
\node at (3, 1) {Learning};
\node at (3, 0.5) {Model};

\draw[arrows=-angle 90, line width=1pt ] (1, 1) -- (1 +1 -0.1, 1);
\node at (0, 1.5) {Verse};
\node at (0, 1) {\textarabic{بيتُ الشعر}};

\draw[arrows=-angle 90, line width=1pt] (4 +.1, 1) -- (4 +1, 1);
\node at (6, 1.5) {Meter};
\node at (6, 1) {\textarabic{البحْرُ}};

\end{tikzpicture}
\end{center}

The output variable is a class/categorical, then our problem can be described as
\textit{supervised learning  classification}.  We have trained some deep learning
models such as LSTM, Bi-LSTM and GRU.  Those models are chosen because of the
nature of our problem. We were trying to detect the verse's meter, which is a
sequence of characters and \textit{recurrent neural network} are suitable  to
learn that pattern, thanks to its cell's share-memory and its recursive
structure.


\section{The Project Road Map}
This is are four-stage project. 

\begin{enumerate}
    \item The first one is where we get the raw data and put it in a
            feature-response form.
    \item The second stage is cleaning the data, by which we mean that any
            non-letter character and unnecessary white-spaces are removed, also, handling any
            diacritics issue, as it will be demonstrated in the next sections, this stage
            include encoding the data to the form that is suitable to be fed to our neural
            networks.
    \item The third stage is where the models are built and are tuned then we
            have automated the experiments to run all the iterations one after another, much
            details will be presented.
    \item Finally, we gather the results and analyse them, also we then conduct
            some additional experiments to see the language and encoding effect over the
            learning curve.
\end{enumerate}

 

The first two steps were much difficult than
building the models. The following figure shows the road map.

\begin{center}
\begin{tikzpicture}
\centering

%\draw[step=0.5, gray, very thin] (-6,0) grid (6.5,2);

\draw[rounded corners=2pt, thick] (-6, 0) rectangle (-6 +2,2);
\draw[rounded corners=2pt, thick] (-2.5, 0) rectangle (-2.5 +2,2);
\draw[rounded corners=2pt, line width=2pt] (1, 0) rectangle (1 +2,2);
\draw[rounded corners=2pt, thick] (4.5, 0) rectangle (4.5 +2,2);


\node at (2, 1.2) {Building};
\node at (2, .7) {Models};

\node at (-1.5, 1.2) {Cleaning};
\node at (-1.5, .7) {Data};

\node at (-5, 1.5) {Scraping};
\node at (-5, 1) {The};
\node at (-5, .5) {Web};
\node at (-5, -.5) {\small \textit{Gathering Data}};

\node at (5.5, 1.2) {Analysing};
\node at (5.5, .7) {Results};

\def \littelM {0.1}
\draw[arrows=-angle 90, line width=1pt] (-6 +2 +\littelM, 1) -- (-2.5 -\littelM, 1);
\draw[arrows=-angle 90, line width=1pt] (-2.5 +2 +\littelM, 1) -- (1 -\littelM, 1);
\draw[arrows=-angle 90, line width=1pt] ( 1 +2 +\littelM, 1) -- (4.5 -\littelM, 1);

\end{tikzpicture}
\end{center}


\section{Tools}
Python is pseudo-code like programming language, it is so easy and high-level
that we can describe complex structures in a few lines of code, the main second
reason is that python recently has been so papular in the Artificial Intelligence
community. Its library is so rich with packages for Machine Learning, Deep
Learning, data manipulation, even for web-scraping; we don't need to parse HTML
by your hands.

We have used:
Two columns:
    \begin{multicols}{2}
\begin{itemize}
\item \textit{Python} 3.6.5
\item \textit{Keras} x.x for deep learning.
\item \textit{Tensorflow} x.x as back-end of Keras.
\item \textit{BeautifulSoup} for web scraping.
\end{itemize}
    \end{multicols}







\section{Scraping The Web \textit{\small --gathering the data}}
To train our models we need dataset of poems. For Arabic, this kind of dataset
are not so popular so you can hardly find a dataset for poetry. So, we had to
create our own. During our search we have found two big poetry web sites
\textarabic{الديوان}\footnote{\textit{aldiwan.net}}, \textarabic{الموسوعة
الشعرية}\footnote{\textit{poetry.tcaabudhabi.ae}}, that
contain tons of metrical-classified poems. And similar is happened for English.
So we have scrapped those web sites. \textit{Scraping} is to write a script to
browse the target web site, and copy the specific content and dump it to our
\textit{csv} file. This was the most difficult step in the hole project. 
 
Also, we present a very large Arabic dataset that can be used for further
research purposes, next sub-section contains much details about the Arabic
dataset. For the English dataset, the situation was worse than scraping big
complex web site, because the there were not big web site that contain a large
amount of metrical-classified poems, so the English dataset is to small if it is
compared to its Arabic counterpart, but this what we have found after an
extensive search. 
% NOTE:
cite(Farren) had been generously granted his data from the
Stanford English literature department, we have tried to contact them through
their email but none has reposed.

The scraping scripts are under the directory \texttt{repository\_root/scraping
the web}, accompanied by  a thorough self-contained \texttt{README.md} file,
which contains everything you may need to use or re-use the code, even the code
is written to be re-used with a lot of  \textit{in-line} documentation.























\section{Introduction}
%% 1* Introduce Arabic poems
% answers “why?”
%% الخليل بن أحمد صنف الشعر
\subsection{Arabic Language}
%% Arabic Introduction

% Hello, Arabic
% \lettrine[lines=3, findent=3pt, nindent=0pt]{A}{}
Arabic is the fifth most widely spoken language\footnote{\textit{according to the
20th edition of Ethnologue, 2017}}. It is written from right to left. Its
alphabet consists of 28 primary letters, and there are 8 more derived letters
from the basic ones, so the total count of Arabic characters is 36 characters.
The writing system is cursive; hence, most letters join to the letter that comes
after them, a few letters remain disjoint.

% Introduction to diacritics: ◌
Each Arabic letter represents a consonant, which means that short vowels are not
represented by the 36 characters, for this reason, the need of \textit{diacritics}
rises. \textit{Diacritics} are symbols that comes after a letter to state the
short vowel accompanied by that letter. There are four diacritics \textarabic{◌َ} \textarabic{◌ُ}
\textarabic{◌ِ} \textarabic{◌ْ} which represent the following short vowels
/\textit{a}/, /\textit{u}/, /\textit{i}/ and \textit{no-vowel} respectively,
their names are \textit{fat-ha, dam-ma, kas-ra and sukun} respectively.  The first
three symbols are called \textit{harakat}. Table \ref{arabic:diacritics_dal}
shows the 4 diacritics on a letter. 
%
There are two more sub-diacritics made up of the basic four to represent two
cases: the first case is when a letter is doubled where the first letter is
accompanied by \textit{sukun} and the second letter is accompanied by
\textit{haraka}, instead of doubling each letter accompanied by their
\textit{diacritics} explicitly, both of them are written once accompanied by
\textit{shadda} \textarabic{◌ّ}, \textit{shadda} states that its letter is
doubled, the first letter is always accompanied by \textit{sukun}, the second
letter's \textit{haraka} is followed by \textit{shadda}; for example,
\textarabic{دْدَ} is written \textarabic{دَّ}.
% the issue
The second case, Arabs pronounce the sound \textit{/n/} accompanied
\textit{sukun} at the end the indefinite words, that sound corresponds to this
letter \textarabic{نْ}, it is called \textit{noon-sakinah}, however, it is
just a phone, it is not a part of the indefinite word, if a word comes as a
definite word, no additional sound is added. Since it is not an essential sound,
it is not written as a letter, but it is written as  \textit{tanween}
\textarabic{◌ٌ ◌ً ◌ٍ}. 
% adding tanween and its relationship to the previous letter
\textit{Tanween} states the sound \textit{noon-sakinah}, but as you have noticed,
there are 3 \textit{tanween} symbols, this because  \textit{tanween} is added as
a diacritic over the last letter of the indefinite word, last letter is
accompanied by one of the 3 \textit{harakat}, the last letter's \textit{harakah}
needs to be stated in addition to the sound \textit{noon-sakinah}, so
\textit{tanween} is doubling the last letter's \textit{haraka}, this way the last
letter's \textit{haraka} is preserved in addition to stating the sound
\textit{noon-sakinah}; for example, \textarabic{رَجُلُ + نْ} is written
\textarabic{رَجُلٌ} and  \textarabic{رَجُلِ + نْ} is written \textarabic{رَجُلٍ}.
%
Those two cases will help us to reduce the dimension of the letter's feature vector as we
will see in \textit{preparing data} section.


% table: dal with diacritics
\begin{table}[!t]
  \centering
  \begin{tabular}{c c c c c c} 
    %\hline
    \toprule
    \textbf{\small{Diacritics}}     & \small{\textit{without}} & \small{\textit{fat-ha}} &
            \small{\textit{kas-ra}} & \small{\textit{dam-ma}} & \small{\textit{sukun}}\\
    %\hline
    \midrule
    \textbf{\small{Shape}}   & \textarabic{د} & \textarabic{دَ} & \textarabic{دِ} &
            \textarabic{دُ} & \textarabic{دْ}\\
    %\hline
    \bottomrule
  \end{tabular}
  \caption{\textit{Diacritics on the letter \textarabic{د}}}\label{arabic:diacritics_dal}
\end{table}

Diacritics are just to make short vowels clearer, but they are not necessary.
Moreover, a phrase without full diacritics or with just some on some letters is
right linguistically, so it is allowed to drop them from text.

% Diacritics in Unicode
In Unicode, Arabic diacritics are standalone symbols, each of them has its own
unicode. This is in contrast to the Latin diacritics; e.g., in the set
\textit{\{ê, é, è, ë, ē, ĕ, ě\}}, each combination of the letter \textit{e} and a diacritic is represented by one unicode.


\subsection{Arabic Poetry \textarabic{الشعر العربى}}
%% Introduction; the circumstances before alarud
Arabic poetry is the earliest form of Arabic literature. It dates back to the sixth century. Poets have written poems without knowing exactly what rules which make a collection of words a poem. People recognize poetry by nature, but only talented ones who could write poems. %
% What al-Farahidi did 
This was the case until \textit{Al-Farahidi} (718 – 786 CE) has analyzed the
Arabic poetry, then he came up with that the succession of consonants and vowels
produce patterns or \textit{meters}, which make the music of poetry.  He has
counted them fifteen meters.  After that, a student of \textit{Al-Farahidi} has
added one more meter to make them sixteen. Arabs call meters \textarabic{بحور}
which means "\textit{seas}".

\bigskip

% \textbf{Some conventions and terminologies}:
A poem is a collection of verses, a verse looks like the following:%
% What are poems and terminologies?
% What does a poem look like? bayt, shatr, ....

\begin{Arabic}
  \begin{traditionalpoem*}
% ويوم موعدهم أن يُحشروا زُمرا يوم التغابن إذ لا ينفع الحذرُ
% والأرض مملوءة جورا مسخرة ... لكل طاغية في الخلق محتكم

    ألا قاتل الله الحمامة غدوةً \quad & \quad على الأيكِ ماذا هيّجتْ حين غنّتِ
    تغَنّت بصوت أعجمىّ فهيجت  \quad & \quad من الوَجد ما كانت ضُلوعى أجنّتِ
  \end{traditionalpoem*}
\end{Arabic}%

A verse, known as \textit{bayt} in Arabic, consists of two halves; a half is
called a \textit{shatr}\footnote{it is a singular in arabic, but for simplicity
we will use it for both singular and plural.}. %
% Meters and Feet
\textit{Al-Farahidi} has introduced \textit{al-'arud
\textarabic{العروض}}\footnote{it is often called the \textit{Knowledge of
Poetry}.}; it is the study of poetic meters, in which he has laid down rigorous
rules and measures, with them we can determine whether a meter of a poem is sound
or broken. A meter is an ordered sequence of \textit{feet}. Feet are the basic
units of meters, there are eight of them. A Foot consists of a sequence of
consonant and vowels. Traditionally, feet are represented by mnemonic words
called \textit{tafa'il} (\textarabic{تفاعيل}).  According to \textit{al-Farahidi}
and his student, there are sixteen combinations of \textit{tafa'il}. A meter
appears in a \textit{verse} twice; each \textit{shatr} carries the same complete
meter.%

% NOTE: transpose it. \small{}
\begin{table}[!t]
  \centering
  \begin{tabular}{|c|c|} 
    \hline
    \textbf{Feet} & \textbf{Scansion} \\ 
    \hline
    \textarabic{فَعُولُنْ}  & \texttt{0/0//}\\
    \textarabic{فَاعِلُنْ}  & \texttt{0//0/}\\
    \textarabic{مُسْتَفْعِلُنْ}& \texttt{0//0/0/}\\
    \textarabic{مَفاعِيلُنْ}& \texttt{0/0/0//}\\
    \textarabic{مَفْعُولاَت} & \texttt{0//0///}\\
    \textarabic{فَاعِلاَتُنْ} & \texttt{0/0//0/}\\
    \textarabic{مُفَاعَلَتُنْ}& \texttt{0///0//}\\
    \textarabic{مُتَفَاعِلُنْ}& \texttt{0//0///}\\
    \hline
  \end{tabular}
  \caption{The eight feet. Every digit represents the corresponding diacritic
over each latter in the feet. \texttt{/} If a letter has got \textit{harakat} (
\textarabic{◌َ} \textarabic{◌ُ} \textarabic{◌ِ}), \texttt{0} if a letter has got
\textit{sukun} (\textarabic{◌ْ}). Any \textit{mad} (\textarabic{و, ا, ى}) is
equivalent to \textit{sukun}.}\label{arud:feet}
\end{table}


For example, the following \textit{shatr} \textarabic{وَيُسأَلُ في الحَوادِثِ ذو صَوابٍ} is
equivalent to the \textit{meter} \textarabic{مفاعلتن مفاعلتن فعول}, which means
it belongs to \textit{Al-Wafeer} meter. We can get the pattern of the
\textit{sukun} and \textit{harakat} by replacing each feet by the corresponding
code in table \ref{arud:feet}, which produces the following pattern that should
be read from right to left:%
\begin{flushright}
  {\texttt{0/0// 0///0// 0///0//}} % You have to filp it.
\end{flushright}
This is a very brief introduction to \textit{Arud}, many details are reduced.

 


\begin{center}
  \begin{tabular}[h!]{|c|c|} 
    \hline
    \textbf{Meter Name} & \textbf{Meter} \small{\textit{feet combination}} \\ 
    \hline
   \textit{al-Wafeer}    & \textarabic{مُفَاعَلَتُن مُفَاعَلَتُن فَعُولُن} \\ %
   \textit{al-Taweel}    & \textarabic{فَعُوْلُنْ مَفَاْعِيْلُنْ فَعُوْلُنْ مَفَاْعِلُنْ} \\ %
   \textit{al-Kamel}     & \textarabic{مُتَفَاْعِلُنْ مُتَفَاْعِلُنْ مُتَفَاْعِلُنْ} \\%
   \textit{al-Baseet}    & \textarabic{مُسْتَفْعِلُنْ فَاْعِلُنْ مُسْتَفْعِلُنْ فَاْعِلُنْ} \\%
   \textit{al-Khafeef}   & \textarabic{فَاْعِلاتُنْ مُسْتَفْعِلُنْ فَاْعِلاتُنْ} \\ %
   \textit{al-Rigz}      & \textarabic{مُسْتَفْعِلُنْ مُسْتَفْعِلُنْ مُسْتَفْعِلُنْ} \\%
   \textit{al-Raml}      & \textarabic{فَاْعِلاتُنْ فَاْعِلاتُنْ فَاْعِلاتُنْ} \\ %
   \textit{al-Motakarib} & \textarabic{فَعُوْلُنْ فَعُوْلُنْ فَعُوْلُنْ فَعُوْلُنْ} \\%
   \textit{al-Sar'e}     & \textarabic{مُسْتَفْعِلُنْ مُسْتَفْعِلُنْ مَفْعُوْلاتُ} \\%
   \textit{al-Monsafeh}  & \textarabic{مُسْتَفْعِلُنْ مَفْعُوْلاتُ مُسْتَفْعِلُنْ} \\
   \textit{al-Mogtath}   & \textarabic{مُسْتَفْعِلُنْ فَاْعِلاتُنْ فَاْعِلاتُنْ} \\
   \textit{al-Madeed}    & \textarabic{فَاْعِلاتُنْ فَاْعِلُنْ فَاْعِلاتُنْ } \\
   \textit{al-Hazg}      & \textarabic{مَفَاْعِيْلُنْ مَفَاْعِيْلُنْ} \\%
   \textit{al-Motadarik} & \textarabic{فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ} \\%
   \textit{al-Moktadib}  & \textarabic{مَفْعُوْلاتُ مُسْتَفْعِلُنْ مُسْتَفْعِلُن} \\
   \textit{al-Modar'e}   & \textarabic{مَفَاْعِيْلُنْ فَاْعِلاتُنْ مَفَاْعِيْلُنْ} \\
    \hline
  \end{tabular}
  % NOTE: add more expressive caption; demonistrate the combinarions.
  \captionof{table}{\textit{The sixteen Arabic poem meters}}\label{arud:meters}
\end{center}

\subsection{English poetry Introduction}
% Introduction
% English prosody -> Cite the Meters.
% Intro: Add English development -> Middle-English -> Now.
English poetry dates back to the seventh century. At that time, poems were
written in \textit{Anglo-Saxon}, also known as \textit{The Old English}. Many
political changes have influenced the language until it becomes as it is
nowadays. English prosody was not formalized rigorously as a stand-alone
knowledge, but many tools of the \textit{Greek} prosody were borrowed to describe
the English prosody, tools like the Greek meters types which pre-dates the
English language by a long time.

% English prosody
A \textit{syllable} is the unit of pronunciation having one vowel sound, with or
without surrounding consonants. English words consist of one or more syllables.
For
% Syllables
% NOTE: smaller forawd slash
\begin{table}[!t]
  \centering
  \begin{tabular}{|c | c|} 
    \hline
    %\toprule
    \textbf{{Feet}}     & \textbf{{Stresses Combination}}\\ 
    \hline
    %\toprule
\textit{Iamb} & $\times$\textit{/}\\             %\midrule
\textit{Trochee}& \textit{/}$\times$\\           %\midrule
\textit{Dactyl} & \textit{/}$\times\times$\\     %\midrule
\textit{Anapest}& $\times\times$\textit{/}\\     %\midrule
\textit{Pyrrhic}& $\times\times$\\               %\midrule
\textit{Amphibrach}& $\times$\textit{/}$\times$\\%\midrule
\textit{Spondee}& \textit{/}\textit{/}\\
    %\bottomrule
    \hline
  \end{tabular}
  \caption{Every foot is a combination of stressed and unstressed syllables,
where stressed syllable is denoted by \textit{/} and unstressed syllable is
denoted by $\times$.}
\label{feet}
\end{table}
example the word "Water" \textipa{\sffamily /"wO:t@/} consists of two phonetic syllables:
\textipa{\sffamily /"wO:/}  
and \textipa{\sffamily /t@(r)/}. As you notice, each syllable have only one vowel
sound.
Syllables can be either stressed or unstressed which are
referred to by \textit{/} and $\times$, respectively. In previous "Water" example, the
first syllable is stressed, stresses are shown using the primary stress symbol
\textipa{\sffamily "} in phonetics, the second syllable is unstressed, so
the word "Water" is a  stressed-unstressed word, which can be denoted by
/$\times$, abstracting the word as stressed and unstressed syllables.
% The seven feet
There are seven different combinations of stressed and unstressed syllables form
make the seven poetic \textit{feet}s.  They are shown in table \ref{feet}.
% Meters
Meters are described as a sequence of feet. English meters are \textit{qualitative}
meters; which are stressed syllables coming at regular intervals.
A meter is repeating one of the previous seven feet one to eight times,
for every verse, then a verse's meter is determined by the repeated foot.
If the foot is repeated once, then verse is \textit{monometer}, if it is
repeated twice  then it is \textit{dimeter} verse, until \textit{octameter} which means
a foot is repeated eight times.  Here is an example, (stressed syllables are bold).
\begin{center}
 That \textbf{time} of \textbf{year} thou \textbf{mayst}  in \textbf{me}
be\textbf{hold}
\end{center}
The first verse belongs to \textit{Iambic} foot and it is repeated five times; so
it is \textit{Iambic pentameter}.





\section{Literature review}
% A little Introduction
% Algorithmic Approaches
% Machine Learning Approaches 


% Classifying meters is addressed differently
% https://english.stackexchange.com/questions/38964/how-to-use-to-v-ing
Classifying and detecting poems problem has been addressed and formalized
differently across the literature. Moreover, the history is so rich of poetry
analysis studies, hundreds of years ago, even before computer appears. However,
the topic is still unexplored, computationally. 


% ## 1* Paper Name: An algorithm for the detection and analysis of arud
\citet{Abuata} present the most related work to our topic. They classify
Arabic poems according to their \textit{meters}.  But they have not addressed it
as a \textit{learning problem}, they have designed a deterministic
five-step \textit{algorithm} for analysing  and detecting meters. The first step
and the most important is having the input text carrying full diacritics, this 
means that every single letter must carry a diacritic, explicitly. The next step
is converting input text into \textit{Arud writing}\footnote{It is a pronounced
version of writing; where only pronounced sounds are written.} using if-else like
rules. Then metrical \textit{scansion} rules are applied to the \textit{Arud writing},
which leaves the input text as a series of zeros and ones. After that each group of zeros and
ones are defined as a \textit{tafa'il} \ref{arud:feet}, so now we have a sequence
of \textit{tafa'il}. And finally  the input text is classified to the closest
meter to the \textit{tafa'il} sequence \ref{arud:meters}.
%%% Results
82.2\% is the classification accuracy on a relatively small sample, only 417 verse.

%% 
% (Mohammad A. Alnagdawi 2013)
\citet{Alnagdawi2013} has taken a similar approach to the previous work,
but they formalized the \textit{scansion}, \textit{Arud} and some
lingual\footnote{like pronounced and silent rules, which is directly related to
\textit{harakat}} rules as
\textit{context-free grammar} and \textit{regular expression} templates, the
result is 75\% correctly classified from 128 verses.

%%%%%%%%%%%%%%%% 
% ## 2* Paper Name: An algorithm for the detection and 
% analysis of arud meter in Diwan poetry
% (Atakan KURT, Mehmet KARA 2010)
\citet{Kurt2012} have worked on detection and analysis of \textit{arud} meter in
Ottoman Language. They have depended on Ottoman \textit{aurd} rules to
construct an algorithm that analyses Ottoman poems. First Ottoman text
must be transliterated to Latin transcription alphabet (LTA) after that text is
fed to the algorithm which uses a database containing all Ottoman meters to
compare the detected meter extracted from LTA to the closest meter found in the
database.

%%%%% 
% My opinion; Criticizing algorithmic approaches.
Both \citet{Abuata} and \citet{Alnagdawi2013} have common problems.  The first
problem is that the test size cannot give an accurate performance for the algorithms
they have constructed, because it is very small. And a 75\% total accuracy of 128
verses is even worse.
The second problem is that the operation of converting verses into zeros and ones
patterns is probabilistic; it also depends on the meaning, which is a source of
randomness.  Then treating such a problem as a deterministic problem is
not going to be satisfying. Moreover, it results in numerous limitations
like obligating verses to have full diacritics on every single letter, before
conducting the classification.


%% Machine Learning Approaches;
% >Recognition of Modern Arabic Poems 
Here is a different approach to the previous, the algorithmic ones,
\citet{Almuhareb2015} has used machine learning to recognize modern Arabic poems
inside documents.  He has built \textit{Naive Bayes} and \textit{Decision Tree}
classifiers which detect poems based on the visual features, like line
length average, line length standard deviation, average number of block\footnote{
\textit{a block: is a group of lines separated by an empty character or more.}}, standard
deviation of block number, word repetition rate, diacritic rate, punctuation
rate. Those features have been extracted from 2067 documents which are
divided into 513 modern poems and 1554 prose.
Then classifiers have been evaluated using \textit{10-fold cross-validation}. 
The best accuracy 99.81\% has been achieved by the \textit{decision tree}
classifier which is trained on all features together.
%%% DONE

\citet{Tizhoosh2006a} has presented similar work to \citet{Almuhareb2015}, have
trained \textit{Naive Bayes} and \textit{Decision Tree}
using visual features, they reached accuracy above 90\%.

% My review
There is a point here, visual features may work when detecting poems inside
documents due to poems are written in specific structure which distinguishes
them from other text inside documents. In theses approaches models have no clue
about the real patterns that create poems, of course the way how words are structured
inside text does not produce a poems, at all.


\citet{Tanasescu2016} has worked on binary classifying English poems where
\textit{metric} and \textit{free-verse} are the categories, he faced an
interesting problem with their dataset, it was imbalanced, (871 metrical poems,
4115 free-verse), for this reason they have used \textit{bootstrap aggregating}
(also known as \textit{bagging}), which is a meta-algorithm that can greatly
improve decision tree accuracy.  With \textit{J48} and \textit{bootstrap
aggregating}, he was able to achieve a 94.39\% correctly classifying poetry as
metrical or not.
%%% END

\subsection*{Encoding the categorical variables and  its impact on neural network performance}
% Abdullah
Encoding features has an impact on the neural network performance.
\citet{Potdar2017} has done a comparative study on six encoding techniques.  We
are interested in the comparison of \textit{one-hot} and \textit{binary}. They
have used Artificial Neural Network for evaluating cars based on seven ordered
qualitative features. The accuracy of the model was the same in both
encodings---\textit{one-hot} and \textit{binary}.


% Motivation Section
\input{MotivationSection}


\section{Datasets}
\subsection{Arabic dataset}
We have scrapped the Arabic dataset from two big poetry websites:
\textarabic{الديوان}\footnote{\textit{aldiwan.net}}, \textarabic{الموسوعة
الشعرية}\footnote{\textit{poetry.tcaabudhabi.ae}}. Both are merged into one large
dataset. It is important to note that the verses' diacritic states are not
consistent, this means that a verse can carry full, semi diacritics or it can
carry nothing. The total number of verses is  1,862,046 poetic verses; each verse is
labeled by its meter, the poet who wrote it, and the
age which it was written in. There are 22 meters, 3701 poets and 11
ages; and they are Pre-Islamic, Islamic, Umayyad, Mamluk, Abbasid, Ayyubid, Ottoman,
Andalusian, era between Umayyad and Abbasid, Fatimid and modern.  We are only
interested  in the 16 classic meters which are attributed to \textit{Al-Farahidi},
and they are the majority of the dataset with a total number of 1,722,321
% NOTE: change the public rep.
verses\footnote{https://wwww.github.com/tahamagdy}.


\begin{figure}
\centering
\input{MeterSizesBarChart}

\caption{Meter names are on the $x$-axis, size  is on the $y$-axis.}
\label{data_size}
\end{figure}


% NOTE: add a line-chart like arabic + percentatge for every class.
\subsection{English dataset}
The English dataset is scraped from many different web
resources\footnote{http://www.eighteenthcenturypoetry.org}. It consists of 199,002
verses, each of them is labeled with one of these four meters: \textit{Iambic},
\textit{Trochee}, \textit{Dactyl} and \textit{Anapaestic}.  The \textit{Iambic}
class dominates the dataset; there are  186,809 \textit{Iambic} verses, 5418
\textit{Trochee} verses, 5378  \textit{Anapaestic} verses, 1397 \textit{Dactyl}
verses.  We have downsampled the \textit{Iambic} class to 5550 verses.

\section{Preparing Data}
% answers “when, where, how, how much?”
% Give question to whet the whet the reader’s appetite to read further and give a taste
% to read the methods. 


\subsection{Data Cleaning}
%%%% Take the common between encoding, with/without are the same length
%%%% ذكر مشكلة ديوان

For both Arabic and English data, they were not clean enough, there were some
non-alphabetical characters and many unnecessary white spaces inside the text,
they have been removed.  For Arabic, there were diacritics mistakes, like the
existence of two consecutive \textit{harakat}, we have only kept one and have
removed the other, or a \textit{haraka} comes after a white space, it has been
removed. 

% Data preparing
As a pre-encoding step, we have factored both of \textit{shadda} and
\textit{tanween} to two letters, as it previously presented in Arabic language
introduction,  by this step we shorten the encoding vector, and save more memory.


\subsection{Data Encoding}
%% Issues:
\citet{Agirrezabal2017} shows that representations of data learned from
character-based neural models are more informative than the ones from
hand-crafted features. 
% Motivation for modeling letters as features.
In our case, we want to detect meters inside verses, meters are nothing but
patterns of the letters successions as previously presented, so it is convenient
to encode verses at \textit{character level} so that our model learns from
sequences of characters.


% General Scenario
Generally, a character will be represented as an $n$ vector. Consequently, a
verse would be an $n \times p$ matrix, where $n$ is the character representation
length  and $p$ is the verse's length,
$n$ varies from one encoding to another, we have used 3 different
techniques to encode the characters.
% An issues
% NOTE: refine it; break it down to smaller parts.
There is an issue; we will take the \textit{one-hot} encoding as an example to
demonstrate it.  A character without diacritics is represented as a $37 \times 1$
vector, where 37 is the 36 letters in addition to a white-space character, so a
phrase like \textarabic{مرحبا} having 5 letters is encoded as a $37 \times 5$
matrix if it came with diacritics it would be represented as $41 \times 1$
vector, where 41 is 37 characters + $4$ diacritics, therefore  a phrase like
\textarabic{مَرْحَبَا} which has 5 letters and 4 diacritics, would be encoded as a
$41 \times 9$ matrix.  The issue comes from considering diacritics as stand-alone
characters while encoding, a phrase with diacritics has more characters than if
it was without diacritics, this leads to different number of time steps in the
\textit{RNN} cell and different input matrix dimensions which in turn leads to
changing the model architecture, we do not want models architectures to differ
whether diacritics exist of not in a verse so that we be able to conduct
fair performance comparisons between models. 
% Solution
As a solution we have encoded both a letter with its diacritic as one character,
there are 36 letters and $36 \times 4$ combinations between each letter and each
diacritic, then we have 181 characters including the white-space. From now
forward, we have a 181-character alphabet, according to it, we will encode verses.

% NOTE: See how to use those two figures
% NOTE: Reform so that you refer to 181-encoding
%       and to refer to the one hot transition from 36-to-180.
% * You may need something else.
So, we used $180 \times 1$ vector to represent a character in order to avoid the
previous two problems, see the upcoming figure  and figure
\ref{fig:one-hot}.



% old idea one hot figure
\input{oldOneHot}

% Let the game begin :)
%% SIDE BY SIDE
\begin{figure}
% One Hot figure
\input{subfigureOneHot}
\hfill
% Binary figure
\input{subfigureBinary}
\end{figure}






% One-Hot encoding figure
\subsubsection{One-Hot encoding}
% 36 Arabic character  + ' ' = 37 char
Here, a character is represented by an $n \times 1$ \textit{one-hot} vector, where
$n$ is the alphabet length, in \textit{Arabic} it is the 181-character alphabet,
and in \textit{English} is the 28-letter alphabet.



% Binary encoding figure
\subsubsection{Binary Encoding}
The idea is to represent a character with an $n
\times 1$ vector which contains a unique combination of ones and zeros.  $n =
\ceil*{\log_2 l}$ where $l$ is the alphabet length, and $n$ is the sufficient
number of digits to represent $l$ unique binary combinations.  For example a phrase
like this \textarabic{مَرْحَبا}, it has 5 characters, figure
\ref{fig:binary} shows how it is encoded as a $8 \times 5$ matrix,
which saves more memory than the \textit{one-hot} and reduces the model capacity,
significantly. But on the other hand, the input characters share some features
between them due to the binary representation as it is shown in figure
\ref{commonFproblem}. We will make a comparison between both
encoding techniques \textit{binary} and \textit{one-hot}, at the end.



% NOTE: put a more expressive caption.
% NOTE; add common features figure.
\begin{center}
\input{commonFeatures}
\label{commonFproblem}
\captionof{figure}{Common feature problem; letters may share some
representation feature.}
\end{center}


% Umar;
It means when characters are fed them to NN, it will compute linear combination,
$Z = XW + B$, and we will multiply the features $X$ by its weights $W$ and apply
activation function then we will compute the cross-entropy loss function and
after computing the gradients using BPTT, the optimizer will update the weights
to reduce the loss so if the optimizer updates weights of this two common
features to reduce error of one character of them it will effect the rest
characters that have the same feature.


% lolo figure
\input{lolo}

\subsubsection{Two-Hot encoding}
This is an intermediate technique which takes the advantages of the previous two
encoding techniques. In which we encode the letter and its diacritic
separately using \textit{one-hot} encoding, this way the letter is encoded as $37
\times 1$ \textit{one-hot} vector and the diacritic is encoded as $4 \times 1$
\textit{one-hot} vector, then both vectors are stacked to form one $41 \times 1$
vector. By this way, we reduces the vector dimension from 180 to 41 and also
minimizes the number of shared features between vectors to maximum 2 ones at each
vector.






\section{Model}
% RNN, LSTM BiLSTM Citation.
Our experiments depend on \textit{LSTM} introduced by \citet{Hochreiter1997}; and
\textit{Bi-LSTM}, which is two \textit{LSTM}s stacked on top of each other.  LSTM
is designed to solve the \textit{long-term dependency} problem.  In theory
\textit{RNN}s are capable of handling long-term dependencies, but in practice
they don not, due to \textit{exploding gradient} problem. Where weights are updated
by the gradient of the loss function with respect to the current weights in each
epoch in training. In some cases the gradient maybe small, vanishingly! this
prevent the weights from changing and may stop the neural network from further
learning. \textit{LSTM}s overcome that problem.

% NOTE: draw LSTM cell using tizk
\begin{center}
\input{lstmUnit}
\captionof{figure}{LSTM Cell}
\label{lstm}
\end{center}

Figure \ref{lstm} \footnote{figure is inspired by  http://colah.github.io/posts/2015-08-Understanding-LSTMs}
shows an LSTM unit. $f_t$ is the forgetting gate, $i_t$ is the input gate, $o_t$
is the output gate, $C_t$ is the memory across cells. %NOTE
$W_j$,$U_j$, $b_j$ are the weight matrices and bias vector, $j \in \{f, i, o\}$.
The cell's hidden representation $h_t$ of $x_t$ is computed as the following:%
\begin{align*}
  f_t  &= \sigma(W_f  x_t + U_f h_{t-1} + b_t)\\
  i_t  &= \sigma(W_i  x_t + U_i h_{t-1} + b_i)\\
  o_t  &= \sigma(W_o  x_t + U_o h_{t-1} + b_o)\\
  C_t  &= f_t \circ c_{t-1} + i_t \circ tanh(W_c x_t + U_c h_{t-1} + b_c)\\
  h_t  &= o_t \circ tanh(c_t)
\end{align*}


% What are we going to do. Except the results
We have conducted many experiments using LSTM and BiLSTM with
different architectures. As it is shown in the table \ref{data_size}, the dataset
is imbalanced, there is a huge gap between the big and the small classes.
Our training set-up is as the following. We built a model then
we fed the data.  There are operations performed on the data before it is fed to
the model.  Each operation is a set of options. The first operation is the
encoding, we have 3 encoding techniques \{One-Hot, Binary, Two-Hot\}. The
second operation is whether we drop the diacritics or we keep it \{With
diacritics, Without diacritics\}. The third operation is whether we drop the
last 5 classes or we keep them \{Eliminate data, Full data\}, by
Eliminated data we mean dropping the last 5 tiny classes so that we have 11
classes, and by Full data we mean keeping the 16 classes.
The reason of eliminating the last 5 classes is the gap between them and the
rest, in addition this helps us in studying imbalanced data training. This
leads to the last operation which is weighting the loss function by%
\[w_c = \frac{1/n_c}{\sum\limits_{c}1/n_c},\]%
where $n_c$ is the sample size of class $c$, $c = 1, 2, ... C$, where $C$ is the number of classes.  Weighting the loss this way keeps the following:
\begin{enumerate}
  \item the density is constant (for normalization).
  \item the smaller the $n_c$ the larger the $w_c$.
  \item $\sum\limits_{c} w_c = 1$
\end{enumerate}
The total number of the experiments is the Cartesian product of all the previous
options sets. The same approch is taken in for the English poems, the operations
does not include Eliminated/Full and Weighted/Not-Weighted.








% \section{Results}
\input{Results}





% \section{Conclusion}




% FutureWork Section
\input{FutureWork}


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
% \ifCLASSOPTIONcaptionsoff
% \newpage
% \fi

\bibliography{references}

\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

\documentclass[12pt]{article}
\usepackage{multicol}


%% Drawing
\usepackage{tikz}
\usetikzlibrary{arrows} % For arrows :"D
\usepackage{subcaption}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\usepackage{amsfonts, amssymb, bm, amsmath}
\usepackage{pgfplots}
\definecolor{myBlue}{HTML}{7982db}
\definecolor{myGreen}{HTML}{ABDDA4}

% For Arabic
\usepackage{polyglossia}
\setmainlanguage{english}
\setotherlanguage{arabic}
\newfontfamily\arabicfont[Script=Arabic]{Amiri}



\begin{document}


\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%-----------------------
%	HEADING SECTIONS
%-----------------------

\textsc{\Large \textarabic{لا غالب إلا الله}}\\[0.9cm] % Major heading such as course name
\textsc{\LARGE Helwan University}\\[1.5cm] % Name of your university/college

%-----------------------
%	TITLE SECTION
%-----------------------

\HRule \\[0.4cm]
{ \LARGE \bfseries Learning meters of Arabic and English poems with recurrent
neural networks}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%-----------------------
%	AUTHOR SECTION
%-----------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Members, \textit{\small alphabetically}:}\\

\small{Abdaullah Ramzy}\\
\small{Ali Abdemoniem}\\
\small{Ali Osama}\\
\small{Taha Magdy}\\
\small{Umar Mohamed}\\
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Prof. Waleed A.\textsc{Yousuf} % Supervisor's Name
\end{flushright}
\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%-----------------------
%	DATE SECTION
%-----------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

%-----------------------
%	LOGO SECTION
%-----------------------
%\includegraphics[width=30mm,scale=0.5]{logo.png}\\[0.5cm] % Include a department/university logo - this will require the graphicx package

\vfill % Fill the rest of the page with whitespace

\end{titlepage}
%\tableofcontents \newpage




%%%%%%
\section{Introduction and Problem Statement}
Detecting the meter of poems is not an easy task for ordinary people, but how
computers will perform? Our task is to train a model so that it can detect the
meter of the input verse/text.
We have worked on Arabic and English in parallel, everything thing is applied to
Arabic is applied also in English, as possible as we can.

To be clearer, the model's input is a verse/text \textarabic{بيت شعر} and the
output is a class which is the verse's meter \textarabic{البحر}, as shown in the
figure below.


% FIGURE
\begin{center}
\begin{tikzpicture}
\centering

%\draw[step=0.5, gray, very thin] (0,0) grid (6,2);
\draw[rounded corners=2pt, thick] (2,0) rectangle (2+2,2);
\node at (3, 1.5) {Deep};
\node at (3, 1) {Learning};
\node at (3, 0.5) {Model};

\draw[arrows=-angle 90, line width=1pt ] (1, 1) -- (1 +1 -0.1, 1);
\node at (0, 1.5) {Verse};
\node at (0, 1) {\textarabic{بيتُ الشعر}};

\draw[arrows=-angle 90, line width=1pt] (4 +.1, 1) -- (4 +1, 1);
\node at (6, 1.5) {Meter};
\node at (6, 1) {\textarabic{البحْرُ}};

\end{tikzpicture}
\end{center}

The output variable is a class/categorical, then our problem can be described as
\textit{supervised learning  classification}.  We have trained some deep learning
models such as LSTM, Bi-LSTM and GRU.  Those models are chosen because of the
nature of our problem. We were trying to detect the verse's meter, which is a
sequence of characters and \textit{recurrent neural network} are suitable  to
learn that pattern, thanks to its cell's share-memory and its recursive
structure.


\section{The Project Road Map}
This is are four-stage project.  The first one is where we get the raw data and
put it is a feature-response form, the second stage is cleaning the data, by that
we mean that any non-letter character and unnecessary white-spaces are removed,
also, handling any diacritics issue, as it will be demonstrated in the next
sections, this stage include encoding the data to the form that is suitable to be
fed to our neural networks. 

The third stage is where the models are built and are tuned then we have
automated the experiments to run all the iterations one after another, much
details will be presented. Finally, we gather the results and analyse them, also
we then conduct some additional experiments to see the language and encoding
effect over the learning curve.  The first two steps were much difficult than
building the models. The following figure shows the road map.

\begin{center}
\begin{tikzpicture}
\centering

%\draw[step=0.5, gray, very thin] (-6,0) grid (6.5,2);

\draw[rounded corners=2pt, thick] (-6, 0) rectangle (-6 +2,2);
\draw[rounded corners=2pt, thick] (-2.5, 0) rectangle (-2.5 +2,2);
\draw[rounded corners=2pt, line width=2pt] (1, 0) rectangle (1 +2,2);
\draw[rounded corners=2pt, thick] (4.5, 0) rectangle (4.5 +2,2);


\node at (2, 1.2) {Building};
\node at (2, .7) {Models};

\node at (-1.5, 1.2) {Cleaning};
\node at (-1.5, .7) {Data};

\node at (-5, 1.5) {Scraping};
\node at (-5, 1) {The};
\node at (-5, .5) {Web};
\node at (-5, -.5) {\small \textit{Gathering Data}};

\node at (5.5, 1.2) {Analysing};
\node at (5.5, .7) {Results};

\def \littelM {0.1}
\draw[arrows=-angle 90, line width=1pt] (-6 +2 +\littelM, 1) -- (-2.5 -\littelM, 1);
\draw[arrows=-angle 90, line width=1pt] (-2.5 +2 +\littelM, 1) -- (1 -\littelM, 1);
\draw[arrows=-angle 90, line width=1pt] ( 1 +2 +\littelM, 1) -- (4.5 -\littelM, 1);

\end{tikzpicture}
\end{center}


\section{Tools}
Python is pseudo-code like programming language, it is so easy and high-level
that we can describe complex structures in a few lines of code, the main second
reason is that python recently has been so papular in the Artificial Intelligence
community. Its library is so rich with packages for Machine Learning, Deep
Learning, data manipulation, even for web-scraping; we don't need to parse HTML
by your hands.

We have used:
Two columns:
    \begin{multicols}{2}
\begin{itemize}
\item \textit{Python} 3.6.5
\item \textit{Keras} x.x for deep learning.
\item \textit{Tensorflow} x.x as back-end of Keras.
\item \textit{BeautifulSoup} for web scraping.
\end{itemize}
    \end{multicols}







\section{Scraping The Web \textit{\small --gathering the data}}
To train our models we need dataset of poems. For Arabic, this kind of dataset
are not so popular so you can hardly find a dataset for poetry. So, we had to
create our own. During our search we have found two big poetry web sites, that
contain tons of metrical-classified poems. And similar is happened for English.
So we have scrapped those web sites. \textit{Scraping} is to write a script to
browse the target web site, and copy the specific content and dump it to our
\textit{csv} file. This was the most difficult step in the hole project. 
 
Also, we present a very large Arabic dataset that can be used for further
research purposes, next sub-section contains much details about the Arabic
dataset. For the English dataset, the situation was worse than scraping big
complex web site, because the there were not big web site that contain a large
amount of metrical-classified poems, so the English dataset is to small if it is
compared to its Arabic counterpart, but this what we have found after an
extensive search. 
% NOTE:
cite(Farren) had been generously granted his data from the
Stanford English literature department, we have tried to contact them through
their email but none has reposed.

The scraping scripts are under the directory \texttt{repository\_root/scraping
the web}, accompanied by  a thorough self-contained \texttt{README.md} file,
which contains everything you may need to use or re-use the code, even the code
is written to be re-used with a lot of  \textit{in-line} documentation.


\subsection{Arabic dataset}
We have scrapped the Arabic dataset from two poetry websites:
\textarabic{الديوان}\footnote{\textit{aldiwan.net}}, \textarabic{الموسوعة
الشعرية}\footnote{\textit{poetry.tcaabudhabi.ae}}. Both are merged into one large
dataset.  The total number of verses is  1,862,046 poetic verses; each verse is
labeled by its meter, the poet who wrote it, and the
age which it was written in. There are 22 meters, 3701 poets and 11
ages; and they are Pre-Islamic, Islamic, Umayyad, Mamluk, Abbasid, Ayyubid, Ottoman,
Andalusian, era between Umayyad and Abbasid, Fatimid and modern.  We are only
interested  in the 16 classic meters which are attributed to \textit{Al-Farahidi},
and they are the majority of the dataset with a total number of 1,722,321
% NOTE: chage to the public rep.
verses\footnote{https://wwww.github.com/tahamagdy}. The following chart
which clarifies the sizes of the classes.


\begin{center}
\input{../Research_paper/barChart}
\end{center}




% NOTE: add a line-chart like arabic + percentatge for every class.
\subsection{English dataset}
The English dataset is scraped from many different web
resources\footnote{http://www.eighteenthcenturypoetry.org}. It consists of 199,002
verses, each of them is labeled with one of these four meters: \textit{Iambic},
\textit{Trochee}, \textit{Dactyl} and \textit{Anapaestic}.  The \textit{Iambic}
class dominates the dataset; there are  186,809 \textit{Iambic} verses, 5418
\textit{Trochee} verses, 5378  \textit{Anapaestic} verses, 1397 \textit{Dactyl}
verses.  We have downsampled the \textit{Iambic} class to 5550 verses.



\section{Cleaning Data}
Data was not clean enough, there were some non-alphabetical characters and many
unnecessary white spaces inside the text, they have been removed.  In addition,
there were diacritics mistakes, like the existence of two consecutive
\textit{harakat}, we have only kept one and have removed the other, or a
\textit{haraka} comes after a white space, it has been removed. 

% ◌
\section{Data preparing}
As a pre-encoding step, we have factored both of \textit{shadda} to two letter 
the first carries \textarabic{◌ْ} and the second carries \textarabic{حركة},
and \textit{tanween} to additional \textarabic{نْ},  by this step we shorten the encoding vector,
and save more memory.







\subsection{Data Encoding}
% NOTE: Copy the paper in here, after the finish the figures.

% General Scenario
Generally, a character will be represented as an $n$ vector. Consequently, a
verse would be an $n \times p$ matrix, where $n$ is the character representation
length; $n$ varies from one encoding to another, we have used 3 different
techniques to encode the characters, and $p$ is the verse's length.

% An issues
% NOTE: refine it; break it down to smaller parts.
There is an issue; we will take the \textit{one-hot} encoding as an example to
demonstrate it.  A character without diacritics is represented as a $37 \times 1$
vector, where 37 is the 36 letters in addition to a white-space character, so a
phrase like \textarabic{مرحبا} having 5 letters is encoded as a $37 \times 5$
matrix if it came with diacritics it would be represented as $41 \times 1$
vector, where 41 is 37 characters + $4$ diacritics, therefore  a phrase like
\textarabic{مَرْحَبَا} which has 5 letters and 4 diacritics, would be encoded as a
$41 \times 9$ matrix.  The issue comes from considering diacritics as stand-alone
characters while encoding, a phrase with diacritics has more characters than if
it was without diacritics, this leads to different number of time steps in the
\textit{RNN} cell and different input matrix dimensions which in turn leads to
changing the model architecture, we do not want models architectures to differ
whether diacritics exist of not in a verse so that we be able to conduct
fair performance comparisons between models. 
% Solution
As a solution we have encoded both a letter with its diacritic as one character,
there are 36 letters and $36 \times 4$ combinations between each letter and each
diacritic, then we have 181 characters including the white-space. From now
forward, we have a 181-character alphabet, according to it, we will encode verses.

% NOTE: See how to use those two figures
% NOTE: Reform so that you refer to 181-encoding
%       and to refer to the one hot transition from 36-to-180.
% * You may need something else.
So, we used $180 \times 1$ vector to represent a character in order to avoid the
previous two problems, see the upcoming figure  and figure
\ref{fig:one-hot}.


% NOTE: put binary next to common-features.
% old idea one hot figure
\input{../Research_paper/oldOneHot}

% Let the game begin :)
%% SIDE BY SIDE
\begin{figure}
% One Hot figure
\input{../Research_paper/subfigureOneHot}
\hfill
% Binary figure
\input{../Research_paper/subfigureBinary}
\end{figure}






% One-Hot encoding figure
\subsection{One-Hot encoding}
% 36 Arabic character  + ' ' = 37 char
Here, a character is represented by an $n \times 1$ \textit{one-hot} vector, where
$n$ is the alphabet length, in \textit{Arabic} it is the 181-character alphabet,
and in \textit{English} is the 26-letter alphabet.



% Binary encoding figure
\subsection{Binary Encoding}
The idea is to represent a character with an $n
\times 1$ vector which contains a unique combination of ones and zeros.  $n =
\ceil*{\log_2 l}$ where $l$ is the alphabet length, and $n$ is the sufficient
number of digits to represent $l$ unique binary combinations.  For example a phrase
like this \textarabic{مَرْحَبا}, it has 5 characters, figure
\ref{fig:binary-encoding} shows how it is encoded as a $8 \times 5$ matrix,
which saves more memory than the \textit{one-hot} and reduces the model capacity,
significantly. But on the other hand, the input characters share some features
between them due to the binary representation as it is shown in figure
\ref{fig:problem-with-binary-encoding}. We will make a comparison between both
encoding techniques \textit{binary} and \textit{one-hot}, at the end.



\subsection{Two-Hot encoding}
This is an intermediate technique which takes the advantages of the previous two
encoding techniques. In which we encode the letter and its diacritic
separately using \textit{one-hot} encoding, this way the letter is encoded as $37
\times 1$ \textit{one-hot} vector and the diacritic is encoded as $4 \times 1$
\textit{one-hot} vector, then both vectors are stacked to form one $41 \times 1$
vector. By this way, we reduces the vector dimension from 180 to 41 and also
minimizes the number of shared features between vectors to maximum 2 ones at each
vector.

% lolo figure
\input{../Research_paper/lolo}





\section{Building models}

\section{Analysing Results}

\end{document}



% XELATEX
\documentclass[12pt]{report}
\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{Helwan University}
\fancyhead[RE,LO]{Learning meters of poems with Deep Learning}
\fancyfoot[CE,CO]{\leftmark}
\fancyfoot[LE,RO]{\thepage}
 
\usepackage{multicol}

\usepackage{geometry}

\let\keptmaketitle\maketitle %<------------ADDED THIS LINE
\usepackage[hyphens]{url}
% \usepackage[ocgcolorlinks]{hyperref}
\usepackage{hyperref}
\hypersetup{
  colorlinks,
  citecolor=blue,
  filecolor=blue,
  linkcolor=blue,
  urlcolor=blue
}

\usepackage[numbers,sort&compress]{natbib}
\usepackage{geometry}
\usepackage{subcaption}
%\geometry{left=0.5in, right=0.5in, top=0.5in, bottom=0.5in}
\geometry{left=1.5in, right=1.5in, top=1.5in, bottom=1.5in}
\usepackage{fourier}
\DeclareMathAlphabet{\mathcal}{OT1}{pzc}{m}{it}
\DeclareSymbolFont{letters}{OML}{cmm}{m}{it}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\usepackage{amsfonts, amssymb, bm, amsmath}
\usepackage{booktabs}  % for \toprule ...
\usepackage{caption}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage{graphicx, type1cm, lettrine}
\usepackage{qtree}
\usepackage[dvipsnames, table]{xcolor}
\usepackage{collcell}
\usepackage{pgf}
\usepackage{forest}
\usepackage{setspace}
\usepackage{fontspec}
\usepackage{polyglossia} % <3
\setmainlanguage{english}
\setotherlanguage{arabic}


%\usepackage{times} % for Sans Serif
% for IPA
\usepackage{tipa}
\usepackage{pgfplots}
\definecolor{myBlue}{HTML}{7982db}
\definecolor{myGreen}{HTML}{ABDDA4}





% Arabic Fonts
%\newfontfamily\arabicfont[Script=Arabic,Scale=1.3]{Scheherazade}
\newfontfamily\arabicfont[Script=Arabic]{Amiri}

% This font is not found on my machine
%\setmainfont[Scale=0.925]{Heuristica}

\usepackage{bidipoem}

\tikzset{
  Above/.style={
    midway,
    above,
    font=\scriptsize,
    text width=1.5cm,
    align=center,
  },
  Below/.style={
    midway,
    below,
    font=\scriptsize,
    text width=1.5cm,
    align=center
  }
}

%% Confusion Matrix Settings

\newcommand\gray{gray}

\newcommand*{\woB}[1]{\multicolumn{1}{c}{#1}}
\newcommand\ColCell[1]{%
  \pgfmathparse{#1 <.8 ? 1 : 0}%
    \ifnum\pgfmathresult=0\relax\color{white}\fi
  \pgfmathparse{1-#1}%
  \expandafter\cellcolor\expandafter[%
    \expandafter\gray\expandafter]\expandafter{\pgfmathresult}#1}

\newcolumntype{E}{>{\collectcell\ColCell}c<{\endcollectcell}}





%% Drawing
\usepackage{tikz}
\usetikzlibrary{arrows} % For arrows :"D



%\usepackage{float}

% \newfontfamily\arabicfont[Script=Arabic,Scale=1.1]{Amiri}


%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% Citation Style
% \bibliographystyle{IEEEtran}
\bibliographystyle{plainnat}
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\renewcommand{\thesection}{\Roman{section}} 
\renewcommand{\thesubsection}{\thesection.\Roman{subsection}}

\let\maketitle\keptmaketitle %<------------ADDED THIS LINE
\definecolor{new_blue}{HTML}{517EB9}
%%%%%%%%%%%%%%%%%% 

\def\thesection{\arabic{section}}
\def\thesection{\arabic{subsection}}

\begin{document}

%\title{Learning meters of Arabic and English poems with Recurrent Neural Networks}
%\author{Yousef. W.A}
%
%%%%%%% 
%\maketitle 




\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%-----------------------
%	HEADING SECTIONS
%-----------------------

\textsc{\Large \textarabic{لا غالب إلا الله}}\\[1.9cm] % Major heading such as course name
\textsc{\LARGE Helwan University}\\[1.5cm] % Name of your university/college

%-----------------------
%	TITLE SECTION
%-----------------------

\HRule \\[.4cm]
{ \LARGE \bfseries Learning meters of Arabic and English poems with recurrent
neural networks}\\[0.4cm] % Title of your document
\HRule \\[3cm]
 
%-----------------------
%	AUTHOR SECTION
%-----------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\textit{\Large Members,} \textit{\small alphabetically}:\\

Abdaullah Ramzy\\
Ali Abdemoniem\\
Ali Osama\\
Taha Magdy\\
Umar Mohamed\\
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\textit{\Large Supervisor:} \\
Prof. Waleed A.\textsc{Yousuf} % Supervisor's Name
\end{flushright}
\end{minipage}\\[4cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%-----------------------
%	DATE SECTION
%-----------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

%-----------------------
%	LOGO SECTION
%-----------------------
%\includegraphics[width=30mm,scale=0.5]{logo.png}\\[0.5cm] % Include a department/university logo - this will require the graphicx package

\vfill % Fill the rest of the page with whitespace

\end{titlepage}









\tableofcontents
\newpage
%\renewcommand\IEEEkeywordsname{Keywords}
%\begin{IEEEkeywords}
%\ Poetry, Meters, Arabic, English, RNN, Deep Learning, Classification,
%\ Text Mining, Arud.
%\end{IEEEkeywords}

\section*{Abstract}
%% Should be revised after finishing this papers.

% 1. Opening appetite 
People can easily determine whether a piece of writing is
a poem or prose, but only specialists can determine which 
meter a poem is belonged to. 
% What we want to build
In the present paper, we build a model that can classify poems
according to their meters; a forward step towards machine understanding of Arabic language.

% 2. comprehensive introduction
A number of different \textit{deep learning} models are proposed for poem meter
classification. As poems are sequence data, then \textit{recurrent neural
networks} are suitable for the task.  We have trained three variants of them,
\textit{LSTM}, \textit{GRU} and \textit{Bi-LSTM} with different architectures.
% 3. character-level inputs, Why!
Because meters are nothing but sequences of characters,
then we have encoded the input text at the \textit{character-level},
% Question; what is thep?!
so that we preserve the information provided by the letters succession directly fed to thep models. In addition,
we introduce a comparative study on the difference between binary and one-hot encoding
in terms of their effect on the learning curve. 
We also introduce a new encoding technique called \textit{Two-Hot} which 
merges the advantages of both \textit{Binary} and \textit{One-Hot} techniques.


% 4. Motivating Questions
% Now, we are trying to answer this question, \textit{can
% computers learn to classify poems according to their meters?}

% Resaults

% \makebox[\textwidth][s]{This is a text that almost fills a complete printed line.}




\section{Introduction and Problem Statement}
Detecting the meter of poems is not an easy task for ordinary people, but how
computers will perform? Our task is to train a model so that it can detect the
meter of the input verse/text.
We have worked on Arabic and English in parallel, everything thing is applied to
Arabic is applied also in English, as possible as we can.

To be clearer, the model's input is a verse/text \textarabic{بيت شعر} and the
output is a class which is the verse's meter \textarabic{البحر}, as shown in the
figure below.


% FIGURE
\begin{center}
\begin{tikzpicture}
\centering

%\draw[step=0.5, gray, very thin] (0,0) grid (6,2);
\draw[rounded corners=2pt, thick] (2,0) rectangle (2+2,2);
\node at (3, 1.5) {Deep};
\node at (3, 1) {Learning};
\node at (3, 0.5) {Model};

\draw[arrows=-angle 90, line width=1pt ] (1, 1) -- (1 +1 -0.1, 1);
\node at (0, 1.5) {Verse};
\node at (0, 1) {\textarabic{بيتُ الشعر}};

\draw[arrows=-angle 90, line width=1pt] (4 +.1, 1) -- (4 +1, 1);
\node at (6, 1.5) {Meter};
\node at (6, 1) {\textarabic{البحْرُ}};

\end{tikzpicture}
\end{center}

The output variable is a class/categorical, then our problem can be described as
\textit{supervised learning  classification}.  We have trained some deep learning
models such as LSTM, Bi-LSTM and GRU.  Those models are chosen because of the
nature of our problem. We were trying to detect the verse's meter, which is a
sequence of characters and \textit{recurrent neural network} are suitable  to
learn that pattern, thanks to its cell's share-memory and its recursive
structure.


\section{The Project Road Map}
This is are four-stage project. 

\begin{enumerate}
    \item The first one is where we get the raw data and put it in a
            feature-response form.
    \item The second stage is cleaning the data, by which we mean that any
            non-letter character and unnecessary white-spaces are removed, also, handling any
            diacritics issue, as it will be demonstrated in the next sections, this stage
            include encoding the data to the form that is suitable to be fed to our neural
            networks.
    \item The third stage is where the models are built and are tuned then we
            have automated the experiments to run all the iterations one after another, much
            details will be presented.
    \item Finally, we gather the results and analyse them, also we then conduct
            some additional experiments to see the language and encoding effect over the
            learning curve.
\end{enumerate}

 

The first two steps were much difficult than
building the models. The following figure shows the road map.

\begin{center}
\begin{tikzpicture}
\centering

%\draw[step=0.5, gray, very thin] (-6,0) grid (6.5,2);

\draw[rounded corners=2pt, thick] (-6, 0) rectangle (-6 +2,2);
\draw[rounded corners=2pt, thick] (-2.5, 0) rectangle (-2.5 +2,2);
\draw[rounded corners=2pt, line width=2pt] (1, 0) rectangle (1 +2,2);
\draw[rounded corners=2pt, thick] (4.5, 0) rectangle (4.5 +2,2);


\node at (2, 1.2) {Building};
\node at (2, .7) {Models};

\node at (-1.5, 1.2) {Cleaning};
\node at (-1.5, .7) {Data};

\node at (-5, 1.5) {Scraping};
\node at (-5, 1) {The};
\node at (-5, .5) {Web};
\node at (-5, -.5) {\small \textit{Gathering Data}};

\node at (5.5, 1.2) {Analysing};
\node at (5.5, .7) {Results};

\def \littelM {0.1}
\draw[arrows=-angle 90, line width=1pt] (-6 +2 +\littelM, 1) -- (-2.5 -\littelM, 1);
\draw[arrows=-angle 90, line width=1pt] (-2.5 +2 +\littelM, 1) -- (1 -\littelM, 1);
\draw[arrows=-angle 90, line width=1pt] ( 1 +2 +\littelM, 1) -- (4.5 -\littelM, 1);

\end{tikzpicture}
\end{center}


\section{Tools}
Python is pseudo-code like programming language, it is so easy and high-level
that we can describe complex structures in a few lines of code, the main second
reason is that python recently has been so papular in the Artificial Intelligence
community. Its library is so rich with packages for Machine Learning, Deep
Learning, data manipulation, even for web-scraping; we don't need to parse HTML
by your hands.

We have used:
Two columns:
\begin{itemize}
\item \textit{Python} 3.6.5
\item \textit{Keras} 2.1.5 for deep learning.
\item \textit{Tensorflow} 1.7.0 as back-end of Keras.
\item \textit{BeautifulSoup} for web scraping.
\end{itemize}







\section{Scraping The Web \textit{\small --gathering the data}}
To train our models we need dataset of poems. For Arabic, this kind of dataset
are not so popular so you can hardly find a dataset for poetry. So, we had to
create our own. During our search we have found two big poetry web sites
\textarabic{الديوان}\footnote{\textit{aldiwan.net}}, \textarabic{الموسوعة
الشعرية}\footnote{\textit{poetry.tcaabudhabi.ae}}, that
contain tons of metrical-classified poems. And similar is happened for English.
So we have scrapped those web sites. \textit{Scraping} is to write a script to
browse the target web site, and copy the specific content and dump it to our
\textit{csv} file. This was the most difficult step in the hole project. 
 
Also, we present a very large Arabic dataset that can be used for further
research purposes, next sub-section contains much details about the Arabic
dataset. For the English dataset, the situation was worse than scraping big
complex web site, because the there were not big web site that contain a large
amount of metrical-classified poems, so the English dataset is to small if it is
compared to its Arabic counterpart, but this what we have found after an
extensive search. 
% NOTE:
\citet{farren} had been generously granted his data from the
Stanford English literature department, we have tried to contact them through
their email but none has reposed.

The scraping scripts are under the directory \texttt{repository\_root/scraping
the web}, accompanied by  a thorough self-contained \texttt{README.md} file,
which contains everything you may need to use or re-use the code, even the code
is written to be re-used with a lot of  \textit{in-line} documentation.























\section{Introduction}
%% 1* Introduce Arabic poems
% answers “why?”
%% الخليل بن أحمد صنف الشعر
\subsection{Arabic Language}
%% Arabic Introduction

% Hello, Arabic
% \lettrine[lines=3, findent=3pt, nindent=0pt]{A}{}
Arabic is the fifth most widely spoken language\footnote{\textit{according to the
20th edition of Ethnologue, 2017}}. It is written from right to left. Its
alphabet consists of 28 primary letters, and there are 8 more derived letters
from the basic ones, so the total count of Arabic characters is 36 characters.
The writing system is cursive; hence, most letters join to the letter that comes
after them, a few letters remain disjoint.

% Introduction to diacritics: ◌
Each Arabic letter represents a consonant, which means that short vowels are not
represented by the 36 characters, for this reason, the need of \textit{diacritics}
rises. \textit{Diacritics} are symbols that comes after a letter to state the
short vowel accompanied by that letter. There are four diacritics \textarabic{◌َ} \textarabic{◌ُ}
\textarabic{◌ِ} \textarabic{◌ْ} which represent the following short vowels
/\textit{a}/, /\textit{u}/, /\textit{i}/ and \textit{no-vowel} respectively,
their names are \textit{fat-ha, dam-ma, kas-ra and sukun} respectively.  The first
three symbols are called \textit{harakat}. Table \ref{arabic:diacritics_dal}
shows the 4 diacritics on a letter. 
%
There are two more sub-diacritics made up of the basic four to represent two
cases: the first case is when a letter is doubled where the first letter is
accompanied by \textit{sukun} and the second letter is accompanied by
\textit{haraka}, instead of doubling each letter accompanied by their
\textit{diacritics} explicitly, both of them are written once accompanied by
\textit{shadda} \textarabic{◌ّ}, \textit{shadda} states that its letter is
doubled, the first letter is always accompanied by \textit{sukun}, the second
letter's \textit{haraka} is followed by \textit{shadda}; for example,
\textarabic{دْدَ} is written \textarabic{دَّ}.
% the issue
The second case, Arabs pronounce the sound \textit{/n/} accompanied
\textit{sukun} at the end the indefinite words, that sound corresponds to this
letter \textarabic{نْ}, it is called \textit{noon-sakinah}, however, it is
just a phone, it is not a part of the indefinite word, if a word comes as a
definite word, no additional sound is added. Since it is not an essential sound,
it is not written as a letter, but it is written as  \textit{tanween}
\textarabic{◌ٌ ◌ً ◌ٍ}. 
% adding tanween and its relationship to the previous letter
\textit{Tanween} states the sound \textit{noon-sakinah}, but as you have noticed,
there are 3 \textit{tanween} symbols, this because  \textit{tanween} is added as
a diacritic over the last letter of the indefinite word, last letter is
accompanied by one of the 3 \textit{harakat}, the last letter's \textit{harakah}
needs to be stated in addition to the sound \textit{noon-sakinah}, so
\textit{tanween} is doubling the last letter's \textit{haraka}, this way the last
letter's \textit{haraka} is preserved in addition to stating the sound
\textit{noon-sakinah}; for example, \textarabic{رَجُلُ + نْ} is written
\textarabic{رَجُلٌ} and  \textarabic{رَجُلِ + نْ} is written \textarabic{رَجُلٍ}.
%
Those two cases will help us to reduce the dimension of the letter's feature vector as we
will see in \textit{preparing data} section.


% table: dal with diacritics
\begin{table}[!t]
  \centering
  \begin{tabular}{c c c c c c} 
    %\hline
    \toprule
    \textbf{\small{Diacritics}}     & \small{\textit{without}} & \small{\textit{fat-ha}} &
            \small{\textit{kas-ra}} & \small{\textit{dam-ma}} & \small{\textit{sukun}}\\
    %\hline
    \midrule
    \textbf{\small{Shape}}   & \textarabic{د} & \textarabic{دَ} & \textarabic{دِ} &
            \textarabic{دُ} & \textarabic{دْ}\\
    %\hline
    \bottomrule
  \end{tabular}
  \caption{\textit{Diacritics on the letter \textarabic{د}}}\label{arabic:diacritics_dal}
\end{table}

Diacritics are just to make short vowels clearer, but they are not necessary.
Moreover, a phrase without full diacritics or with just some on some letters is
right linguistically, so it is allowed to drop them from text.

% Diacritics in Unicode
In Unicode, Arabic diacritics are standalone symbols, each of them has its own
unicode. This is in contrast to the Latin diacritics; e.g., in the set
\textit{\{ê, é, è, ë, ē, ĕ, ě\}}, each combination of the letter \textit{e} and a diacritic is represented by one unicode.


\subsection{Attempts To Define Poetry}
Poetry is the other way of using language. Perhaps in some hypothetical beginning
of things it was the only way of using language or simply was language tout
court, prose being the derivative and younger rival. Both poetry and language are
fashionably thought to have belonged to ritual in early agricultural societies;
and poetry in particular, it has been claimed, arose at first in the form of
magical spells recited to ensure a good harvest. Whatever the truth of this
hypothesis, it blurs a useful distinction: by the time there begins to be a
separate class of objects called poems, recognizable as such, these objects are
no longer much regarded for their possible yam-growing properties, and such magic
as they may be thought capable of has retired to do its business upon the human
spirit and not directly upon the natural world outside.

Formally, poetry is recognizable by its greater dependence on at least one more
parameter, the line, than appears in prose composition. This changes its
appearance on the page; and it seems clear that people take their cue from this
changed appearance, reading poetry aloud in a very different voice from their
habitual voice, possibly because, as Ben Jonson said, poetry “speaketh somewhat
above a mortal mouth.” If, as a test of this description, people are shown poems
printed as prose, it most often turns out that they will read the result as prose
simply because it looks that way; which is to say that they are no longer guided
in their reading by the balance and shift of the line in relation to the breath
as well as the syntax.

That is a minimal definition but perhaps not altogether uninformative. It may be all that ought to be attempted in the way of a definition: Poetry is the way it is because it looks that way, and it looks that way because it sounds that way and vice versa.



\subsection{Arabic Poetry \textarabic{الشعر العربى}}
%% Introduction; the circumstances before alarud
Arabic poetry is the earliest form of Arabic literature. It dates back to the sixth century. Poets have written poems without knowing exactly what rules which make a collection of words a poem. People recognize poetry by nature, but only talented ones who could write poems. %
% What al-Farahidi did 
This was the case until \textit{Al-Farahidi} (718 – 786 CE) has analyzed the
Arabic poetry, then he came up with that the succession of consonants and vowels
produce patterns or \textit{meters}, which make the music of poetry.  He has
counted them fifteen meters.  After that, a student of \textit{Al-Farahidi} has
added one more meter to make them sixteen. Arabs call meters \textarabic{بحور}
which means "\textit{seas}".

\bigskip

% \textbf{Some conventions and terminologies}:
A poem is a collection of verses, a verse looks like the following:%
% What are poems and terminologies?
% What does a poem look like? bayt, shatr, ....

\vspace{0.5cm}
\begin{Arabic}
  \begin{traditionalpoem*}
% ويوم موعدهم أن يُحشروا زُمرا يوم التغابن إذ لا ينفع الحذرُ
% والأرض مملوءة جورا مسخرة ... لكل طاغية في الخلق محتكم

    ألا قاتل الله الحمامة غدوةً \quad & \quad على الأيكِ ماذا هيّجتْ حين غنّتِ

    تغَنّت بصوت أعجمىّ فهيجت  \quad & \quad من الوَجد ما كانت ضُلوعى أجنّتِ
  \end{traditionalpoem*}
\end{Arabic}%
\vspace{0.5cm}

A verse, known as \textit{bayt} in Arabic, consists of two halves; a half is
called a \textit{shatr}\footnote{it is a singular in arabic, but for simplicity
we will use it for both singular and plural.}. %
% Meters and Feet
\textit{Al-Farahidi} has introduced \textit{al-'arud
\textarabic{العروض}}\footnote{it is often called the \textit{Knowledge of
Poetry}.}; it is the study of poetic meters, in which he has laid down rigorous
rules and measures, with them we can determine whether a meter of a poem is sound
or broken. A meter is an ordered sequence of \textit{feet}. Feet are the basic
units of meters, there are eight of them. A Foot consists of a sequence of
consonant and vowels. Traditionally, feet are represented by mnemonic words
called \textit{tafa'il} (\textarabic{تفاعيل}).  According to \textit{al-Farahidi}
and his student, there are sixteen combinations of \textit{tafa'il}. A meter
appears in a \textit{verse} twice; each \textit{shatr} carries the same complete
meter.%

% NOTE: transpose it. \small{}
\begin{table}[!t]
  \centering
  \begin{tabular}{|c|c|} 
    \hline
    \textbf{Feet} & \textbf{Scansion} \\ 
    \hline
    \textarabic{فَعُولُنْ}  & \texttt{0/0//}\\
    \textarabic{فَاعِلُنْ}  & \texttt{0//0/}\\
    \textarabic{مُسْتَفْعِلُنْ}& \texttt{0//0/0/}\\
    \textarabic{مَفاعِيلُنْ}& \texttt{0/0/0//}\\
    \textarabic{مَفْعُولاَت} & \texttt{0//0///}\\
    \textarabic{فَاعِلاَتُنْ} & \texttt{0/0//0/}\\
    \textarabic{مُفَاعَلَتُنْ}& \texttt{0///0//}\\
    \textarabic{مُتَفَاعِلُنْ}& \texttt{0//0///}\\
    \hline
  \end{tabular}
  \caption{The eight feet. Every digit represents the corresponding diacritic
over each latter in the feet. \texttt{/} If a letter has got \textit{harakat} (
\textarabic{◌َ} \textarabic{◌ُ} \textarabic{◌ِ}), \texttt{0} if a letter has got
\textit{sukun} (\textarabic{◌ْ}). Any \textit{mad} (\textarabic{و, ا, ى}) is
equivalent to \textit{sukun}.}\label{arud:feet}
\end{table}


For example, the following \textit{shatr} \textarabic{وَيُسأَلُ في الحَوادِثِ ذو صَوابٍ} is
equivalent to the \textit{meter} \textarabic{مفاعلتن مفاعلتن فعول}, which means
it belongs to \textit{Al-Wafeer} meter. We can get the pattern of the
\textit{sukun} and \textit{harakat} by replacing each feet by the corresponding
code in table \ref{arud:feet}, which produces the following pattern that should
be read from right to left:%
\begin{flushright}
  {\texttt{0/0// 0///0// 0///0//}} % You have to filp it.
\end{flushright}
This is a very brief introduction to \textit{Arud}, many details are reduced.


\begin{center}
\includegraphics[scale=0.15]{verse}
\end{center}


 


\begin{center}
  \begin{tabular}[h!]{|c|c|} 
    \hline
    \textbf{Meter Name} & \textbf{Meter} \small{\textit{feet combination}} \\ 
    \hline
   \textit{al-Wafeer}    & \textarabic{مُفَاعَلَتُن مُفَاعَلَتُن فَعُولُن} \\ %
   \textit{al-Taweel}    & \textarabic{فَعُوْلُنْ مَفَاْعِيْلُنْ فَعُوْلُنْ مَفَاْعِلُنْ} \\ %
   \textit{al-Kamel}     & \textarabic{مُتَفَاْعِلُنْ مُتَفَاْعِلُنْ مُتَفَاْعِلُنْ} \\%
   \textit{al-Baseet}    & \textarabic{مُسْتَفْعِلُنْ فَاْعِلُنْ مُسْتَفْعِلُنْ فَاْعِلُنْ} \\%
   \textit{al-Khafeef}   & \textarabic{فَاْعِلاتُنْ مُسْتَفْعِلُنْ فَاْعِلاتُنْ} \\ %
   \textit{al-Rigz}      & \textarabic{مُسْتَفْعِلُنْ مُسْتَفْعِلُنْ مُسْتَفْعِلُنْ} \\%
   \textit{al-Raml}      & \textarabic{فَاْعِلاتُنْ فَاْعِلاتُنْ فَاْعِلاتُنْ} \\ %
   \textit{al-Motakarib} & \textarabic{فَعُوْلُنْ فَعُوْلُنْ فَعُوْلُنْ فَعُوْلُنْ} \\%
   \textit{al-Sar'e}     & \textarabic{مُسْتَفْعِلُنْ مُسْتَفْعِلُنْ مَفْعُوْلاتُ} \\%
   \textit{al-Monsafeh}  & \textarabic{مُسْتَفْعِلُنْ مَفْعُوْلاتُ مُسْتَفْعِلُنْ} \\
   \textit{al-Mogtath}   & \textarabic{مُسْتَفْعِلُنْ فَاْعِلاتُنْ فَاْعِلاتُنْ} \\
   \textit{al-Madeed}    & \textarabic{فَاْعِلاتُنْ فَاْعِلُنْ فَاْعِلاتُنْ } \\
   \textit{al-Hazg}      & \textarabic{مَفَاْعِيْلُنْ مَفَاْعِيْلُنْ} \\%
   \textit{al-Motadarik} & \textarabic{فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ فَاْعِلُنْ} \\%
   \textit{al-Moktadib}  & \textarabic{مَفْعُوْلاتُ مُسْتَفْعِلُنْ مُسْتَفْعِلُن} \\
   \textit{al-Modar'e}   & \textarabic{مَفَاْعِيْلُنْ فَاْعِلاتُنْ مَفَاْعِيْلُنْ} \\
    \hline
  \end{tabular}
  % NOTE: add more expressive caption; demonistrate the combinarions.
  \captionof{table}{\textit{The sixteen Arabic poem meters}}\label{arud:meters}
\end{center}

\subsection{Arabic sound system}
The sound system of Arabic is very different from that of English and the other
languages of Europe. It includes a number of distinctive guttural sounds
(pharyngeal and uvular fricatives) and a series of velarized consonants
(pronounced with accompanying constriction of the pharynx and raising of the back
of the tongue). There are three short and three long vowels (/a/, /i/, /u/ and
/ā/, /ī/, /ū/). Arabic words always start with a single consonant followed by a
vowel, and long vowels are rarely followed by more than a single consonant.
Clusters containing more than two consonants do not occur in the language.

Arabic shows the fullest development of typical Semitic word structure. An Arabic
word is composed of two parts: (1) the root, which generally consists of three
consonants and provides the basic lexical meaning of the word, and (2) the
pattern, which consists of vowels and gives grammatical meaning to the word.
Thus, the root /k-t-b/ combined with the pattern /-i-ā-/ gives kitāb ‘book,’
whereas the same root combined with the pattern /-ā-i-/ gives kātib ‘one who
writes’ or ‘clerk.’ The language also makes use of prefixes and suffixes, which
act as subject markers, pronouns, prepositions, and the definite article.

\subsection{Metre and rhyme}
The recording of the earliest-known Arabic poetry provided future generations
with examples of recitations by bards of 7th- or 8th-century versions of poems
whose original composition and performance date back perhaps centuries. The
collections reveal an already elaborate prosodic system, the earliest phases in
the development of which remain substantially unknown.

The various types of poem are marked by particular patterns of rhyme and syllabic
pulse. Each line is divided into two half-lines (called miṣrāʿ); the second of
the two ends with a rhyming syllable that is used throughout the poem. In order
that the listening audience may internalize the rhyme that is to be used, the
first line (which is often repeated) uses the rhyme at the end of both halves of
the line; thereafter the rhyme occurs only at the end of the complete line.











\subsection{English poetry Introduction}
% Introduction
% English prosody -> Cite the Meters.
% Intro: Add English development -> Middle-English -> Now.
English poetry dates back to the seventh century. At that time, poems were
written in \textit{Anglo-Saxon}, also known as \textit{The Old English}. Many
political changes have influenced the language until it becomes as it is
nowadays. English prosody was not formalized rigorously as a stand-alone
knowledge, but many tools of the \textit{Greek} prosody were borrowed to describe
the English prosody, tools like the Greek meters types which pre-dates the
English language by a long time.

% English prosody
A \textit{syllable} is the unit of pronunciation having one vowel sound, with or
without surrounding consonants. English words consist of one or more syllables.
For
% Syllables
% NOTE: smaller forawd slash
\begin{table}[!t]
  \centering
  \begin{tabular}{|c | c|} 
    \hline
    %\toprule
    \textbf{{Feet}}     & \textbf{{Stresses Combination}}\\ 
    \hline
    %\toprule
\textit{Iamb} & $\times$\textit{/}\\             %\midrule
\textit{Trochee}& \textit{/}$\times$\\           %\midrule
\textit{Dactyl} & \textit{/}$\times\times$\\     %\midrule
\textit{Anapest}& $\times\times$\textit{/}\\     %\midrule
\textit{Pyrrhic}& $\times\times$\\               %\midrule
\textit{Amphibrach}& $\times$\textit{/}$\times$\\%\midrule
\textit{Spondee}& \textit{/}\textit{/}\\
    %\bottomrule
    \hline
  \end{tabular}
  \caption{Every foot is a combination of stressed and unstressed syllables,
where stressed syllable is denoted by \textit{/} and unstressed syllable is
denoted by $\times$.}
\label{feet}
\end{table}
example the word "Water" \textipa{\sffamily /"wO:t@/} consists of two phonetic syllables:
\textipa{\sffamily /"wO:/}  
and \textipa{\sffamily /t@(r)/}. As you notice, each syllable have only one vowel
sound.
Syllables can be either stressed or unstressed which are
referred to by \textit{/} and $\times$, respectively. In previous "Water" example, the
first syllable is stressed, stresses are shown using the primary stress symbol
\textipa{\sffamily "} in phonetics, the second syllable is unstressed, so
the word "Water" is a  stressed-unstressed word, which can be denoted by
/$\times$, abstracting the word as stressed and unstressed syllables.
% The seven feet
There are seven different combinations of stressed and unstressed syllables form
make the seven poetic \textit{feet}s.  They are shown in table \ref{feet}.
% Meters
Meters are described as a sequence of feet. English meters are \textit{qualitative}
meters; which are stressed syllables coming at regular intervals.
A meter is repeating one of the previous seven feet one to eight times,
for every verse, then a verse's meter is determined by the repeated foot.
If the foot is repeated once, then verse is \textit{monometer}, if it is
repeated twice  then it is \textit{dimeter} verse, until \textit{octameter} which means
a foot is repeated eight times.  Here is an example, (stressed syllables are bold).
\begin{center}
 That \textbf{time} of \textbf{year} thou \textbf{mayst}  in \textbf{me}
be\textbf{hold}
\end{center}
The first verse belongs to \textit{Iambic} foot and it is repeated five times; so
it is \textit{Iambic pentameter}.




\subsection{Origins And Basic Characteristics}
English belongs to the Indo-European family of languages and is therefore related
to most other languages spoken in Europe and western Asia from Iceland to India.

The parent tongue, called Proto-Indo-European, was spoken about 5,000 years ago
by nomads believed to have roamed the southeast European plains. Germanic, one of
the language groups descended from this ancestral speech, is usually divided by
scholars into three regional groups: East (Burgundian, Vandal, and Gothic, all
extinct), North (Icelandic, Faroese, Norwegian, Swedish, and Danish), and West
(German, Dutch and Flemish, Frisian, and English). Though closely related to
English, German remains far more conservative than English in its retention of a
fairly elaborate system of inflections. Frisian, spoken by the inhabitants of the
Dutch province of Friesland and the islands off the west coast of Schleswig, is
the language most nearly related to Modern English. Icelandic, which has changed
little over the last thousand years, is the living language most nearly
resembling Old English in grammatical structure.


Modern English is analytic (i.e., relatively uninflected), whereas
Proto-Indo-European, the ancestral tongue of most of the modern European
languages (e.g., German, French, Russian, Greek), was synthetic, or inflected.
During the course of thousands of years, English words have been slowly
simplified from the inflected variable forms found in Sanskrit, Greek, Latin,
Russian, and German, toward invariable forms, as in Chinese and Vietnamese. The
German and Chinese words for the noun man are exemplary. German has five forms:
Mann, Mannes, Manne, Männer, Männern. Chinese has one form: ren. English stands
in between, with four forms: man, man’s, men, men’s. In English, only nouns,
pronouns (as in he, him, his), adjectives (as in big, bigger, biggest), and verbs
are inflected. English is the only European language to employ uninflected
adjectives; e.g., the tall man, the tall woman, compared to Spanish el hombre
alto and la mujer alta. As for verbs, if the Modern English word ride is compared
with the corresponding words in Old English and Modern German, it will be found
that English now has only 5 forms (ride, rides, rode, riding, ridden), whereas
Old English ridan had 13, and Modern German reiten has 16.

In addition to the simplicity of inflections, English has two other basic
characteristics: flexibility of function and openness of vocabulary.

Flexibility of function has grown over the last five centuries as a consequence
of the loss of inflections. Words formerly distinguished as nouns or verbs by
differences in their forms are now often used as both nouns and verbs. One can
speak, for example, of planning a table or tabling a plan, booking a place or
placing a book, lifting a thumb or thumbing a lift. In the other Indo-European
languages, apart from rare exceptions in Scandinavian languages, nouns and verbs
are never identical because of the necessity of separate noun and verb endings.
In English, forms for traditional pronouns, adjectives, and adverbs can also
function as nouns; adjectives and adverbs as verbs; and nouns, pronouns, and
adverbs as adjectives. One speaks in English of the Frankfurt Book Fair, but in
German one must add the suffix -er to the place-name and put attributive and noun
together as a compound, Frankfurter Buchmesse. In French one has no choice but to
construct a phrase involving the use of two prepositions: Foire du Livre de
Francfort. In English it is now possible to employ a plural noun as adjunct
(modifier), as in wages board and sports editor; or even a conjunctional group,
as in prices and incomes policy and parks and gardens committee. Any word class
may alter its function in this way: the ins and outs (prepositions becoming
nouns), no buts (conjunction becoming noun).

Openness of vocabulary implies both free admission of words from other languages
and the ready creation of compounds and derivatives. English adopts (without
change) or adapts (with slight change) any word really needed to name some new
object or to denote some new process. Words from more than 350 languages have
entered English in this way. Like French, Spanish, and Russian, English
frequently forms scientific terms from Classical Greek word elements. Although a
Germanic language in its sounds and grammar, the bulk of English vocabulary is in
fact Romance or Classical in origin.

English possesses a system of orthography that does not always accurately reflect
the pronunciation of words; see below Orthography.




\section{Literature review}
% A little Introduction
% Algorithmic Approaches
% Machine Learning Approaches 


% Classifying meters is addressed differently
% https://english.stackexchange.com/questions/38964/how-to-use-to-v-ing
Classifying and detecting poems problem has been addressed and formalized
differently across the literature. Moreover, the history is so rich of poetry
analysis studies, hundreds of years ago, even before computer appears. However,
the topic is still unexplored, computationally. 


% ## 1* Paper Name: An algorithm for the detection and analysis of arud
\citet{Abuata} present the most related work to our topic. They classify
Arabic poems according to their \textit{meters}.  But they have not addressed it
as a \textit{learning problem}, they have designed a deterministic
five-step \textit{algorithm} for analysing  and detecting meters. 


\begin{enumerate}
\item The first step and the most important is having the input text carrying
    full diacritics, this means that every single letter must carry a diacritic,
    explicitly.
\item The next step is converting input text into \textit{Arud
    writing}\footnote{It is a pronounced version of writing; where only pronounced
    sounds are written.} using if-else like rules.
\item Then metrical \textit{scansion} rules are applied to the \textit{Arud
    writing}, which leaves the input text as a series of zeros and ones. 
\item After that each group of zeros and ones are defined as a \textit{tafa'il}
    \ref{arud:feet}, so now we have a sequence of \textit{tafa'il}.

\item And finally  the input text is classified to the closest meter to the
    \textit{tafa'il} sequence \ref{arud:meters}.
\end{enumerate}

%%% Results
82.2\% is the classification accuracy on a relatively small sample, only 417
verse.  There are few well designated algorithms for detecting the meter of
Classical Arabic poem. Those algorithms as explained in the literature are either
complicated and use database or/and not well defined and tested. The proposed
algorithm presented above computes the correct meter of verses with high accuracy
( 82\%).  That algorithm is implemented to find classical Arabic poetry meter
(Buhūr). The algorithm utilizes the complete verse (ذٞث / bayt) writing styles and
characteristics to identify the type of meter that represents the verse. The
proposed algorithm consists of five main steps. These steps covert the input
poetry into Arud Writing in order to find the correct suitable meter that
represents the verse. The algorithm is based on a set of well defined rules used
through the algorithm steps. The most important part of the algorithm is the Arud
Writing part. Here we have many rules to apply and according to these rules we
have to add or remove different letters. The algorithm only needs to rewrite the
first part (sadr صذس (of the poem verse and not the whole verse The algorithm was
tested with a set of verses from different classical Arabic poems and we tried to
choose poems that cover all meters types. The results showed a high level of
accuracy with 82% of the verses were correctly matched the algorithm recognized
meters. 



%% 
% (Mohammad A. Alnagdawi 2013)
\citet{Alnagdawi2013} has taken a similar approach to the previous work,
but they formalized the \textit{scansion}, \textit{Arud} and some
lingual\footnote{like pronounced and silent rules, which is directly related to
\textit{harakat}} rules as
\textit{context-free grammar} and \textit{regular expression} templates, the
result is 75\% correctly classified from 128 verses.

%%%%%%%%%%%%%%%% 
% ## 2* Paper Name: An algorithm for the detection and 
% analysis of arud meter in Diwan poetry
% (Atakan KURT, Mehmet KARA 2010)
\citet{Kurt2012} have worked on detection and analysis of \textit{arud} meter in
Ottoman Language. They have depended on Ottoman \textit{aurd} rules to
construct an algorithm that analyses Ottoman poems. First Ottoman text
must be transliterated to Latin transcription alphabet (LTA) after that text is
fed to the algorithm which uses a database containing all Ottoman meters to
compare the detected meter extracted from LTA to the closest meter found in the
database.

%%%%% 
% My opinion; Criticizing algorithmic approaches.
Both \citet{Abuata} and \citet{Alnagdawi2013} have common problems.  The first
problem is that the test size cannot give an accurate performance for the algorithms
they have constructed, because it is very small. And a 75\% total accuracy of 128
verses is even worse.
The second problem is that the operation of converting verses into zeros and ones
patterns is probabilistic; it also depends on the meaning, which is a source of
randomness.  Then treating such a problem as a deterministic problem is
not going to be satisfying. Moreover, it results in numerous limitations
like obligating verses to have full diacritics on every single letter, before
conducting the classification.


%% Machine Learning Approaches;
% >Recognition of Modern Arabic Poems 
Here is a different approach to the previous, the algorithmic ones,
\citet{Almuhareb2015} has used machine learning to recognize modern Arabic poems
inside documents.  He has built \textit{Naive Bayes} and \textit{Decision Tree}
classifiers which detect poems based on the visual features, like line
length average, line length standard deviation, average number of block\footnote{
\textit{a block: is a group of lines separated by an empty character or more.}}, standard
deviation of block number, word repetition rate, diacritic rate, punctuation
rate. Those features have been extracted from 2067 documents which are
divided into 513 modern poems and 1554 prose.
Then classifiers have been evaluated using \textit{10-fold cross-validation}. 
The best accuracy 99.81\% has been achieved by the \textit{decision tree}
classifier which is trained on all features together.

As a reault they, built several classifier using the Decision Tree and Naive Bayes algorithms.
The first classifier was a F-Measure decision tree using all of the features.
This classifier achieved the best overall accuracy, 99.81\%. The remaining
classifiers were all Naive Bayes. Table I shows the results of these classifiers
using different sets of features. The best accuracy was achieved using only the
visual features at 99.71\%. The all feature classifier scored a slightly lower
performance of 99.61\%. On the other hand, the linguistic features achieved only
87.13\%. The baseline accuracy is 75.18\% based on the majority class in the
dataset. The results for the single feature classifiers varied widely. The best
single feature classifier is the classifier that was built using the “Block
Average Number of Lines” feature, 97.58\%. This is a very excellent performance;
about 2\% less than the best achieved result. The best linguistic feature is
“Diacritic Rate”, 82.44\%. Almost all of the remaining single feature classifiers
performed below or equal to the baseline. Even though the “Block Average Number
of Lines” feature achieved a very high accuracy, the effect of removing it from
the all features set is negligible, at an accuracy of 98.94\%. 
\begin{center}
\includegraphics[scale=0.22]{table}
\end{center}
The F-Measure scores were similar. The "Block Average Number of Lines” feature achieved the best F score for
a single feature (97.6\%). The excellent performance of the "Block Average Number of Lines” visual feature is
attributed to that poems are usually arranged in multiple blocks of text (stanzas) with a bunch of short lines
(verses). While plain unformatted text articles are normally organized as one or more blocks of less but longer
lines of text representing paragraphs. In the dataset, the average value of this feature is 10.7 lines for poems, and
10.2 lines for article documents. For item list documents, this value is 13.4. Fig. 7 shows the class distribution
using the "Block Average Number of Lines" and "Average Line Length" features.
Four other features achieved good precision rates, but with reduced recall: Rhyme Rate, Punctuation Rate,
Average and SD of Line Length. This shows that rhymes and punctuations are good indicators for poems,
however, in some poems, they may not be emphasized enough. Fig. 8 and 9 shows the poem and non-poem
distribution in the rhyme and punctuation feature space. The same applies to the line length features. Poem lines
tend to be short and with low standard deviation in most cases but not always. The mean of the average and SD
of line length of poems in the dataset is 23.6 and 8.5 characters, respectively, compared to 390.4 and 375.7
characters for non-poems. Fig.10 shows the distribution in the line length space.
The precisions of the remaining four features were low, namely, Diacritic Rate, SD of Block Number of Lines,
Line and Word Repetition Rates. These features, found to be not very indicatives for poems. On the other hand,
the Diacritic Rate feature achieved a good recall score (82.4\%) but the low
precision score (56.5\%) is due to
having a similar usage rate of diacritics in poem and non-poem samples in the dataset. 







%%% DONE

\citet{Tizhoosh2006a} has presented similar work to \citet{Almuhareb2015}, have
trained \textit{Naive Bayes} and \textit{Decision Tree}
using visual features, they reached accuracy above 90\%.

% My review
There is a point here, visual features may work when detecting poems inside
documents due to poems are written in specific structure which distinguishes
them from other text inside documents. In theses approaches models have no clue
about the real patterns that create poems, of course the way how words are structured
inside text does not produce a poems, at all.


\citet{Tanasescu2016} has worked on binary classifying English poems where
\textit{metric} and \textit{free-verse} are the categories, he faced an
interesting problem with their dataset, it was imbalanced, (871 metrical poems,
4115 free-verse), for this reason they have used \textit{bootstrap aggregating}
(also known as \textit{bagging}), which is a meta-algorithm that can greatly
improve decision tree accuracy.  With \textit{J48} and \textit{bootstrap
aggregating}, he was able to achieve a 94.39\% correctly classifying poetry as
metrical or not.

By the way,\textit{Bootstrap aggregating} , also called bagging, is a machine learning
ensemble meta-algorithm designed to improve the stability and accuracy of machine
learning algorithms used in statistical classification and regression. It also
reduces variance and helps to avoid overfitting. Although it is usually applied
to decision tree methods, it can be used with any type of method. Bagging is a
special case of the model averaging approach.



As a results, binary classification experiments (metered or not me- tered) were
better than the baseline. The unbalanced test experiment resulted in an accuracy
of 94.39\% (Figure 10), which was significantly better than ZeroR (82.53\%) but
only marginally better than OneR (92.10\%). The two balanced test results (from
over and under sampling) were not signif- icantly different from the original
test (less than 1\% differ- ence between the three). Classification may improve
with more advanced classification algorithms and may improve further still as we
continue to add more training data to our corpus.  The Scandroid was more
accurate at determining lexical stress and parsing text into metrical feet than
the work done by the Stanford Literary Lab. It may be possible to train a neural
network in a way similar to the work done by Hay- ward (but using more modern
methods) to further improve accuracy and introduce sensitivity to different
levels of stress (not just binary encoding).  There are known issues with the
Scandroid, including lex- ical ambiguity and phrasal verbs (Hartman 2005). Lexi-
cal ambiguity affects the pronunciation of certain words de- pending on whether
they are verbs or nouns. Hartman uses “con VICT” (verb) and “CON vict” (noun) as
an example. This ambiguity could be lessened by including contextual information
from a POS (Part of Speech) tagger. Phrasal verbs are lexical units that operate
as a single word but do not look that way, e.g. “put out”, “put down” or “put
away”. Stress tends to be evenly distributed over the verb and gram- matical
structure following it. A POS tagger could also be used to identify phrasal verbs
and adjust stress appropriately. Finally, our implementation of the Scandroid is
missing the ability to identify promoted stress. Source code cleanup should make
that easier to implement. Rhyme detection can be improved and extended by adding
the ability to detect internal/nonterminal rhymes and rhymes spread out over
multiple words (known as mosaic rhymes), e.g. “Poet” and “know it”. If a word is
not listed in the cmudict, we used the double metaphone algorithm by Lawrence
Philips (Philips 1990) and spelling as crude back- ups. A more sophisticated
approach may involve the use of a computer text to speech system.

%%% END

\subsection*{Encoding the categorical variables and  its impact on neural network performance}
% Abdullah
Encoding features has an impact on the neural network performance.
\citet{Potdar2017} has done a comparative study on six encoding techniques.  We
are interested in the comparison of \textit{one-hot} and \textit{binary}. They
have used Artificial Neural Network for evaluating cars based on seven ordered
qualitative features. The accuracy of the model was the same in both
encodings---\textit{one-hot} and \textit{binary}.


% Motivation Section
\input{MotivationSection}


\section{Datasets}
\subsection{Arabic dataset}
We have scrapped the Arabic dataset from two big poetry websites:
\textarabic{الديوان}\footnote{\textit{aldiwan.net}}, \textarabic{الموسوعة
الشعرية}\footnote{\textit{poetry.tcaabudhabi.ae}}. Both are merged into one large
dataset. It is important to note that the verses' diacritic states are not
consistent, this means that a verse can carry full, semi diacritics or it can
carry nothing. The total number of verses is  1,862,046 poetic verses; each verse is
labeled by its meter, the poet who wrote it, and the
age which it was written in. There are 22 meters, 3701 poets and 11
ages; and they are Pre-Islamic, Islamic, Umayyad, Mamluk, Abbasid, Ayyubid, Ottoman,
Andalusian, era between Umayyad and Abbasid, Fatimid and modern.  We are only
interested  in the 16 classic meters which are attributed to \textit{Al-Farahidi},
and they are the majority of the dataset with a total number of 1,722,321
% NOTE: change the public rep.
verses\footnote{https://wwww.github.com/tahamagdy}.


\begin{center}
\input{MeterSizesBarChart}

\captionof{figure}{Meter names are on the $x$-axis, size  is on the $y$-axis.}
\label{data_size}
\end{center}


% NOTE: add a line-chart like arabic + percentatge for every class.
\subsection{English dataset}
The English dataset is scraped from many different web
resources\footnote{http://www.eighteenthcenturypoetry.org}. It consists of 199,002
verses, each of them is labeled with one of these four meters: \textit{Iambic},
\textit{Trochee}, \textit{Dactyl} and \textit{Anapaestic}.  The \textit{Iambic}
class dominates the dataset; there are  186,809 \textit{Iambic} verses, 5418
\textit{Trochee} verses, 5378  \textit{Anapaestic} verses, 1397 \textit{Dactyl}
verses.  We have downsampled the \textit{Iambic} class to 5550 verses.

\section{Preparing Data}
% answers “when, where, how, how much?”
% Give question to whet the whet the reader’s appetite to read further and give a taste
% to read the methods. 


\subsection{Data Cleaning}
%%%% Take the common between encoding, with/without are the same length
%%%% ذكر مشكلة ديوان

For both Arabic and English data, they were not clean enough, there were some
non-alphabetical characters and many unnecessary white spaces inside the text,
they have been removed.  For Arabic, there were diacritics mistakes, like the
existence of two consecutive \textit{harakat}, we have only kept one and have
removed the other, or a \textit{haraka} comes after a white space, it has been
removed. 

% Data preparing
As a pre-encoding step, we have factored both of \textit{shadda} and
\textit{tanween} to two letters, as it previously presented in Arabic language
introduction,  by this step we shorten the encoding vector, and save more memory.


\subsection{Data Encoding}

\subsection{Categorical Data}
a categorical variable is a variable that can take on one of a limited, and
usually fixed, number of possible values, assigning each individual or other unit
of observation to a particular group or nominal category on the basis of some
qualitative property. In computer science and some branches of mathematics,
categorical variables are referred to as enumerations or enumerated types.
Commonly (though not in this article), each of the possible values of a
categorical variable is referred to as a level. The probability distribution
associated with a random categorical variable is called a categorical
distribution.

Categorical data is the statistical data type consisting of categorical variables or of data that has been converted into that form, for example as grouped data. More specifically, categorical data may derive from observations made of qualitative data that are summarised as counts or cross tabulations, or from observations of quantitative data grouped within given intervals. Often, purely categorical data are summarised in the form of a contingency table. However, particularly when considering data analysis, it is common to use the term "categorical data" to apply to data sets that, while containing some categorical variables, may also contain non-categorical variables.


%% Issues:
\citet{Agirrezabal2017} shows that representations of data learned from
character-based neural models are more informative than the ones from
hand-crafted features. 
% Motivation for modeling letters as features.
In our case, we want to detect meters inside verses, meters are nothing but
patterns of the letters successions as previously presented, so it is convenient
to encode verses at \textit{character level} so that our model learns from
sequences of characters.


% General Scenario
Generally, a character will be represented as an $n$ vector. Consequently, a
verse would be an $n \times p$ matrix, where $n$ is the character representation
length  and $p$ is the verse's length,
$n$ varies from one encoding to another, we have used 3 different
techniques to encode the characters.
% An issues
% NOTE: refine it; break it down to smaller parts.
There is an issue; we will take the \textit{one-hot} encoding as an example to
demonstrate it.  A character without diacritics is represented as a $37 \times 1$
vector, where 37 is the 36 letters in addition to a white-space character, so a
phrase like \textarabic{مرحبا} having 5 letters is encoded as a $37 \times 5$
matrix if it came with diacritics it would be represented as $41 \times 1$
vector, where 41 is 37 characters + $4$ diacritics, therefore  a phrase like
\textarabic{مَرْحَبَا} which has 5 letters and 4 diacritics, would be encoded as a
$41 \times 9$ matrix.  The issue comes from considering diacritics as stand-alone
characters while encoding, a phrase with diacritics has more characters than if
it was without diacritics, this leads to different number of time steps in the
\textit{RNN} cell and different input matrix dimensions which in turn leads to
changing the model architecture, we do not want models architectures to differ
whether diacritics exist of not in a verse so that we be able to conduct
fair performance comparisons between models. 
% Solution
As a solution we have encoded both a letter with its diacritic as one character,
there are 36 letters and $36 \times 4$ combinations between each letter and each
diacritic, then we have 181 characters including the white-space. From now
forward, we have a 181-character alphabet, according to it, we will encode verses.

% NOTE: See how to use those two figures
% NOTE: Reform so that you refer to 181-encoding
%       and to refer to the one hot transition from 36-to-180.
% * You may need something else.
So, we used $180 \times 1$ vector to represent a character in order to avoid the
previous two problems, see the upcoming figure  and figure
\ref{fig:one-hot}.



% old idea one hot figure
\input{oldOneHot}



% Let the game begin :)
%% SIDE BY SIDE
\begin{figure}
% One Hot figure
\input{subfigureOneHot}
\hfill
% Binary figure
\input{subfigureBinary}
\end{figure}






% One-Hot encoding figure
\subsubsection{One-Hot encoding}
% 36 Arabic character  + ' ' = 37 char
Here, a character is represented by an $n \times 1$ \textit{one-hot} vector, where
$n$ is the alphabet length, in \textit{Arabic} it is the 181-character alphabet,
and in \textit{English} is the 28-letter alphabet.



% Binary encoding figure
\subsubsection{Binary Encoding}
The idea is to represent a character with an $n
\times 1$ vector which contains a unique combination of ones and zeros.  $n =
\ceil*{\log_2 l}$ where $l$ is the alphabet length, and $n$ is the sufficient
number of digits to represent $l$ unique binary combinations.  For example a phrase
like this \textarabic{مَرْحَبا}, it has 5 characters, figure
\ref{fig:binary} shows how it is encoded as a $8 \times 5$ matrix,
which saves more memory than the \textit{one-hot} and reduces the model capacity,
significantly. But on the other hand, the input characters share some features
between them due to the binary representation as it is shown in figure
\ref{commonFproblem}. We will make a comparison between both
encoding techniques \textit{binary} and \textit{one-hot}, at the end.



% NOTE: put a more expressive caption.
% NOTE; add common features figure.
\begin{center}
\input{commonFeatures}
\label{commonFproblem}
\captionof{figure}{Common feature problem; letters may share some
representation feature.}
\end{center}


% Umar;
It means when characters are fed them to NN, it will compute linear combination,
$Z = XW + B$, and we will multiply the features $X$ by its weights $W$ and apply
activation function then we will compute the cross-entropy loss function and
after computing the gradients using BPTT, the optimizer will update the weights
to reduce the loss so if the optimizer updates weights of this two common
features to reduce error of one character of them it will effect the rest
characters that have the same feature.


% lolo figure
\input{lolo}

\subsubsection{Two-Hot encoding}
This is an intermediate technique which takes the advantages of the previous two
encoding techniques. In which we encode the letter and its diacritic
separately using \textit{one-hot} encoding, this way the letter is encoded as $37
\times 1$ \textit{one-hot} vector and the diacritic is encoded as $4 \times 1$
\textit{one-hot} vector, then both vectors are stacked to form one $41 \times 1$
vector. By this way, we reduces the vector dimension from 180 to 41 and also
minimizes the number of shared features between vectors to maximum 2 ones at each
vector.






\section{Model}
% RNN, LSTM BiLSTM Citation.
Our experiments depend on \textit{LSTM} introduced by \citet{Hochreiter1997}; and
\textit{Bi-LSTM}, which is two \textit{LSTM}s stacked on top of each other.  LSTM
is designed to solve the \textit{long-term dependency} problem.  In theory
\textit{RNN}s are capable of handling long-term dependencies, but in practice
they don not, due to \textit{exploding gradient} problem. Where weights are updated
by the gradient of the loss function with respect to the current weights in each
epoch in training. In some cases the gradient maybe small, vanishingly! this
prevent the weights from changing and may stop the neural network from further
learning. \textit{LSTM}s overcome that problem.

% NOTE: draw LSTM cell using tizk
\begin{center}
\input{lstmUnit}
\captionof{figure}{LSTM Cell}
\label{lstm}
\end{center}

Figure \ref{lstm} \footnote{figure is inspired by  http://colah.github.io/posts/2015-08-Understanding-LSTMs}
shows an LSTM unit. $f_t$ is the forgetting gate, $i_t$ is the input gate, $o_t$
is the output gate, $C_t$ is the memory across cells. %NOTE
$W_j$,$U_j$, $b_j$ are the weight matrices and bias vector, $j \in \{f, i, o\}$.
The cell's hidden representation $h_t$ of $x_t$ is computed as the following:%
\begin{align*}
  f_t  &= \sigma(W_f  x_t + U_f h_{t-1} + b_t)\\
  i_t  &= \sigma(W_i  x_t + U_i h_{t-1} + b_i)\\
  o_t  &= \sigma(W_o  x_t + U_o h_{t-1} + b_o)\\
  C_t  &= f_t \circ c_{t-1} + i_t \circ tanh(W_c x_t + U_c h_{t-1} + b_c)\\
  h_t  &= o_t \circ tanh(c_t)
\end{align*}


% What are we going to do. Except the results
We have conducted many experiments using LSTM and BiLSTM with
different architectures. As it is shown in the table \ref{data_size}, the dataset
is imbalanced, there is a huge gap between the big and the small classes.
Our training set-up is as the following. We built a model then
we fed the data.  There are operations performed on the data before it is fed to
the model.  Each operation is a set of options. The first operation is the
encoding, we have 3 encoding techniques \{One-Hot, Binary, Two-Hot\}. The
second operation is whether we drop the diacritics or we keep it \{With
diacritics, Without diacritics\}. The third operation is whether we drop the
last 5 classes or we keep them \{Eliminate data, Full data\}, by
Eliminated data we mean dropping the last 5 tiny classes so that we have 11
classes, and by Full data we mean keeping the 16 classes.
The reason of eliminating the last 5 classes is the gap between them and the
rest, in addition this helps us in studying imbalanced data training. This
leads to the last operation which is weighting the loss function by%
\[w_c = \frac{1/n_c}{\sum\limits_{c}1/n_c},\]%
where $n_c$ is the sample size of class $c$, $c = 1, 2, ... C$, where $C$ is the number of classes.  Weighting the loss this way keeps the following:
\begin{enumerate}
  \item the density is constant (for normalization).
  \item the smaller the $n_c$ the larger the $w_c$.
  \item $\sum\limits_{c} w_c = 1$
\end{enumerate}
The total number of the experiments is the Cartesian product of all the previous
options sets. The same approch is taken in for the English poems, the operations
does not include Eliminated/Full and Weighted/Not-Weighted.

For Enlgish, we have use used another recurrent unit, we have used
Gated Recurrent Unit.

\includegraphics[scale=0.3]{gru}








% \section{Results}
\input{Results}





% \section{Conclusion}




% FutureWork Section
\input{FutureWork}


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
% \ifCLASSOPTIONcaptionsoff
% \newpage
% \fi

\bibliography{references}

\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

@Article{Abandah2015,
  author =	 {Abandah, Gheith A and Graves, Alex and Al-shagoor, Balkees and Arabiyat, Alaa and
                  Jamour, Fuad and Al-taee, Majid},
  title =	 {{Automatic Diacritization of Arabic Text Using Recurrent Neural networks}},
  journal =	 {International Journal on Document Analysis and Recognition (IJDAR) June 201},
  year =	 2015,
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Abandah et
                  al. - 2015 - Automatic diacritization of Arabic text using recurrent neural
                  networks.pdf:pdf},
  isbn =	 9626530081,
  keywords =	 {arabic text,automatic diacritization,deep neural networks,long short-term,machine
                  learning,neural networks,recurrent,sequence transcription}
}


@Article{Anderson2017,
  author =	 {Anderson, Blake and Paul, Subharthi and McGrew, David},
  title =	 {{Deciphering Malware's Use of Tls (without decryption)}},
  journal =	 {Journal of Computer Virology and Hacking Techniques},
  year =	 2017,
  pages =	 {1-17},
  doi =		 {10.1007/s11416-017-0306-6},
  archiveprefix ={arXiv},
  arxivid =	 {1607.01639},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Deciphering Malware's use of
                  TLS (without Decryption).pdf:pdf},
  keywords =	 {Cryptography}}


@article{Aslett2015,
  author =	 {Aslett, Louis J. M. and Esperan{\c{c}}a, Pedro M. and Holmes, Chris C.},
  title =	 {{A Review of Homomorphic Encryption and Software Tools for Encrypted Statistical
                  Machine learning}},
  pages =	 {1--21},
  year =	 {2015},
  url =		 {http://arxiv.org/abs/1508.06574},
  abstract =	 {Recent advances in cryptography promise to enable secure statistical computation
                  on encrypted data, whereby a limited set of operations can be carried out without
                  the need to first decrypt. We review these homomorphic encryption schemes in a
                  manner accessible to statisticians and machine learners, focusing on pertinent
                  limitations inherent in the current state of the art. These limitations restrict
                  the kind of statistics and machine learning algorithms which can be implemented
                  and we review those which have been successfully applied in the
                  literature. Finally, we document a high performance R package implementing a
                  recent homomorphic scheme in a general framework.},
  archivePrefix ={arXiv},
  arxivId =	 {1508.06574},
  eprint =	 {1508.06574},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/A review of homomorphic
                  encryption and software tools for encrypted statistical machine learning.pdf:pdf},
  keywords =	 {Cryptography,data privacy,encrypted statistical analysis,homomorphic
                  encryption,homomorphic encryption r package},
  mendeley-tags ={Cryptography},
}


@Article{Belinkov2015,
  author =	 {Belinkov, Yonatan and Glass, James},
  title =	 {{Arabic Diacritization With Recurrent Neural Networks}},
  journal =	 {Proceedings of the 2015 Conference on Empirical Methods in Natural Language
                  Processing},
  year =	 2015,
  number =	 {September},
  pages =	 {2281-2285},
  doi =		 {10.18653/v1/D15-1274},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Belinkov,
                  Glass - 2015 - Arabic Diacritization with Recurrent Neural Networks.pdf:pdf},
  isbn =	 9781941643327
}


@article{Cho2014,
  author =	 {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau,
                  Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  title =	 {{Learning Phrase Representations Using Rnn Encoder-Decoder for Statistical Machine
                  Translation}},
  year =	 {2014},
  doi =		 {10.3115/v1/D14-1179},
  url =		 {https://doi.org/10.3115/v1/D14-1179},
  abstract =	 {In this paper, we propose a novel neural network model called RNN Encoder-Decoder
                  that consists of two recurrent neural networks (RNN). One RNN encodes a sequence
                  of symbols into a fixed-length vector representation, and the other decodes the
                  representation into another sequence of symbols. The encoder and decoder of the
                  proposed model are jointly trained to maximize the conditional probability of a
                  target sequence given a source sequence. The performance of a statistical machine
                  translation system is empirically found to improve by using the conditional
                  probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional
                  feature in the existing log-linear model. Qualitatively, we show that the proposed
                  model learns a semantically and syntactically meaningful representation of
                  linguistic phrases.},
  archivePrefix ={arXiv},
  arxivId =	 {1406.1078},
  eprint =	 {1406.1078},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. -
                  2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical
                  Machine Translation.pdf:pdf},
  isbn =	 {9781937284961},
  issn =	 {09205691},
  pmid =	 {2079951},
}


@article{Eldesouki,
  author =	 {Eldesouki, Mohamed and Samih, Younes and Abdelali, Ahmed and Attia, Mohammed and
                  Mubarak, Hamdy and Darwish, Kareem and Kallmeyer, Laura and City, New York},
  title =	 {{Arabic Multi-Dialect Segmentation: Bi-Lstm-Crf Vs. SVM}},
  archivePrefix ={arXiv},
  arxivId =	 {arXiv:1708.05891v1},
  eprint =	 {arXiv:1708.05891v1},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Arabic Multi-Dialect
                  Segmentation- bi-LSTM-CRF vs. SVM.pdf:pdf},
}


@article{Ficler2017,
  author =	 {Ficler, Jessica and Goldberg, Yoav},
  title =	 {{Controlling Linguistic Style Aspects in Neural Language Generation}},
  number =	 {section 3},
  year =	 {2017},
  url =		 {http://arxiv.org/abs/1707.02633},
  abstract =	 {Most work on neural natural language generation (NNLG) focus on controlling the
                  content of the generated text. We experiment with controlling several stylistic
                  aspects of the generated text, in addition to its content. The method is based on
                  conditioned RNN language model, where the desired content as well as the stylistic
                  parameters serve as conditioning contexts. We demonstrate the approach on the
                  movie reviews domain and show that it is successful in generating coherent
                  sentences corresponding to the required linguistic style and content.},
  archivePrefix ={arXiv},
  arxivId =	 {1707.02633},
  eprint =	 {1707.02633},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Controlling Linguistic Style
                  Aspects in Neural Language Generation.pdf:pdf},
}


@article{Ghosh2015,
  author =	 {Ghosh, Shaona and Kristensson, Ola},
  title =	 {{Neural Networks for Text Correction and Completion in Keyboard Decoding}},
  volume =	 {14},
  number =	 {8},
  pages =	 {1--14},
  year =	 {2015},
  abstract =	 {-Despite the ubiquity of mobile and wearable text messaging applications, the
                  problem of keyboard text decoding is not tackled sufficiently in the light of the
                  enormous success of the deep learning Recurrent Neural Network (RNN) and
                  Convolutional Neural Networks (CNN) for natural language understanding. In
                  particular, considering that the keyboard decoders should operate on devices with
                  memory and processor resource constraints, makes it challenging to deploy
                  industrial scale deep neural network (DNN) models. This paper proposes a
                  sequence-to-sequence neural attention network system for automatic text correction
                  and completion. Given an erroneous sequence, our model encodes character level
                  hidden representations and then decodes the revised sequence thus enabling
                  auto-correction and completion. Further, what makes the problem different from
                  vanilla language modelling is the simultaneous text correction and completion. We
                  achieve this by a combination of character level CNN and gated recurrent unit
                  (GRU) encoder along with and a word level gated recurrent unit (GRU) attention
                  decoder. Unlike traditional language models that learn from billions of words, our
                  corpus size is only 12 million words; an order of magnitude smaller. The memory
                  footprint of our learnt model for inference and prediction is also an order of
                  magnitude smaller than the conventional language model based text decoders. We
                  report baseline performance for neural keyboard decoders in such limited
                  domain. Our models achieve a word level accuracy of 90{\ \%} and a character error
                  rate CER of 2.4 over the Twitter typo dataset. We present a novel dataset of noisy
                  to corrected mappings by inducing the noise distribution from the Twitter data
                  over the OpenSubtitles 2009 dataset; on which our model predicts with a word level
                  accuracy of 98{\ \%} and sequence accuracy of 0.689. We have also conducted an
                  user study from 8 users, with our model predicting with an average CER of 2.6{\
                  \%} while being competitive with the state-of-the-art non-neural touch-screen
                  keyboard decoders at CER of 1.6{\ \%}. We observe a CER of 2.1{\ \%} on physical
                  keyboard based decoding. Further, we plan to release the training dataset and the
                  associated software along with the baselines as an open-source tool-kit. We also
                  propose an alternative smooth evaluation measure over the character error rate
                  (CER) for evaluating model predictions based on the contextual underlying intent
                  of the sentence. Additionally, we have released our trained decoder as an
                  inference server available at www-edc.eng.cam.ac.uk/shaona.},
  archivePrefix ={arXiv},
  arxivId =	 {1709.06429},
  eprint =	 {1709.06429},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Neural Networks for Text
                  Correction and Completion in Keyboard Decoding.pdf:pdf},
  keywords =	 {Artificial Neural Networks,Convolution Neural Networks,Decoding,Index
                  Terms-Machine Learning,Recurrent Neural Networks,Supervised Learning},
}


@article{Graves2013,
  author =	 {Graves, Alex},
  title =	 {{Generating Sequences With Recurrent Neural Networks}},
  pages =	 {1--43},
  year =	 {2013},
  doi =		 {10.1145/2661829.2661935},
  url =		 {https://doi.org/10.1145/2661829.2661935},
  abstract =	 {This paper shows how Long Short-term Memory recurrent neural networks can be used
                  to generate complex sequences with long-range structure, simply by predicting one
                  data point at a time. The approach is demonstrated for text (where the data are
                  discrete) and online handwriting (where the data are real-valued). It is then
                  extended to handwriting synthesis by allowing the network to condition its
                  predictions on a text sequence. The resulting system is able to generate highly
                  realistic cursive handwriting in a wide variety of styles.},
  archivePrefix ={arXiv},
  arxivId =	 {1308.0850},
  eprint =	 {1308.0850},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Graves - 2013
                  - Generating Sequences With Recurrent Neural Networks.pdf:pdf},
  isbn =	 {2000201075},
  issn =	 {18792782},
  pmid =	 {23459267},
}


@article{Greydanus2017,
  author =	 {Greydanus, Sam},
  title =	 {{Learning the Enigma With Recurrent Neural Networks}},
  year =	 {2017},
  url =		 {http://arxiv.org/abs/1708.07576},
  abstract =	 {Recurrent neural networks (RNNs) represent the state of the art in translation,
                  image captioning, and speech recognition. They are also capable of learning
                  algorithmic tasks such as long addition, copying, and sorting from a set of
                  training examples. We demonstrate that RNNs can learn decryption algorithms -- the
                  mappings from plaintext to ciphertext -- for three polyalphabetic ciphers
                  (Vigen$\backslash$`ere, Autokey, and Enigma). Most notably, we demonstrate that an
                  RNN with a 3000-unit Long Short-Term Memory (LSTM) cell can learn the decryption
                  function of the Enigma machine. We argue that our model learns efficient internal
                  representations of these ciphers 1) by exploring activations of individual memory
                  neurons and 2) by comparing memory usage across the three ciphers. To be clear,
                  our work is not aimed at 'cracking' the Enigma cipher. However, we do show that
                  our model can perform elementary cryptanalysis by running known-plaintext attacks
                  on the Vigen$\backslash$`ere and Autokey ciphers. Our results indicate that RNNs
                  can learn algorithmic representations of black box polyalphabetic ciphers and that
                  these representations are useful for cryptanalysis.},
  annote =	 {Input: Polyalphabetic Encrypted Text Output: Decrypted Text Data Representation:
                  One-Hot Vector},
  archivePrefix ={arXiv},
  arxivId =	 {1708.07576},
  eprint =	 {1708.07576},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Learning the Enigma with
                  Recurrent Neural Networks .pdf:pdf},
  keywords =	 {Cryptography,RNN},
  mendeley-tags ={Cryptography,RNN},
}


@article{HendySusanto,
  author =	 {{Hendy Susanto}, Raymond and {Leong Chieu}, Hai and Lu, Wei},
  title =	 {{Learning To Capitalize With Character-Level Recurrent Neural Networks: an
                  Empirical Study}},
  pages =	 {2090--2095},
  abstract =	 {In this paper, we investigate case restoration for text without case
                  information. Previous such work operates at the word level. We pro-pose an
                  approach using character-level recur-rent neural networks (RNN), which performs
                  competitively compared to language model-ing and conditional random fields (CRF)
                  ap-proaches. We further provide quantitative and qualitative analysis on how RNN
                  helps im-prove truecasing.},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Hendy Susanto,
                  Leong Chieu, Lu - Unknown - Learning to Capitalize with Character-Level Recurrent
                  Neural Networks An Empirical Study.pdf:pdf},
}


@Article{Hopkins2017,
  author =	 {Hopkins, Jack},
  title =	 {{Automatically Generating Rhythmic Verse With Neural Networks}},
  journal =	 {Acl},
  year =	 2017,
  pages =	 {168-178},
  doi =		 {10.18653/v1/P17-1016},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Hopkins - 2017
                  - Automatically Generating Rhythmic Verse with Neural Networks.pdf:pdf},
  keywords =	 {Character Language Model,Generating Rhythmic,LSTM,Orthographic decoding,RNN,Rhythm
                  Modeling,Use LSTM to train,Use phonetic encoding to learn,english poetry,one-hot
                  encoding}}


@Article{Hwang2017,
  author =	 {Hwang, Kyuyeon and Sung, Wonyong},
  title =	 {{Character-Level Language Modeling With Hierarchical Recurrent Neural networks}},
  journal =	 {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -
                  Proceedings},
  year =	 2017,
  pages =	 {5720-5724},
  doi =		 {10.1109/ICASSP.2017.7953252},
  archiveprefix ={arXiv},
  arxivid =	 {1609.03777},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/CHARACTER-LEVEL LANGUAGE
                  MODELING WITH HIERARCHICAL RECURRENT NEURAL NETWORKS.pdf:pdf},
  isbn =	 9781509041176,
  keywords =	 {Character-level language model,hierarchical recurrent neural network,long
                  short-term memory}
}


@article{Kim2015,
  author =	 {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
  title =	 {{Character-Aware Neural Language Models}},
  year =	 {2015},
  doi =		 {2},
  url =		 {https://doi.org/2},
  abstract =	 {We describe a simple neural language model that relies only on character-level
                  inputs. Predictions are still made at the word-level. Our model employs a
                  convolutional neural network (CNN) and a highway network over characters, whose
                  output is given to a long short-term memory (LSTM) recurrent neural network
                  language model (RNN-LM). On the English Penn Treebank the model is on par with the
                  existing state-of-the-art despite having 60{\ \%} fewer parameters. On languages
                  with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model
                  outperforms word-level/morpheme-level LSTM baselines, again with fewer
                  parameters. The results suggest that on many languages, character inputs are
                  sufficient for language modeling. Analysis of word representations obtained from
                  the character composition part of the model reveals that the model is able to
                  encode, from characters only, both semantic and orthographic information.},
  annote =	 {Input: Output: Data Representation: طه يقرأ هذه الورقة},
  archivePrefix ={arXiv},
  arxivId =	 {1508.06615},
  eprint =	 {1508.06615},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Character-Aware Neural Language
                  Models.pdf:pdf},
  isbn =	 {9781577357605},
  issn =	 {14814374},
  pmid =	 {15003161},
}


@article{Kowsari2017,
  author =	 {Kowsari, Kamran and Brown, Donald E. and Heidarysafa, Mojtaba and Meimandi, Kiana
                  Jafari and Gerber, Matthew S. and Barnes, Laura E.},
  title =	 {{HDLTex: Hierarchical Deep Learning for Text Classification}},
  year =	 {2017},
  abstract =	 {The continually increasing number of documents produced each year necessitates
                  ever improving information processing methods for searching, retrieving, and
                  organizing text. Central to these information processing methods is document
                  classification, which has become an important application for supervised
                  learning. Recently the performance of these traditional classifiers has degraded
                  as the number of documents has increased. This is because along with this growth
                  in the number of documents has come an increase in the number of categories. This
                  paper approaches this problem differently from current document classification
                  methods that view the problem as multi-class classification. Instead we perform
                  hierarchical classification using an approach we call Hierarchical Deep Learning
                  for Text classification (HDLTex). HDLTex employs stacks of deep learning
                  architectures to provide specialized understanding at each level of the document
                  hierarchy.},
  archivePrefix ={arXiv},
  arxivId =	 {1709.08267},
  eprint =	 {1709.08267},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Kowsari et
                  al. - 2017 - HDLTex Hierarchical Deep Learning for Text Classification.pdf:pdf},
}


@Article{Lai2015,
  author =	 {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
  title =	 {{Recurrent Convolutional Neural Networks for Text Classification}},
  journal =	 {Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year =	 2015,
  pages =	 {2267-2273},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Lai et al. -
                  2015 - Recurrent Convolutional Neural Networks for Text Classification.pdf:pdf},
  isbn =	 9781577357018,
  keywords =	 {NLP and Machine Learning Track}
}


@article{Lee2016,
  author =	 {Lee, Jason and Cho, Kyunghyun and Hofmann, Thomas},
  title =	 {{Fully Character-Level Neural Machine Translation Without Explicit Segmentation}},
  year =	 {2016},
  url =		 {https://arxiv.org/pdf/1603.06147.pdf},
  abstract =	 {Most existing machine translation systems operate at the level of words, relying
                  on explicit segmentation to extract tokens. We introduce a neural machine
                  translation (NMT) model that maps a source character sequence to a target
                  character sequence without any segmentation. We employ a character-level
                  convolutional network with max-pooling at the encoder to reduce the length of
                  source representation, allowing the model to be trained at a speed comparable to
                  subword-level models while capturing local regularities. Our
                  character-to-character model outperforms a recently proposed baseline with a
                  subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance
                  on FI-EN and RU-EN. We then demonstrate that it is possible to share a single
                  character-level encoder across multiple languages by training a model on a
                  many-to-one translation task. In this multilingual setting, the character-level
                  encoder significantly outperforms the subword-level encoder on all the language
                  pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual
                  character-level translation even surpasses the models specifically trained on that
                  language pair alone, both in terms of BLEU score and human judgment.},
  archivePrefix ={arXiv},
  arxivId =	 {1610.03017},
  eprint =	 {1610.03017},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Cho,
                  Hofmann - 2016 - Fully Character-Level Neural Machine Translation without Explicit
                  Segmentation.pdf:pdf},
  isbn =	 {9781510827585},
}


@article{Lee2017,
  author =	 {Lee, Jinhyuk and Kim, Hyunjae and Ko, Miyoung and Choi, Donghee and Choi, Jaehoon
                  and Korea, Jaewoo Kang},
  title =	 {{Name Nationality Classification With Recurrent Neural Networks}},
  year =	 {2017},
  abstract =	 {Personal names tend to have many variations differ-ing from country to
                  country. Though there exists a large amount of personal names on the Web,
                  na-tionality prediction solely based on names has not been fully studied due to
                  its difficulties in extracting subtle character level features. We propose a
                  recur-rent neural network based model which predicts na-tionalities of each name
                  using automatic feature ex-traction. Evaluation of Olympic record data shows that
                  our model achieves greater accuracy than pre-vious feature based approaches in
                  nationality pre-diction tasks. We also evaluate our proposed model and baseline
                  models on name ethnicity classifica-tion task, again achieving better or
                  comparable per-formances. We further investigate the effective-ness of character
                  embeddings used in our proposed model.},
  annote =	 {I Will read and omar this paper},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. -
                  2017 - Name Nationality Classification with Recurrent Neural Networks.pdf:pdf},
  keywords =	 {Machine Learning: Classification,Machine Learning: Deep Learning,Machine Learning:
                  Neural Networks,Use RNN,Use character embeddings,Use n-gram models on char-level},
  mendeley-tags ={Use RNN,Use character embeddings,Use n-gram models on char-level},
}


@article{Lewis2016,
  author =	 {Lewis, Gene},
  title =	 {{Sentence Correction Using Recurrent Neural Networks}},
  year =	 {2016},
  url =		 {https://cs224d.stanford.edu/reports/Lewis.pdf},
  abstract =	 {In this work, we propose that a pre-processing method for changing text data to
                  conform closer to the distribution of standard English will help increase the
                  perfor-mance of many state-of-the-art NLP models and algorithms when confronted
                  with data taken " from the wild " . Our system receives as input a text word,
                  sentence or paragraph which we assume contains (possibly none) random corruptions;
                  for-mally, we say that the input comes from a corrupted language domain that is a
                  superset of our target language domain. Our system then processes this input and
                  outputs a " translation " or " projection " to our target language domain, with
                  the goal of the output being to preserve the latent properties of the input text
                  (senti-ment, named entities, etc.) but mutated in a way that embeds these
                  properties in a representation familiar to other NLP systems.},
  annote =	 {reading by Omar {\&} Abdullah (1)},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Lewis - 2016 -
                  Sentence Correction using Recurrent Neural Networks.pdf:pdf},
  keywords =	 {Char-RNN,English text,LSTM,NLP to translation,Text using RNN,mini-batch,one-hot
                  encoding,softmax},
  mendeley-tags ={Char-RNN,English text,LSTM,NLP to translation,Text using RNN,mini-batch,one-hot
                  encoding,softmax},
}


@article{Liu2016,
  author =	 {Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
  title =	 {{Recurrent Neural Network for Text Classification With Multi-Task Learning}},
  year =	 {2016},
  abstract =	 {Neural network based methods have obtained great progress on a variety of natural
                  language processing tasks. However, in most previous works, the models are learned
                  based on single-task supervised objectives, which often suffer from insufficient
                  training data. In this paper, we use the multi-task learning framework to jointly
                  learn across multiple related tasks. Based on recurrent neural network, we propose
                  three different mechanisms of sharing information to model text with task-specific
                  and shared layers. The entire network is trained jointly on all these
                  tasks. Experiments on four benchmark text classification tasks show that our
                  proposed models can improve the performance of a task with the help of other
                  related tasks.},
  archivePrefix ={arXiv},
  arxivId =	 {1605.05101},
  eprint =	 {1605.05101},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Qiu,
                  Huang - 2016 - Recurrent Neural Network for Text Classification with Multi-Task
                  Learning.pdf:pdf},
  isbn =	 {978-1-57735-770-4},
}


@article{Lopyrev2015,
  author =	 {Lopyrev, Konstantin},
  title =	 {{Generating News Headlines With Recurrent Neural Networks}},
  year =	 {2015},
  url =		 {https://arxiv.org/pdf/1512.01712.pdf},
  abstract =	 {We describe an application of an encoder-decoder recurrent neural network with
                  LSTM units and attention to generating headlines from the text of news
                  articles. We find that the model is quite effective at concisely paraphrasing news
                  articles. Furthermore, we study how the neural network decides which input words
                  to pay attention to, and specifically we identify the function of the different
                  neurons in a simplified attention mechanism. Interestingly, our simplified
                  attention mechanism performs better that the more complex attention mechanism on a
                  held out set of articles.},
  archivePrefix ={arXiv},
  arxivId =	 {1512.01712},
  eprint =	 {1512.01712},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Lopyrev - 2015
                  - Generating News Headlines with Recurrent Neural Networks.pdf:pdf},
}


@Article{Martens2011,
  author =	 {Martens, James},
  title =	 {{Generating Text With Recurrent Neural Networks}},
  journal =	 {Neural Networks},
  year =	 2011,
  volume =	 131,
  number =	 1,
  pages =	 {1017-1024},
  doi =		 2,
  archiveprefix ={arXiv},
  arxivid =	 {arXiv:gr-qc/9809069v1},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Generating Text with Recurrent
                  Neural Networks.pdf:pdf},
  isbn =	 9781450306195,
  primaryclass = {arXiv:gr-qc}
}


@Article{Prieto2017,
  author =	 {Prieto, Luis P. and Rodr{\'{i}}guez-Triana, Mar{\'{i}}a Jes{\'{u}}s and Kusmin,
                  Marge and Laanpere, Mart},
  title =	 {{Smart School Multimodal Dataset and challenges}},
  journal =	 {CEUR Workshop Proceedings},
  year =	 2017,
  volume =	 1828,
  pages =	 {53-59},
  doi =		 {10.1145/1235},
  archiveprefix ={arXiv},
  arxivid =	 {arXiv:1603.07016v1},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Learning text representation
                  using recurrent convolutional neural network with highway layers.pdf:pdf},
  isbn =	 9781450321389,
  keywords =	 {Multimodal learning analytics,Multimodal teaching analytics,STEM
                  education,Sensors,Smart classroom,Smart school}
}


@Article{Tizhoosh2008,
  author =	 {Tizhoosh, Hamid R and Dara, Rozita},
  title =	 {{Poetic Features for Poem Recognition : a Comparative Study}},
  journal =	 {Design Engineering},
  year =	 2008,
  number =	 {April 2014},
  pages =	 {24-39},
  doi =		 {10.13176/11.62},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Tizhoosh, Dara
                  - 2008 - Poetic Features for Poem Recognition A Comparative Study.pdf:pdf},
  keywords =	 {English poem}
}


@Article{Wu2013,
  author =	 {Wu, Zhizheng and Virtanen, Tuomas and Kinnunen, Tomi and Chng, Eng Siong and Li,
                  Haizhou},
  title =	 {{Exemplar-Based Unit Selection for Voice Conversion Utilizing Temporal
                  Information}},
  journal =	 {Proceedings of the Annual Conference of the International Speech Communication
                  Association, INTERSPEECH},
  year =	 2013,
  pages =	 {3057-3061},
  doi =		 {10.1063/1.4906785},
  archiveprefix ={arXiv},
  arxivid =	 {1502.01710},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. -
                  2013 - Exemplar-based unit selection for voice conversion utilizing temporal
                  Information.pdf:pdf},
  isbn =	 0123456789,
  keywords =	 {Multi-frame exemplar,Temporal information,Unit selection,Voice conversion}}


@article{Xie,
  author =	 {Xie, Stanley and Rastogi, Ruchir and Chang, Max},
  title =	 {{Deep Poetry: Word-Level and Character-Level Language Models for Shakespearean
                  Sonnet Generation}},
  abstract =	 {Text generation is a foundational task in natural language processing, forming the
                  core of a diverse set of practical applications ranging from image captioning and
                  text summarization to question answering. However, most of this work has focused
                  on generating prose. We investigate whether deep learning systems can be used to
                  synthesize poetry, in particular Shakespearean-styled works. Previous work on
                  generating Shakespeare prose involved training models exclusively on the word or
                  character level. Here, we implement those previous models for poetry generation
                  and show that models that combine word and character level informa-tion, such as a
                  Gated LSTM and a CNN-based LSTM, significantly outperform the baseline word-LSTM
                  and char-LSTM models. Perplexity scores for the two complex models are almost 10
                  fold better than that for our baselines, and human ratings of the model-generated
                  sonnets reflect this as well. In particular, the son-nets our complex models
                  generate have a coherent meaning and relatively correct meter without blatantly
                  copying Shakespeare's original works. These results en-courage us that models that
                  blend word and character level information would be useful for a variety of tasks
                  outside of just poetry generation and may be crucial in bridging the gap between
                  computer generated and human written text.},
  file =	 {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Xie, Rastogi,
                  Chang - Unknown - Deep Poetry Word-Level and Character-Level Language Models for
                  Shakespearean Sonnet Generation.pdf:pdf},
}


@article{Xu2016,
  author =	 {Xu, Weidi and Sun, Haoze and Deng, Chao and Tan, Ying},
  title =	 {{Variational Autoencoders for Semi-Supervised Text Classification}},
  year =	 {2016},
  url =		 {http://arxiv.org/abs/1603.02514},
  abstract =	 {Although semi-supervised variational autoencoder (SemiVAE) works in image
                  classification task, it fails in text classification task if using vanilla LSTM as
                  its decoder. From a perspective of reinforcement learning, it is verified that the
                  decoder's capability to distinguish between different categorical labels is
                  essential. Therefore, Semi-supervised Sequential Variational Autoencoder (SSVAE)
                  is proposed, which increases the capability by feeding label into its decoder RNN
                  at each time-step. Two specific decoder structures are investigated and both of
                  them are verified to be effective. Besides, in order to reduce the computational
                  complexity in training, a novel optimization method is proposed, which estimates
                  the gradient of the unlabeled objective function by sampling, along with two
                  variance reduction techniques. Experimental results on Large Movie Review Dataset
                  (IMDB) and AG's News corpus show that the proposed approach significantly improves
                  the classification accuracy compared with pure-supervised classifiers, and
                  achieves competitive performance against previous advanced
                  methods. State-of-the-art results can be obtained by integrating other
                  pretraining-based methods.},
  archivePrefix ={arXiv},
  arxivId =	 {1603.02514},
  eprint =	 {1603.02514},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Variational Autoencoder for
                  Semi-supervised Text Classification.pdf:pdf},
}


@Article{Yan2016,
  author =	 {Yan, Rui},
  title =	 {{I, Poet: Automatic Poetry Composition Through Recurrent Neural Networks With
                  Iterative Polishing schema}},
  journal =	 {IJCAI International Joint Conference on Artificial Intelligence},
  year =	 2016,
  volume =	 {2016-Janua},
  pages =	 {2238-2244},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Automatic Poetry Composition
                  through Recurrent Neural Networks with Iterative Polishing Schema.pdf:pdf},
  keywords =	 {Machine Learning}
}


@article{Yogatama2017,
  author =	 {Yogatama, Dani and Dyer, Chris and Ling, Wang and Blunsom, Phil},
  title =	 {{Generative and Discriminative Text Classification With Recurrent Neural
                  Networks}},
  year =	 {2017},
  url =		 {http://arxiv.org/abs/1703.01898},
  abstract =	 {We empirically characterize the performance of discriminative and generative LSTM
                  models for text classification. We find that although RNN-based generative models
                  are more powerful than their bag-of-words ancestors (e.g., they account for
                  conditional dependencies across words in a document), they have higher asymptotic
                  error rates than discriminatively trained RNN models. However we also find that
                  generative models approach their asymptotic error rate more rapidly than their
                  discriminative counterparts---the same pattern that Ng {\&} Jordan (2001) proved
                  holds for linear classification models that make more naive conditional
                  independence assumptions. Building on this finding, we hypothesize that RNN-based
                  generative classification models will be more robust to shifts in the data
                  distribution. This hypothesis is confirmed in a series of experiments in zero-shot
                  and continual learning settings that show that generative models substantially
                  outperform discriminative models.},
  archivePrefix ={arXiv},
  arxivId =	 {1703.01898},
  eprint =	 {1703.01898},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Generative and Discriminative
                  Text Classification with Recurrent Neural Networks.pdf:pdf},
}


@article{Zhou2015,
  author =	 {Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis C. M.},
  title =	 {{A C-Lstm Neural Network for Text Classification}},
  year =	 {2015},
  url =		 {http://arxiv.org/abs/1511.08630},
  abstract =	 {Neural network models have been demonstrated to be capable of achieving remarkable
                  performance in sentence and document modeling. Convolutional neural network (CNN)
                  and recurrent neural network (RNN) are two mainstream architectures for such
                  modeling tasks, which adopt totally different ways of understanding natural
                  languages. In this work, we combine the strengths of both architectures and
                  propose a novel and unified model called C-LSTM for sentence representation and
                  text classification. C-LSTM utilizes CNN to extract a sequence of higher-level
                  phrase representations, and are fed into a long short-term memory recurrent neural
                  network (LSTM) to obtain the sentence representation. C-LSTM is able to capture
                  both local features of phrases as well as global and temporal sentence
                  semantics. We evaluate the proposed architecture on sentiment classification and
                  question classification tasks. The experimental results show that the C-LSTM
                  outperforms both CNN and LSTM and can achieve excellent performance on these
                  tasks.},
  archivePrefix ={arXiv},
  arxivId =	 {1511.08630},
  eprint =	 {1511.08630},
  file =	 {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/A C-LSTM Neural Network for
                  Text Classification.pdf:pdf},
}



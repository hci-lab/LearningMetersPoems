@article{Abandah2015,
author = {Abandah, Gheith A and Graves, Alex and Al-shagoor, Balkees and Arabiyat, Alaa and Jamour, Fuad and Al-taee, Majid},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Abandah et al. - 2015 - Automatic diacritization of Arabic text using recurrent neural networks.pdf:pdf},
isbn = {9626530081},
journal = {International Journal on Document Analysis and Recognition (IJDAR) June 201},
keywords = {arabic text,automatic diacritization,deep neural networks,long short-term,machine learning,neural networks,recurrent,sequence transcription},
title = {{Automatic diacritization of Arabic text using recurrent neural networks}},
url = {http://www.abandah.com/gheith/wp-content/uploads/2015/02/Auto{\_}Diac.pdf},
year = {2015}
}
@article{Anderson2017,
abstract = {The use of TLS by malware poses new challenges to network threat detection because traditional pattern-matching techniques can no longer be applied to its messages. However, TLS also introduces a complex set of observable data features that allow many inferences to be made about both the client and the server. We show that these features can be used to detect and understand malware communication, while at the same time preserving the privacy of benign uses of encryption. These data features also allow for accurate malware family attribution of network communication, even when restricted to a single, encrypted flow. To demonstrate this, we performed a detailed study of how TLS is used by malware and enterprise applications. We provide a general analysis on millions of TLS encrypted flows, and a targeted study on 18 malware families composed of thousands of unique malware samples and ten-of-thousands of malicious TLS flows. Importantly, we identify and accommodate the bias introduced by the use of a malware sandbox. The performance of a malware classifier is correlated with a malware family's use of TLS, i.e., malware families that actively evolve their use of cryptography are more difficult to classify. We conclude that malware's usage of TLS is distinct from benign usage in an enterprise setting, and that these differences can be effectively used in rules and machine learning classifiers.},
archivePrefix = {arXiv},
arxivId = {1607.01639},
author = {Anderson, Blake and Paul, Subharthi and McGrew, David},
doi = {10.1007/s11416-017-0306-6},
eprint = {1607.01639},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Deciphering Malware's use of TLS (without Decryption).pdf:pdf},
issn = {22638733},
journal = {Journal of Computer Virology and Hacking Techniques},
keywords = {Cryptography},
mendeley-tags = {Cryptography},
pages = {1--17},
title = {{Deciphering malware's use of TLS (without decryption)}},
year = {2017}
}
@article{Aslett2015,
abstract = {Recent advances in cryptography promise to enable secure statistical computation on encrypted data, whereby a limited set of operations can be carried out without the need to first decrypt. We review these homomorphic encryption schemes in a manner accessible to statisticians and machine learners, focusing on pertinent limitations inherent in the current state of the art. These limitations restrict the kind of statistics and machine learning algorithms which can be implemented and we review those which have been successfully applied in the literature. Finally, we document a high performance R package implementing a recent homomorphic scheme in a general framework.},
archivePrefix = {arXiv},
arxivId = {1508.06574},
author = {Aslett, Louis J. M. and Esperan{\c{c}}a, Pedro M. and Holmes, Chris C.},
eprint = {1508.06574},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/A review of homomorphic encryption and software tools for encrypted statistical machine learning.pdf:pdf},
keywords = {Cryptography,data privacy,encrypted statistical analysis,homomorphic encryption,homomorphic encryption r package},
mendeley-tags = {Cryptography},
pages = {1--21},
title = {{A review of homomorphic encryption and software tools for encrypted statistical machine learning}},
url = {http://arxiv.org/abs/1508.06574},
year = {2015}
}
@article{Belinkov2015,
abstract = {Arabic, Hebrew, and similar languages are typically written without diacritics, leading to ambiguity and posing a major challenge for core language processing tasks like speech recognition. Previous approaches to automatic diacritization employed a variety of machine learning techniques. However, they typically rely on existing tools like morphological analyzers and therefore cannot be easily extended to new genres and languages. We develop a recurrent neural network with long short-term memory layers for predicting diacritics in Arabic text. Our language-independent approach is trained solely from diacritized text without relying on external tools. We show experimentally that our model can rival state-of-the-art methods that have access to additional resources.},
author = {Belinkov, Yonatan and Glass, James},
doi = {10.18653/v1/D15-1274},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Belinkov, Glass - 2015 - Arabic Diacritization with Recurrent Neural Networks.pdf:pdf},
isbn = {9781941643327},
journal = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
number = {September},
pages = {2281--2285},
title = {{Arabic Diacritization with Recurrent Neural Networks}},
year = {2015}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
issn = {09205691},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Ficler2017,
abstract = {Most work on neural natural language generation (NNLG) focus on controlling the content of the generated text. We experiment with controlling several stylistic aspects of the generated text, in addition to its content. The method is based on conditioned RNN language model, where the desired content as well as the stylistic parameters serve as conditioning contexts. We demonstrate the approach on the movie reviews domain and show that it is successful in generating coherent sentences corresponding to the required linguistic style and content.},
archivePrefix = {arXiv},
arxivId = {1707.02633},
author = {Ficler, Jessica and Goldberg, Yoav},
eprint = {1707.02633},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Controlling Linguistic Style Aspects in Neural Language Generation.pdf:pdf},
number = {section 3},
title = {{Controlling Linguistic Style Aspects in Neural Language Generation}},
url = {http://arxiv.org/abs/1707.02633},
year = {2017}
}
@article{Ghosh2015,
abstract = {—Despite the ubiquity of mobile and wearable text messaging applications, the problem of keyboard text decoding is not tackled sufficiently in the light of the enormous success of the deep learning Recurrent Neural Network (RNN) and Convolutional Neural Networks (CNN) for natural language understanding. In particular, considering that the keyboard decoders should operate on devices with memory and processor resource constraints, makes it challenging to deploy industrial scale deep neural network (DNN) models. This paper proposes a sequence-to-sequence neural attention network system for automatic text correction and completion. Given an erroneous sequence, our model encodes character level hidden representations and then decodes the revised sequence thus enabling auto-correction and completion. Further, what makes the problem different from vanilla language modelling is the simultaneous text correction and completion. We achieve this by a combination of character level CNN and gated recurrent unit (GRU) encoder along with and a word level gated recurrent unit (GRU) attention decoder. Unlike traditional language models that learn from billions of words, our corpus size is only 12 million words; an order of magnitude smaller. The memory footprint of our learnt model for inference and prediction is also an order of magnitude smaller than the conventional language model based text decoders. We report baseline performance for neural keyboard decoders in such limited domain. Our models achieve a word level accuracy of 90{\%} and a character error rate CER of 2.4 over the Twitter typo dataset. We present a novel dataset of noisy to corrected mappings by inducing the noise distribution from the Twitter data over the OpenSubtitles 2009 dataset; on which our model predicts with a word level accuracy of 98{\%} and sequence accuracy of 0.689. We have also conducted an user study from 8 users, with our model predicting with an average CER of 2.6{\%} while being competitive with the state-of-the-art non-neural touch-screen keyboard decoders at CER of 1.6{\%}. We observe a CER of 2.1{\%} on physical keyboard based decoding. Further, we plan to release the training dataset and the associated software along with the baselines as an open-source tool-kit. We also propose an alternative smooth evaluation measure over the character error rate (CER) for evaluating model predictions based on the contextual underlying intent of the sentence. Additionally, we have released our trained decoder as an inference server available at www-edc.eng.cam.ac.uk/shaona.},
archivePrefix = {arXiv},
arxivId = {1709.06429},
author = {Ghosh, Shaona and Kristensson, Ola},
eprint = {1709.06429},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Neural Networks for Text Correction and Completion in Keyboard Decoding.pdf:pdf},
keywords = {Artificial Neural Networks,Convolution Neural Networks,Decoding,Index Terms—Machine Learning,Recurrent Neural Networks,Supervised Learning},
number = {8},
pages = {1--14},
title = {{Neural Networks for Text Correction and Completion in Keyboard Decoding}},
volume = {14},
year = {2015}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {1308.0850},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {1308.0850},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Graves - 2013 - Generating Sequences With Recurrent Neural Networks.pdf:pdf},
isbn = {2000201075},
issn = {18792782},
pages = {1--43},
pmid = {23459267},
title = {{Generating Sequences With Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{HendySusanto,
abstract = {In this paper, we investigate case restoration for text without case information. Previous such work operates at the word level. We pro-pose an approach using character-level recur-rent neural networks (RNN), which performs competitively compared to language model-ing and conditional random fields (CRF) ap-proaches. We further provide quantitative and qualitative analysis on how RNN helps im-prove truecasing.},
author = {{Hendy Susanto}, Raymond and {Leong Chieu}, Hai and Lu, Wei},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Hendy Susanto, Leong Chieu, Lu - Unknown - Learning to Capitalize with Character-Level Recurrent Neural Networks An Empirical Study.pdf:pdf},
pages = {2090--2095},
title = {{Learning to Capitalize with Character-Level Recurrent Neural Networks: An Empirical Study}}
}
@article{Hwang2017,
abstract = {Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling out-of-vocabulary words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different timescales. Despite the multi-timescale structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word Benchmark with only 2{\%} of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30{\%}.},
archivePrefix = {arXiv},
arxivId = {1609.03777},
author = {Hwang, Kyuyeon and Sung, Wonyong},
doi = {10.1109/ICASSP.2017.7953252},
eprint = {1609.03777},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/CHARACTER-LEVEL LANGUAGE MODELING WITH HIERARCHICAL RECURRENT NEURAL NETWORKS.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Character-level language model,hierarchical recurrent neural network,long short-term memory},
pages = {5720--5724},
title = {{Character-level language modeling with hierarchical recurrent neural networks}},
year = {2017}
}
@article{Kim2015,
abstract = {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60{\%} fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.},
annote = {Input:
Output:
Data Representation:

طه يقرأ هذه الورقة},
archivePrefix = {arXiv},
arxivId = {1508.06615},
author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
doi = {2},
eprint = {1508.06615},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Character-Aware Neural Language Models.pdf:pdf},
isbn = {9781577357605},
issn = {14814374},
pmid = {15003161},
title = {{Character-Aware Neural Language Models}},
url = {http://arxiv.org/abs/1508.06615},
year = {2015}
}
@article{Kowsari2017,
abstract = {The continually increasing number of documents produced each year necessitates ever improving information processing methods for searching, retrieving, and organizing text. Central to these information processing methods is document classification, which has become an important application for supervised learning. Recently the performance of these traditional classifiers has degraded as the number of documents has increased. This is because along with this growth in the number of documents has come an increase in the number of categories. This paper approaches this problem differently from current document classification methods that view the problem as multi-class classification. Instead we perform hierarchical classification using an approach we call Hierarchical Deep Learning for Text classification (HDLTex). HDLTex employs stacks of deep learning architectures to provide specialized understanding at each level of the document hierarchy.},
archivePrefix = {arXiv},
arxivId = {1709.08267},
author = {Kowsari, Kamran and Brown, Donald E. and Heidarysafa, Mojtaba and Meimandi, Kiana Jafari and Gerber, Matthew S. and Barnes, Laura E.},
eprint = {1709.08267},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Kowsari et al. - 2017 - HDLTex Hierarchical Deep Learning for Text Classification.pdf:pdf},
title = {{HDLTex: Hierarchical Deep Learning for Text Classification}},
year = {2017}
}
@article{Lai2015,
abstract = {Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent con-volutional neural network for text classification with-out human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to cap-ture the key components in texts. We conduct experi-ments on four commonly used datasets. The experimen-tal results show that the proposed method outperforms the state-of-the-art methods on several datasets, partic-ularly on document-level datasets.},
author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Lai et al. - 2015 - Recurrent Convolutional Neural Networks for Text Classification.pdf:pdf},
isbn = {9781577357018},
journal = {Twenty-Ninth AAAI Conference on Artificial Intelligence},
keywords = {NLP and Machine Learning Track},
pages = {2267--2273},
title = {{Recurrent Convolutional Neural Networks for Text Classification}},
year = {2015}
}
@article{Lee2016,
abstract = {Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of BLEU score and human judgment.},
archivePrefix = {arXiv},
arxivId = {1610.03017},
author = {Lee, Jason and Cho, Kyunghyun and Hofmann, Thomas},
eprint = {1610.03017},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Cho, Hofmann - 2016 - Fully Character-Level Neural Machine Translation without Explicit Segmentation.pdf:pdf},
isbn = {9781510827585},
title = {{Fully Character-Level Neural Machine Translation without Explicit Segmentation}},
url = {https://arxiv.org/pdf/1603.06147.pdf},
year = {2016}
}
@article{Lee2017,
abstract = {Personal names tend to have many variations differ-ing from country to country. Though there exists a large amount of personal names on the Web, na-tionality prediction solely based on names has not been fully studied due to its difficulties in extracting subtle character level features. We propose a recur-rent neural network based model which predicts na-tionalities of each name using automatic feature ex-traction. Evaluation of Olympic record data shows that our model achieves greater accuracy than pre-vious feature based approaches in nationality pre-diction tasks. We also evaluate our proposed model and baseline models on name ethnicity classifica-tion task, again achieving better or comparable per-formances. We further investigate the effective-ness of character embeddings used in our proposed model.},
annote = {I Will read and omar this paper},
author = {Lee, Jinhyuk and Kim, Hyunjae and Ko, Miyoung and Choi, Donghee and Choi, Jaehoon and Korea, Jaewoo Kang},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2017 - Name Nationality Classification with Recurrent Neural Networks.pdf:pdf},
keywords = {Machine Learning: Classification,Machine Learning: Deep Learning,Machine Learning: Neural Networks,Use RNN,Use character embeddings,Use n-gram models on char-level},
mendeley-tags = {Use RNN,Use character embeddings,Use n-gram models on char-level},
title = {{Name Nationality Classification with Recurrent Neural Networks}},
year = {2017}
}
@article{Lewis2016,
abstract = {In this work, we propose that a pre-processing method for changing text data to conform closer to the distribution of standard English will help increase the perfor-mance of many state-of-the-art NLP models and algorithms when confronted with data taken " from the wild " . Our system receives as input a text word, sentence or paragraph which we assume contains (possibly none) random corruptions; for-mally, we say that the input comes from a corrupted language domain that is a superset of our target language domain. Our system then processes this input and outputs a " translation " or " projection " to our target language domain, with the goal of the output being to preserve the latent properties of the input text (senti-ment, named entities, etc.) but mutated in a way that embeds these properties in a representation familiar to other NLP systems.},
annote = {reading by Omar {\&} Abdullah (1)},
author = {Lewis, Gene},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Lewis - 2016 - Sentence Correction using Recurrent Neural Networks.pdf:pdf},
keywords = {Char-RNN,English text,LSTM,NLP to translation,Text using RNN,mini-batch,one-hot encoding,softmax},
mendeley-tags = {Char-RNN,English text,LSTM,NLP to translation,Text using RNN,mini-batch,one-hot encoding,softmax},
title = {{Sentence Correction using Recurrent Neural Networks}},
url = {https://cs224d.stanford.edu/reports/Lewis.pdf},
year = {2016}
}
@article{Liu2016,
abstract = {Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multi-task learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks.},
archivePrefix = {arXiv},
arxivId = {1605.05101},
author = {Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
eprint = {1605.05101},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Qiu, Huang - 2016 - Recurrent Neural Network for Text Classification with Multi-Task Learning.pdf:pdf},
isbn = {978-1-57735-770-4},
title = {{Recurrent Neural Network for Text Classification with Multi-Task Learning}},
year = {2016}
}
@article{Lopyrev2015,
abstract = {We describe an application of an encoder-decoder recurrent neural network with LSTM units and attention to generating headlines from the text of news articles. We find that the model is quite effective at concisely paraphrasing news articles. Furthermore, we study how the neural network decides which input words to pay attention to, and specifically we identify the function of the different neurons in a simplified attention mechanism. Interestingly, our simplified attention mechanism performs better that the more complex attention mechanism on a held out set of articles.},
archivePrefix = {arXiv},
arxivId = {1512.01712},
author = {Lopyrev, Konstantin},
eprint = {1512.01712},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Lopyrev - 2015 - Generating News Headlines with Recurrent Neural Networks.pdf:pdf},
title = {{Generating News Headlines with Recurrent Neural Networks}},
url = {https://arxiv.org/pdf/1512.01712.pdf},
year = {2015}
}
@article{Martens2011,
abstract = {Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely diffi- cult to train them properly. Fortunately, re- cent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence prob- lems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free op- timizer (HF) by applying them to character-level language modeling tasks. The standard RNN ar- chitecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or gated) con- nections which allow the current input charac- ter to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character- level language modeling a hierarchical non- parametric sequence model. To our knowledge this represents the largest recurrent neural net- work application to date.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Martens, James},
doi = {2},
eprint = {9809069v1},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Generating Text with Recurrent Neural Networks.pdf:pdf},
isbn = {9781450306195},
issn = {1},
journal = {Neural Networks},
number = {1},
pages = {1017--1024},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Generating Text with Recurrent Neural Networks}},
url = {http://www.icml-2011.org/papers/524{\_}icmlpaper.pdf},
volume = {131},
year = {2011}
}
@article{Prieto2017,
abstract = {The pervasive presence of interconnected objects enables new communication paradigms where devices can easily reach each other while interacting within their environment. The so-called Internet of Things (IoT) represents the integration of several computing and communications systems aiming at facilitating the interaction between these devices. Arduino is one of the most popular platforms used to prototype new IoT devices due to its open, flexible and easy-to-use archi- tecture. Ardunio Yun is a dual board microcontroller that supports a Linux distribution and it is currently one of the most versatile and powerful Arduino systems. This feature positions Arduino Yun as a popular platform for developers, but it also introduces unique infection vectors from the secu- rity viewpoint. In this work, we present a security analysis of Arduino Yun. We show that Arduino Yun is vulnerable to a number of attacks and we implement a proof of concept capable of exploiting some of them.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.07016v1},
author = {Prieto, Luis P. and Rodr{\'{i}}guez-Triana, Mar{\'{i}}a Jes{\'{u}}s and Kusmin, Marge and Laanpere, Mart},
doi = {10.1145/1235},
eprint = {arXiv:1603.07016v1},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Learning text representation using recurrent convolutional neural network with highway layers.pdf:pdf},
isbn = {9781450321389},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Multimodal learning analytics,Multimodal teaching analytics,STEM education,Sensors,Smart classroom,Smart school},
pages = {53--59},
title = {{Smart school multimodal dataset and challenges}},
volume = {1828},
year = {2017}
}
@article{Tizhoosh2008,
abstract = {Poetry is a form of art that is used to express emotions and feelings. Humans can easily distinguish poetry without any sophisticated tools. This study is concerned with developing intelligent methods which can be used to distinguish poem from prose. The goal is to distinguish and extract effective poetic features with which poems/lyrics can be accurately classified from other type of texts. In this paper, we propose five different approaches to poem classification. In each approach, we extracted a different set of poetic features and evaluated their performances against each other. In addition, we empirically assessed the effectiveness of traditional text classification methods for poem recognition and compared it with the proposed poetic features. While all of these approaches performed well, some showed superior results. Findings of this study suggest thatthe proposed features generate highly accurate classifiers, which can be used for poem mining in large databases such as WorldWide Web.},
author = {Tizhoosh, Hamid R and Dara, Rozita},
doi = {10.13176/11.62},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Tizhoosh, Dara - 2008 - Poetic Features for Poem Recognition A Comparative Study.pdf:pdf},
issn = {1558884X},
journal = {Design Engineering},
keywords = {English poem},
number = {April 2014},
pages = {24--39},
title = {{Poetic Features for Poem Recognition : A Comparative Study}},
url = {https://www.researchgate.net/profile/Hamid{\_}Tizhoosh/publication/228375375{\_}Poetic{\_}Features{\_}for{\_}Poem{\_}Recognition{\_}A{\_}Comparative{\_}Study/links/0a85e535713f57e109000000.pdf},
year = {2008}
}
@article{Wu2013,
abstract = {Although temporal information of speech has been shown to play an important role in perception, most of the voice conversion approaches assume the speech frames are independent of each other, thereby ignoring the temporal information. In this study, we improve conventional unit selection approach by using exemplars which span multiple frames as base units, and also take temporal information constraint into voice conversion by using overlapping frames to generate speech parameters. This approach thus provides more stable concatenation cost and avoids discontinuity problem in conventional unit selection approach. The proposed method also keeps away from the over-smoothing problem in the mainstream joint density Gaussian mixture model (JD-GMM) based conversion method by directly using target speaker's training data for synthesizing the converted speech. Both objective and subjective evaluations indicate that our proposed method outperforms JD-GMM and conventional unit selection methods. Copyright {\textcopyright} 2013 ISCA.},
archivePrefix = {arXiv},
arxivId = {1502.01710},
author = {Wu, Zhizheng and Virtanen, Tuomas and Kinnunen, Tomi and Chng, Eng Siong and Li, Haizhou},
doi = {10.1063/1.4906785},
eprint = {1502.01710},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2013 - Exemplar-based unit selection for voice conversion utilizing temporal Information.pdf:pdf},
isbn = {0123456789},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Multi-frame exemplar,Temporal information,Unit selection,Voice conversion},
pages = {3057--3061},
pmid = {25246403},
title = {{Exemplar-based unit selection for voice conversion utilizing temporal Information}},
url = {http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf},
year = {2013}
}
@article{Xie,
abstract = {Text generation is a foundational task in natural language processing, forming the core of a diverse set of practical applications ranging from image captioning and text summarization to question answering. However, most of this work has focused on generating prose. We investigate whether deep learning systems can be used to synthesize poetry, in particular Shakespearean-styled works. Previous work on generating Shakespeare prose involved training models exclusively on the word or character level. Here, we implement those previous models for poetry generation and show that models that combine word and character level informa-tion, such as a Gated LSTM and a CNN-based LSTM, significantly outperform the baseline word-LSTM and char-LSTM models. Perplexity scores for the two complex models are almost 10 fold better than that for our baselines, and human ratings of the model-generated sonnets reflect this as well. In particular, the son-nets our complex models generate have a coherent meaning and relatively correct meter without blatantly copying Shakespeare's original works. These results en-courage us that models that blend word and character level information would be useful for a variety of tasks outside of just poetry generation and may be crucial in bridging the gap between computer generated and human written text.},
author = {Xie, Stanley and Rastogi, Ruchir and Chang, Max},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Xie, Rastogi, Chang - Unknown - Deep Poetry Word-Level and Character-Level Language Models for Shakespearean Sonnet Generation.pdf:pdf},
title = {{Deep Poetry: Word-Level and Character-Level Language Models for Shakespearean Sonnet Generation}}
}
@article{Xu2016,
abstract = {Although semi-supervised variational autoencoder (SemiVAE) works in image classification task, it fails in text classification task if using vanilla LSTM as its decoder. From a perspective of reinforcement learning, it is verified that the decoder's capability to distinguish between different categorical labels is essential. Therefore, Semi-supervised Sequential Variational Autoencoder (SSVAE) is proposed, which increases the capability by feeding label into its decoder RNN at each time-step. Two specific decoder structures are investigated and both of them are verified to be effective. Besides, in order to reduce the computational complexity in training, a novel optimization method is proposed, which estimates the gradient of the unlabeled objective function by sampling, along with two variance reduction techniques. Experimental results on Large Movie Review Dataset (IMDB) and AG's News corpus show that the proposed approach significantly improves the classification accuracy compared with pure-supervised classifiers, and achieves competitive performance against previous advanced methods. State-of-the-art results can be obtained by integrating other pretraining-based methods.},
archivePrefix = {arXiv},
arxivId = {1603.02514},
author = {Xu, Weidi and Sun, Haoze and Deng, Chao and Tan, Ying},
eprint = {1603.02514},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Variational Autoencoder for Semi-supervised Text Classification.pdf:pdf},
title = {{Variational Autoencoders for Semi-supervised Text Classification}},
url = {http://arxiv.org/abs/1603.02514},
year = {2016}
}
@article{Yan2016,
abstract = {Part of the long lasting cultural heritage of human-ity is the art of classical poems, which are created by fitting words into certain formats and representa-tions. Automatic poetry composition by computers is considered as a challenging problem which re-quires high Artificial Intelligence assistance. This study attracts more and more attention in the re-search community. In this paper, we formulate the poetry composition task as a natural language gen-eration problem using recurrent neural networks. Given user specified writing intents, the system generates a poem via sequential language model-ing. Unlike the traditional one-pass generation for previous neural network models, poetry composi-tion needs polishing to satisfy certain requirements. Hence, we propose a new generative model with a polishing schema, and output a refined poem com-position. In this way, the poem is generated incre-mentally and iteratively by refining each line. We run experiments based on large datasets of 61,960 classic poems in Chinese. A comprehensive eval-uation, using perplexity and BLEU measurements as well as human judgments, has demonstrated the effectiveness of our proposed approach.},
author = {Yan, Rui},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Automatic Poetry Composition through Recurrent Neural Networks with Iterative Polishing Schema.pdf:pdf},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {2238--2244},
title = {{I, Poet: Automatic poetry composition through recurrent neural networks with iterative polishing schema}},
volume = {2016-Janua},
year = {2016}
}
@article{Yogatama2017,
abstract = {We empirically characterize the performance of discriminative and generative LSTM models for text classification. We find that although RNN-based generative models are more powerful than their bag-of-words ancestors (e.g., they account for conditional dependencies across words in a document), they have higher asymptotic error rates than discriminatively trained RNN models. However we also find that generative models approach their asymptotic error rate more rapidly than their discriminative counterparts---the same pattern that Ng {\&} Jordan (2001) proved holds for linear classification models that make more naive conditional independence assumptions. Building on this finding, we hypothesize that RNN-based generative classification models will be more robust to shifts in the data distribution. This hypothesis is confirmed in a series of experiments in zero-shot and continual learning settings that show that generative models substantially outperform discriminative models.},
archivePrefix = {arXiv},
arxivId = {1703.01898},
author = {Yogatama, Dani and Dyer, Chris and Ling, Wang and Blunsom, Phil},
eprint = {1703.01898},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Generative and Discriminative Text Classification with Recurrent Neural Networks.pdf:pdf},
title = {{Generative and Discriminative Text Classification with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1703.01898},
year = {2017}
}
@article{Zhou2015,
abstract = {Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.},
archivePrefix = {arXiv},
arxivId = {1511.08630},
author = {Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis C. M.},
eprint = {1511.08630},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/A C-LSTM Neural Network for Text Classification.pdf:pdf},
title = {{A C-LSTM Neural Network for Text Classification}},
url = {http://arxiv.org/abs/1511.08630},
year = {2015}
}
@article{Eldesouki,
archivePrefix = {arXiv},
arxivId = {arXiv:1708.05891v1},
author = {Eldesouki, Mohamed and Samih, Younes and Abdelali, Ahmed and Attia, Mohammed and Mubarak, Hamdy and Darwish, Kareem and Kallmeyer, Laura and City, New York},
eprint = {arXiv:1708.05891v1},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Arabic Multi-Dialect Segmentation- bi-LSTM-CRF vs. SVM.pdf:pdf},
title = {{Arabic Multi-Dialect Segmentation: bi-LSTM-CRF vs. SVM}}
}
@article{Hopkins2017,
abstract = {We propose two novel methodologies for the automatic generation of rhythmic po-etry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic de-vices such as rhyme, rhythm and allitera-tion. The second approach considers po-etry generation as a constraint satisfac-tion problem where a generative neural language model is tasked with learning a representation of content, and a discrimi-native weighted finite state machine con-strains it on the basis of form. By manip-ulating the constraints of the latter model, we can generate coherent poetry with arbi-trary forms and themes. A large-scale ex-trinsic evaluation demonstrated that partic-ipants consider machine-generated poems to be written by humans 54{\%} of the time. In addition, participants rated a machine-generated poem to be the most human-like amongst all evaluated.},
annote = {reading by Omar {\&} abdullah (2)},
author = {Hopkins, Jack},
doi = {10.18653/v1/P17-1016},
file = {:Users/taha/Library/Application Support/Mendeley Desktop/Downloaded/Hopkins - 2017 - Automatically Generating Rhythmic Verse with Neural Networks.pdf:pdf},
journal = {Acl},
keywords = {Character Language Model,Generating Rhythmic,LSTM,Orthographic decoding,RNN,Rhythm Modeling,Use LSTM to train,Use phonetic encoding to learn,english poetry,one-hot encoding},
mendeley-tags = {Character Language Model,Generating Rhythmic,LSTM,Orthographic decoding,RNN,Rhythm Modeling,Use LSTM to train,Use phonetic encoding to learn,english poetry,one-hot encoding},
pages = {168--178},
title = {{Automatically Generating Rhythmic Verse with Neural Networks}},
url = {http://www.aclweb.org/anthology/P17-1016},
year = {2017}
}
@article{Greydanus2017,
abstract = {Recurrent neural networks (RNNs) represent the state of the art in translation, image captioning, and speech recognition. They are also capable of learning algorithmic tasks such as long addition, copying, and sorting from a set of training examples. We demonstrate that RNNs can learn decryption algorithms -- the mappings from plaintext to ciphertext -- for three polyalphabetic ciphers (Vigen$\backslash$`ere, Autokey, and Enigma). Most notably, we demonstrate that an RNN with a 3000-unit Long Short-Term Memory (LSTM) cell can learn the decryption function of the Enigma machine. We argue that our model learns efficient internal representations of these ciphers 1) by exploring activations of individual memory neurons and 2) by comparing memory usage across the three ciphers. To be clear, our work is not aimed at 'cracking' the Enigma cipher. However, we do show that our model can perform elementary cryptanalysis by running known-plaintext attacks on the Vigen$\backslash$`ere and Autokey ciphers. Our results indicate that RNNs can learn algorithmic representations of black box polyalphabetic ciphers and that these representations are useful for cryptanalysis.},
annote = {Input: Polyalphabetic Encrypted Text
Output: Decrypted Text
Data Representation: One-Hot Vector},
archivePrefix = {arXiv},
arxivId = {1708.07576},
author = {Greydanus, Sam},
eprint = {1708.07576},
file = {:Users/taha/ComputerScience/الترم الحالي/GP/Papers/Learning the Enigma with Recurrent Neural Networks .pdf:pdf},
keywords = {Cryptography,RNN},
mendeley-tags = {Cryptography,RNN},
title = {{Learning the Enigma with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1708.07576},
year = {2017}
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluating the model with Recall, Precisiona and F1 Score\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def recall_precision_f1(confusionMatrix_DF):\n",
    "    '''\n",
    "    Args:\n",
    "        confusionMatrix_DF: a datafram with index_col=0\n",
    "        \n",
    "    returns: (x, y)\n",
    "        x: is a datafrom of recall and precision for every class\n",
    "        y: is the f1 score for the model.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    confusionMatrix_np  = confusionMatrix_DF.values\n",
    "    bahr = 0\n",
    "    sum_rows = []\n",
    "    sum_columns = []\n",
    "    diagonal_recall = []\n",
    "    diagonal_precision = []\n",
    "    \n",
    "    matrices = [confusionMatrix_np, np.transpose(confusionMatrix_np)]\n",
    "    flag = 0\n",
    "    for matrix in matrices:\n",
    "        # print(matrix)\n",
    "        for x in matrix:\n",
    "            # class_num is the sum of the ith row of the confusion matrix\n",
    "            # Also it it the sum of the ith column, when flag = 1\n",
    "            class_sum = np.sum(x)\n",
    "            # Recall\n",
    "            if flag == 0:\n",
    "                sum_rows.append(class_sum)\n",
    "                diagonal_recall.append(x[bahr])\n",
    "            # Precision\n",
    "            elif flag == 1:\n",
    "                sum_columns.append(class_sum)\n",
    "                diagonal_precision.append(x[bahr])\n",
    "            \n",
    "            bahr += 1 \n",
    "        flag += 1\n",
    "        bahr = 0\n",
    "    '''\n",
    "    # Recall per class\n",
    "    print(np.array(diagonal_recall)/ np.array(sum_rows))\n",
    "    # Precision per class\n",
    "    print(np.array(diagonal_precision)/ np.array(sum_columns))\n",
    "    '''\n",
    "    recall_per_class = np.array(diagonal_recall)/ np.array(sum_rows)\n",
    "    precision_per_class = np.array(diagonal_precision)/ np.array(sum_columns)\n",
    "    \n",
    "    \n",
    "    recall_mean = np.mean(recall_per_class)\n",
    "    precision_mean = np.mean(precision_per_class)\n",
    "    \n",
    "    sum_rec_pre = recall_mean + precision_mean\n",
    "    mul_rec_re  = recall_mean * precision_mean\n",
    "    f1_score = 2 * (mul_rec_re / sum_rec_pre)\n",
    "    \n",
    "    # Building the Data Frame\n",
    "    resulat_dict = {'Recall': recall_per_class,\n",
    "                    'Precision': precision_per_class\n",
    "                   }\n",
    "    resulat_DF = pd.DataFrame(resulat_dict, dtype=float)\n",
    "    resulat_DF.index = confusionMatrix_DF.index\n",
    "    resulat_DF\n",
    "    \n",
    "    return resulat_DF, f1_score\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8925944521556847\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example\n",
    "df = pd.read_csv('../Results/Exp_1_full_data_matrix_with_tashkeel_8bitsEncoding_Bidirectional_LSTM_3_50_1/Exp_1_full_data_matrix_with_tashkeel_8bitsEncoding_Bidirectional_LSTM_3_50_1.csv', index_col=0)\n",
    "dataFrame, f1 = recall_precision_f1(df)\n",
    "print(f1)\n",
    "#print(dataFrame)\n",
    "#display(dataFrame)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

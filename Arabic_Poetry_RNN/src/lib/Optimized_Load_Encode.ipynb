{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arabic\n",
    "import pyarabic\n",
    "import helpers\n",
    "from helpers import Clean_data,separate_token_with_dicrites,save_h5\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from numpy import array, argmax\n",
    "import os,errno,re,sys\n",
    "import pickle\n",
    "from itertools import product \n",
    "from pyarabic.araby import strip_tashkeel, strip_tatweel\n",
    "import h5py\n",
    "import random as rn\n",
    "##########################\n",
    "rn.seed(123456)\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths created\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * Paths\n",
    "'''\n",
    "all_data_path = \"../../data/Almoso3a_Alshe3rya/data/All_ksaied.csv\"\n",
    "\n",
    "'''\n",
    "    * Creating Directories which will contain the Encoded data.\n",
    "'''\n",
    "try:\n",
    "    os.makedirs(\"../../data/Encoded/8bits/WithoutTashkeel/Eliminated/\")\n",
    "    os.makedirs(\"../../data/Encoded/8bits/WithoutTashkeel/Full_Data/\")\n",
    "    os.makedirs(\"../../data/Encoded/8bits/WithTashkeel/Eliminated/\")\n",
    "    os.makedirs(\"../../data/Encoded/8bits/WithTashkeel/Full_Data/\")\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        print(\"Can't create file for checkpoints or for logs please check \")\n",
    "        raise\n",
    "print(\"Paths created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/numpy/lib/arraysetops.py:463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1831770\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * Load and Clear\n",
    "'''\n",
    "all_data = pd.read_csv(all_data_path, index_col=0)\n",
    "print( len(all_data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1831770 entries, 0 to 1835252\n",
      "Data columns (total 8 columns):\n",
      "العصر           object\n",
      "الشاعر          object\n",
      "الديوان         object\n",
      "القافية         object\n",
      "البحر           object\n",
      "الشطر الايسر    object\n",
      "الشطر الايمن    object\n",
      "البيت           object\n",
      "dtypes: object(8)\n",
      "memory usage: 1.8 GB\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * (X,Y) form of the dataset. [uncleaned]\n",
    "'''\n",
    "all_data.info(memory_usage='deep')\n",
    "bahr = 'Bahr'\n",
    "bayt = 'Bayt_Text'\n",
    "#all_data  = all_data[[bayt, bahr]]\n",
    "bahr = 'البحر'\n",
    "bayt = 'البيت'\n",
    "all_data  = all_data[[bayt, bahr]]\n",
    "all_data.columns = ['Bayt_Text','Bahr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    * Cleaning Data\n",
    "    # Outputs:\n",
    "        $ all_data_cleaned\n",
    "        % Cleaned, and Ready for filtering, encoding\n",
    "'''\n",
    "\n",
    "# Cleaning \n",
    "all_data_cleaned = Clean_data(data_frame=all_data,\n",
    "                              max_bayt_len=115,\n",
    "                              verse_column_name='Bayt_Text')\n",
    "\n",
    "# Computing the maximum bayt length\n",
    "MAX_LEN_BAYT = np.max((all_data_cleaned.Bayt_Text.apply(pyarabic.araby.strip_tashkeel).apply(len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['الوافر', 'المنسرح', 'المجتث', 'المتقارب', 'الكامل', 'الطويل', 'السريع', 'الرمل', 'الرجز', 'الخفيف', 'البسيط']\n",
      "['الوافر', 'المنسرح', 'المديد', 'المجتث', 'المتقارب', 'الكامل', 'الطويل', 'السريع', 'الرمل', 'الرجز', 'الخفيف', 'البسيط', 'المقتضب', 'الهزج', 'المضارع', 'المتدارك']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * Filtering the data\n",
    "    # Outputs:\n",
    "        1 all_filtered_data\n",
    "        2 eliminated_filtered_data\n",
    "'''\n",
    "\n",
    "elminated_classic_bohor = ['الوافر', 'المنسرح','المجتث', 'المتقارب', 'الكامل', 'الطويل', 'السريع', 'الرمل',\n",
    "                         'الرجز', 'الخفيف', 'البسيط']\n",
    "\n",
    "full_bohor_classes = ['الوافر', 'المنسرح', 'المديد', 'المجتث', 'المتقارب', 'الكامل', 'الطويل', 'السريع', 'الرمل',\n",
    "                         'الرجز', 'الخفيف', 'البسيط', 'المقتضب', 'الهزج', 'المضارع', 'المتدارك']\n",
    "\n",
    "print(elminated_classic_bohor)\n",
    "print(full_bohor_classes)\n",
    "\n",
    "all_filtered_data = all_data_cleaned.loc[all_data_cleaned['Bahr'].isin(full_bohor_classes)]\n",
    "eliminated_filtered_data = all_data_cleaned.loc[all_data_cleaned['Bahr'].isin(elminated_classic_bohor)]\n",
    "\n",
    "#all_filtered_data.head().head()\n",
    "#eliminated_filtered_data.head()\n",
    "#full_filtered_data.iloc[:, 1].value_counts()\n",
    "#eliminated_filtered_data.iloc[:, 1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Encode <3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Taha; Encoding Full Data classes as OneHot vectors. \n",
    "'''\n",
    "\n",
    "full_data_label_encoder = LabelEncoder()\n",
    "full_data_integer_encoded = full_data_label_encoder.fit_transform(list(full_filtered_data[\"Bahr\"]))\n",
    "# binary encode\n",
    "full_data_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "full_data_integer_encoded = full_data_integer_encoded.reshape(len(full_data_integer_encoded), 1)\n",
    "full_data_bohor_encoded = full_data_onehot_encoder.fit_transform(full_data_integer_encoded)\n",
    "save_h5('../../data/Encoded/8bits/WithoutTashkeel/Full_Data/full_data_Y_Meters.h5', 'Y', full_data_bohor_encoded)\n",
    "save_h5('../../data/Encoded/8bits/WithTashkeel/Full_Data/full_data_Y_Meters.h5', 'Y', full_data_bohor_encoded)\n",
    "#saving with pickle: https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "print('saving preprocessors....')\n",
    "with open(\"../../encoders_full_dat.pickle\", 'wb') as pre_file:\n",
    "    pickle.dump(full_data_label_encoder, pre_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('saved ....')\n",
    "\n",
    "\n",
    "'''\n",
    "    Taha; This cell is entirly doublicated.\n",
    "    It Encodes the eliminated data classes.\n",
    "'''\n",
    "eliminated_data_label_encoder = LabelEncoder()\n",
    "eliminated_data_integer_encoded = eliminated_data_label_encoder.fit_transform(list(eliminated_filtered_data[\"Bahr\"]))\n",
    "# binary encode\n",
    "eliminated_data_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "eliminated_data_integer_encoded = eliminated_data_integer_encoded.reshape(len(eliminated_data_integer_encoded), 1)\n",
    "eliminated_data_bohor_encoded = eliminated_data_onehot_encoder.fit_transform(eliminated_data_integer_encoded)\n",
    "save_h5('../../data/Encoded/8bits/WithoutTashkeel/Eliminated/Eliminated_data_Y_Meters.h5', 'Y', eliminated_data_bohor_encoded)\n",
    "save_h5('../../data/Encoded/8bits/WithTashkeel/Eliminated/Eliminated_data_Y_Meters.h5', 'Y', eliminated_data_bohor_encoded)\n",
    "#saving with pickle: https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "print('saving preprocessors....')\n",
    "with open(\"../../encoders_eliminated_data.pickle\", 'wb') as pre_file:\n",
    "    pickle.dump(eliminated_data_bohor_encoded, pre_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('saved ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    * Encoding the full data with Tashkeel\n",
    "'''\n",
    "\n",
    "\n",
    "X = all_filtered_data.Bayt_Text.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, MAX_LEN_BAYT))\n",
    "print(\"full_cleaned_data_with_tashkeel_encoded encoded ..\")\n",
    "\n",
    "X_stacked = np.stack(X,axis=0)\n",
    "print(\"stacked!\")\n",
    "\n",
    "save_h5('../../data/Encoded/8bits/WithTashkeel/Full_Data/full_data_matrix_with_tashkeel_8bitsEncoding.h5', 'X', X_stacked) \n",
    "print(\"full_cleaned_data_with_tashkeel_encoded saved ..\")\n",
    "print(X_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    * Encoding the eleiminated data with Tashkeel\n",
    "'''\n",
    "\n",
    "X = eliminated_filtered_data.Bayt_Text.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, MAX_LEN_BAYT))\n",
    "print(\"eliminated_cleaned_data_with_tashkeel_encoded encoded ..\")\n",
    "\n",
    "X_stacked = np.stack(X,axis=0)\n",
    "save_h5('../../data/Encoded/8bits/WithTashkeel/Eliminated/eliminated_data_matrix_with_tashkeel_8bitsEncoding.h5', 'X', X_stacked)\n",
    "\n",
    "print(\"full_cleaned_data_with_tashkeel_encoded saved ..\")\n",
    "print(X_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping Taskeel\n",
    "all_data_cleaned = all_data_cleaned.apply(pyarabic.araby.strip_tashkeel)\n",
    "all_filtered_data = all_data_cleaned.loc[all_data_cleaned['Bahr'].isin(full_bohor_classes)]\n",
    "eliminated_filtered_data = all_data_cleaned.loc[all_data_cleaned['Bahr'].isin(elminated_classic_bohor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    * Encoding all data without Tashkeel\n",
    "'''\n",
    "X = all_filtered_data.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, MAX_LEN_BAYT))\n",
    "print(\"full_filtered_data_without_tashkeel encoded ..\")\n",
    "\n",
    "X_stacked = np.stack(X, axis=0)\n",
    "save_h5('../../data/Encoded/8bits/WithoutTashkeel/Full_Data/full_data_matrix_without_tashkeel_8bitsEncoding.h5', 'X', X_stacked) \n",
    "print(\"full_filtered_data_without_tashkeel_staked saved ..\")\n",
    "\n",
    "print(full_filtered_data_without_tashkeel_staked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    * Encoding all data without Tashkeel\n",
    "'''\n",
    "X = eliminated_filtered_data_without_tashkeel.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, MAX_LEN_BAYT))\n",
    "print(\"eliminated_filtered_data_without_tashkeel encoded ..\")\n",
    "\n",
    "X_stacked = np.stack(X, axis=0)\n",
    "save_h5('../../data/Encoded/8bits/WithoutTashkeel/Eliminated/eliminated_data_matrix_without_tashkeel_8bitsEncoding.h5', 'X', X_stacked) \n",
    "print(\"eliminated_filtered_data_without_tashkeel_staked saved ..\")\n",
    "\n",
    "print(eliminated_filtered_data_without_tashkeel_staked.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arabic\n",
    "import pyarabic\n",
    "import helpers\n",
    "from helpers import Clean_data,separate_token_with_dicrites,save_h5\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from numpy import array, argmax\n",
    "import os,errno,re,sys\n",
    "import pickle\n",
    "from itertools import product \n",
    "from pyarabic.araby import strip_tashkeel, strip_tatweel\n",
    "import h5py\n",
    "import random as rn\n",
    "##########################\n",
    "rn.seed(123456)\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths created\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * Paths\n",
    "'''\n",
    "all_data_path = \"../../data/Almoso3a_Alshe3rya/data/All_ksaied.csv\"\n",
    "all_data_cleaned_path = \"../../data/Almoso3a_Alshe3rya/data/All_ksaied_cleaned.csv\"\n",
    "\n",
    "'''\n",
    "    * Creating Directories which will contain the Encoded data.\n",
    "'''\n",
    "try:\n",
    "    os.makedirs(\"../../data/Encoded/8bits/WithoutTashkeel/Eliminated/\")\n",
    "    os.makedirs(\"../../data/Encoded/8bits/WithoutTashkeel/Full_Data/\")\n",
    "    os.makedirs(\"../../data/Encoded/8bits/WithTashkeel/Eliminated/\")\n",
    "    os.makedirs(\"../../data/Encoded/8bits/WithTashkeel/Full_Data/\")\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        print(\"Can't create file for checkpoints or for logs please check \")\n",
    "        raise\n",
    "print(\"Paths created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1831770\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * Load and Clear\n",
    "'''\n",
    "all_data = pd.read_csv(all_data_path, index_col=0)\n",
    "print( len(all_data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1831770 entries, 0 to 1835252\n",
      "Data columns (total 8 columns):\n",
      "العصر           object\n",
      "الشاعر          object\n",
      "الديوان         object\n",
      "القافية         object\n",
      "البحر           object\n",
      "الشطر الايسر    object\n",
      "الشطر الايمن    object\n",
      "البيت           object\n",
      "dtypes: object(8)\n",
      "memory usage: 1.8 GB\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * (X,Y) form of the dataset. [uncleaned]\n",
    "'''\n",
    "all_data.info(memory_usage='deep')\n",
    "bahr = 'Bahr'\n",
    "bayt = 'Bayt_Text'\n",
    "#all_data  = all_data[[bayt, bahr]]\n",
    "bahr = 'البحر'\n",
    "bayt = 'البيت'\n",
    "all_data  = all_data[[bayt, bahr]]\n",
    "all_data.columns = ['Bayt_Text','Bahr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    * Cleaning Data\n",
    "    # Outputs:\n",
    "        $ all_data_cleaned\n",
    "        % Cleaned, and Ready for filtering, encoding\n",
    "'''\n",
    "\n",
    "# Computing the maximum bayt length\n",
    "MAX_LEN_BAYT = np.max((all_data.Bayt_Text.apply(pyarabic.araby.strip_tashkeel).apply(len)))\n",
    "\n",
    "\n",
    "# Cleaning \n",
    "all_data_cleaned = Clean_data(data_frame=all_data,\n",
    "                              max_bayt_len=MAX_LEN_BAYT,\n",
    "                              verse_column_name='Bayt_Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tatwieel and tanween + shadda\n",
    "all_data_cleaned['Bayt_Text'] = all_data_cleaned['Bayt_Text'].apply(pyarabic.araby.strip_tatweel)\n",
    "all_data_cleaned['Bayt_Text'] = all_data_cleaned['Bayt_Text'].apply(helpers.factor_shadda_tanwin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data.\n",
    "all_data_cleaned.to_csv(all_data_cleaned_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * Load the cleaned Data\n",
    "'''\n",
    "all_data_cleaned = pd.read_csv(all_data_cleaned_path)\n",
    "all_data_cleaned = all_data_cleaned.dropna()\n",
    "\n",
    "# Computing the maximum bayt length\n",
    "MAX_LEN_BAYT = np.max((all_data_cleaned.Bayt_Text.apply(pyarabic.araby.strip_tashkeel).apply(len)))\n",
    "print(MAX_LEN_BAYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['الوافر', 'المنسرح', 'المجتث', 'المتقارب', 'الكامل', 'الطويل', 'السريع', 'الرمل', 'الرجز', 'الخفيف', 'البسيط']\n",
      "['الوافر', 'المنسرح', 'المديد', 'المجتث', 'المتقارب', 'الكامل', 'الطويل', 'السريع', 'الرمل', 'الرجز', 'الخفيف', 'البسيط', 'المقتضب', 'الهزج', 'المضارع', 'المتدارك']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * Filtering the data\n",
    "    # Outputs:\n",
    "        1 all_filtered_data\n",
    "        2 eliminated_filtered_data\n",
    "'''\n",
    "\n",
    "elminated_classic_bohor = ['الوافر', 'المنسرح','المجتث', 'المتقارب', 'الكامل', 'الطويل', 'السريع', 'الرمل',\n",
    "                         'الرجز', 'الخفيف', 'البسيط']\n",
    "\n",
    "full_bohor_classes = ['الوافر', 'المنسرح', 'المديد', 'المجتث', 'المتقارب', 'الكامل', 'الطويل', 'السريع', 'الرمل',\n",
    "                         'الرجز', 'الخفيف', 'البسيط', 'المقتضب', 'الهزج', 'المضارع', 'المتدارك']\n",
    "\n",
    "print(elminated_classic_bohor)\n",
    "print(full_bohor_classes)\n",
    "\n",
    "all_filtered_data = all_data_cleaned.loc[all_data_cleaned['Bahr'].isin(full_bohor_classes)]\n",
    "eliminated_filtered_data = all_data_cleaned.loc[all_data_cleaned['Bahr'].isin(elminated_classic_bohor)]\n",
    "\n",
    "#all_filtered_data.head().head()\n",
    "#eliminated_filtered_data.head()\n",
    "#full_filtered_data.iloc[:, 1].value_counts()\n",
    "#eliminated_filtered_data.iloc[:, 1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Encode <3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving preprocessors....\n",
      "saved ....\n",
      "saving preprocessors....\n",
      "saved ...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Taha; Encoding Full Data classes as OneHot vectors. \n",
    "'''\n",
    "\n",
    "full_data_label_encoder = LabelEncoder()\n",
    "full_data_integer_encoded = full_data_label_encoder.fit_transform(list(all_filtered_data[\"Bahr\"]))\n",
    "# binary encode\n",
    "full_data_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "full_data_integer_encoded = full_data_integer_encoded.reshape(len(full_data_integer_encoded), 1)\n",
    "full_data_bohor_encoded = full_data_onehot_encoder.fit_transform(full_data_integer_encoded)\n",
    "save_h5('../../data/Encoded/8bits/WithoutTashkeel/Full_Data/full_data_Y_Meters.h5', 'Y', full_data_bohor_encoded)\n",
    "save_h5('../../data/Encoded/8bits/WithTashkeel/Full_Data/full_data_Y_Meters.h5', 'Y', full_data_bohor_encoded)\n",
    "#saving with pickle: https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "print('saving preprocessors....')\n",
    "with open(\"../../encoders_full_dat.pickle\", 'wb') as pre_file:\n",
    "    pickle.dump(full_data_label_encoder, pre_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('saved ....')\n",
    "\n",
    "\n",
    "'''\n",
    "    Taha; This cell is entirly doublicated.\n",
    "    It Encodes the eliminated data classes.\n",
    "'''\n",
    "eliminated_data_label_encoder = LabelEncoder()\n",
    "eliminated_data_integer_encoded = eliminated_data_label_encoder.fit_transform(list(eliminated_filtered_data[\"Bahr\"]))\n",
    "# binary encode\n",
    "eliminated_data_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "eliminated_data_integer_encoded = eliminated_data_integer_encoded.reshape(len(eliminated_data_integer_encoded), 1)\n",
    "eliminated_data_bohor_encoded = eliminated_data_onehot_encoder.fit_transform(eliminated_data_integer_encoded)\n",
    "save_h5('../../data/Encoded/8bits/WithoutTashkeel/Eliminated/Eliminated_data_Y_Meters.h5', 'Y', eliminated_data_bohor_encoded)\n",
    "save_h5('../../data/Encoded/8bits/WithTashkeel/Eliminated/Eliminated_data_Y_Meters.h5', 'Y', eliminated_data_bohor_encoded)\n",
    "#saving with pickle: https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "print('saving preprocessors....')\n",
    "with open(\"../../encoders_eliminated_data.pickle\", 'wb') as pre_file:\n",
    "    pickle.dump(eliminated_data_bohor_encoded, pre_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('saved ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_cleaned_data_with_tashkeel_encoded encoded ..\n",
      "stacked!\n",
      "full_cleaned_data_with_tashkeel_encoded saved ..\n",
      "(1691636, 111, 8)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * Encoding the full data with Tashkeel\n",
    "'''\n",
    "\n",
    "\n",
    "X = all_filtered_data.Bayt_Text.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, MAX_LEN_BAYT))\n",
    "print(\"full_cleaned_data_with_tashkeel_encoded encoded ..\")\n",
    "\n",
    "X_stacked = np.stack(X,axis=0)\n",
    "print(\"stacked!\")\n",
    "\n",
    "save_h5('../../data/Encoded/8bits/WithTashkeel/Full_Data/full_data_matrix_with_tashkeel_8bitsEncoding.h5', 'X', X_stacked) \n",
    "print(\"full_cleaned_data_with_tashkeel_encoded saved ..\")\n",
    "print(X_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminated_cleaned_data_with_tashkeel_encoded encoded ..\n",
      "full_cleaned_data_with_tashkeel_encoded saved ..\n",
      "(1670144, 111, 8)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * Encoding the eleiminated data with Tashkeel\n",
    "'''\n",
    "\n",
    "X = None\n",
    "X_stacked = None\n",
    "\n",
    "X = eliminated_filtered_data.Bayt_Text.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, MAX_LEN_BAYT))\n",
    "print(\"eliminated_cleaned_data_with_tashkeel_encoded encoded ..\")\n",
    "\n",
    "X_stacked = np.stack(X,axis=0)\n",
    "save_h5('../../data/Encoded/8bits/WithTashkeel/Eliminated/eliminated_data_matrix_with_tashkeel_8bitsEncoding.h5', 'X', X_stacked)\n",
    "\n",
    "print(\"full_cleaned_data_with_tashkeel_encoded saved ..\")\n",
    "print(X_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    خليليي لا تستعجلا أن تزوودا وأن تجمعا شملي وتن...\n",
       "1    فما لبثن يومن بسابقن مغنمن ولا سرعتي يومن بساب...\n",
       "2    وإن تنظراني اليوم أقض لبانتن وتستوجبا مننن علي...\n",
       "3    لعمرك ما نفسن بجدن رشيدةتن تؤامرني سررن لأصرم ...\n",
       "4    وإن ظهرت منه قوارص جممتن وأفرع في لومي مرارن و...\n",
       "Name: Bayt_Text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = None\n",
    "X_stacked = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping Taskeel\n",
    "all_data_cleaned.Bayt_Text = all_data_cleaned.Bayt_Text.apply(pyarabic.araby.strip_tashkeel)\n",
    "all_filtered_data = all_data_cleaned.loc[all_data_cleaned['Bahr'].isin(full_bohor_classes)]\n",
    "eliminated_filtered_data = all_data_cleaned.loc[all_data_cleaned['Bahr'].isin(elminated_classic_bohor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    * Encoding all data without Tashkeel\n",
    "'''\n",
    "\n",
    "X = None\n",
    "X_stacked = None\n",
    "\n",
    "X = all_filtered_data.Bayt_Text.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, MAX_LEN_BAYT))\n",
    "print(\"full_filtered_data_without_tashkeel encoded ..\")\n",
    "\n",
    "X_stacked = np.stack(X, axis=0)\n",
    "save_h5('../../data/Encoded/8bits/WithoutTashkeel/Full_Data/full_data_matrix_without_tashkeel_8bitsEncoding.h5', 'X', X_stacked) \n",
    "print(\"full_filtered_data_without_tashkeel_staked saved ..\")\n",
    "\n",
    "print(X_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eliminated_filtered_data_without_tashkeel encoded ..\n",
      "eliminated_filtered_data_without_tashkeel_staked saved ..\n",
      "(1670144, 111, 8)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    * Encoding elminated data without Tashkeel\n",
    "'''\n",
    "\n",
    "X = None\n",
    "X_stacked = None\n",
    "\n",
    "X = eliminated_filtered_data.Bayt_Text.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, MAX_LEN_BAYT))\n",
    "print(\"eliminated_filtered_data_without_tashkeel encoded ..\")\n",
    "\n",
    "X_stacked = np.stack(X, axis=0)\n",
    "save_h5('../../data/Encoded/8bits/WithoutTashkeel/Eliminated/eliminated_data_matrix_without_tashkeel_8bitsEncoding.h5', 'X', X_stacked) \n",
    "print(\"eliminated_filtered_data_without_tashkeel_staked saved ..\")\n",
    "\n",
    "print(X_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

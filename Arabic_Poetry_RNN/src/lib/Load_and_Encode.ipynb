{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arabic\n",
    "import pyarabic\n",
    "import helpers\n",
    "from helpers import Clean_data,separate_token_with_dicrites,save_h5\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from numpy import array, argmax\n",
    "import os,errno,re,sys\n",
    "import pickle\n",
    "from itertools import product \n",
    "from pyarabic.araby import strip_tashkeel, strip_tatweel\n",
    "import h5py\n",
    "import random as rn\n",
    "##########################\n",
    "rn.seed(123456)\n",
    "np.random.seed(123456)\n",
    "input_data_path = \"../data/Almoso3a_Alshe3rya/data/All_ksaied.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(\"../data/Encoded/8bits/WithoutTashkeel/Eliminated/\")\n",
    "    os.makedirs(\"../data/Encoded/8bits/WithoutTashkeel/Full_Data/\")\n",
    "    os.makedirs(\"../data/Encoded/8bits/WithTashkeel/Eliminated/\")\n",
    "    os.makedirs(\"../data/Encoded/8bits/WithTashkeel/Full_Data/\")\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        print(\"Can't create file for checkpoints or for logs please check \")\n",
    "        raise\n",
    "print(\"Paths created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_csv(input_data_path, index_col=0)\n",
    "len(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample = input_data.sample(1000)\n",
    "#sample\n",
    "len(input_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#input_data = pd.read_csv(input_data_path, index_col=0)\n",
    "\n",
    "# TAHA\n",
    "#input_data = sample\n",
    "\n",
    "input_data.info(memory_usage='deep')\n",
    "bahr = 'Bahr'\n",
    "bayt = 'Bayt_Text'\n",
    "#input_data  = input_data[[bayt, bahr]]\n",
    "bahr = 'البحر'\n",
    "bayt = 'البيت'\n",
    "input_data  = input_data[[bayt, bahr]]\n",
    "input_data.columns = ['Bayt_Text','Bahr']\n",
    "our_alphabets = \"\".join(arabic.alphabet) + \"\".join(arabic.tashkeel)+\" \"\n",
    "our_alphabets = \"\".join(our_alphabets)\n",
    "input_data['Bayt_Text'] = input_data['Bayt_Text'].apply(lambda x: re.sub(r'[^'+our_alphabets+']','',str(x))).apply(lambda x: re.sub(r'  *',\" \",x)).apply(lambda x: re.sub(r'ّ+', 'ّ', x)).apply(lambda x: x.strip())\n",
    "#input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taha: first class of the first observation\n",
    "row_first_observation_class = input_data.iloc[0][1]\n",
    "row_first_observation_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taha: Counts the opservations of each class\n",
    "input_data.iloc[:, 1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Filteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elminated_classic_bohor = ['الوافر', 'المنسرح','المجتث', 'المتقارب', 'الكامل', 'الطويل', 'السريع', 'الرمل',\n",
    "                         'الرجز', 'الخفيف', 'البسيط']\n",
    "elminated_classic_bohor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_bohor_classes = ['الوافر', 'المنسرح', 'المديد', 'المجتث', 'المتقارب', 'الكامل', 'الطويل', 'السريع', 'الرمل',\n",
    "                         'الرجز', 'الخفيف', 'البسيط', 'المقتضب', 'الهزج', 'المضارع', 'المتدارك']\n",
    "full_bohor_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_filtered_data = input_data.loc[input_data['Bahr'].isin(full_bohor_classes)]\n",
    "full_filtered_data['Bayt_Text'] = full_filtered_data['Bayt_Text'].apply(pyarabic.araby.strip_tatweel)\n",
    "full_filtered_data['Bayt_Text'] = full_filtered_data['Bayt_Text'].apply(helpers.factor_shadda_tanwin)\n",
    "#full_filtered_data.head()\n",
    "# Taha\n",
    "full_filtered_data.iloc[:, 1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminated_filtered_data = input_data.loc[input_data['Bahr'].isin(elminated_classic_bohor)]\n",
    "eliminated_filtered_data['Bayt_Text'] = eliminated_filtered_data['Bayt_Text'].apply(pyarabic.araby.strip_tatweel)\n",
    "eliminated_filtered_data['Bayt_Text'] = eliminated_filtered_data['Bayt_Text'].apply(helpers.factor_shadda_tanwin)\n",
    "# eliminated_filtered_data.head()\n",
    "\n",
    "# Taha\n",
    "eliminated_filtered_data.iloc[:, 1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminated_filtered_data.to_csv('Big_eliminated_filtered_data.csv')\n",
    "#full_filtered_data.to_csv('Big_full_filtered_data.csv')\n",
    "#eliminated_filtered_data = pd.read_csv('Big_eliminated_filtered_data.csv')\n",
    "#full_filtered_data = pd.read_csv('Big_full_filtered_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Taha; Encoding Full Data classes as OneHot vectors. \n",
    "'''\n",
    "\n",
    "full_data_label_encoder = LabelEncoder()\n",
    "full_data_integer_encoded = full_data_label_encoder.fit_transform(list(full_filtered_data[\"Bahr\"]))\n",
    "# binary encode\n",
    "full_data_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "full_data_integer_encoded = full_data_integer_encoded.reshape(len(full_data_integer_encoded), 1)\n",
    "full_data_bohor_encoded = full_data_onehot_encoder.fit_transform(full_data_integer_encoded)\n",
    "save_h5('../data/Encoded/8bits/WithoutTashkeel/Full_Data/full_data_Y_Meters.h5', 'Y', full_data_bohor_encoded)\n",
    "save_h5('../data/Encoded/8bits/WithTashkeel/Full_Data/full_data_Y_Meters.h5', 'Y', full_data_bohor_encoded)\n",
    "#saving with pickle: https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "print('saving preprocessors....')\n",
    "with open(\"../encoders_full_dat.pickle\", 'wb') as pre_file:\n",
    "    pickle.dump(full_data_label_encoder, pre_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('saved ....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing\n",
    "#print('loading saved encoders from %s'%(encoders_file_name))\n",
    "with open(\"../encoders_full_dat.pickle\", 'rb') as f:\n",
    "    encoders = pickle.load(f)\n",
    "\n",
    "##testing \n",
    "#inverted = encoders.inverse_transform([argmax(full_filtered_data[\"Bahr\"][1,])])\n",
    "\n",
    "\n",
    "# Taha\n",
    "Ys = h5py.File('../data/Encoded/8bits/WithoutTashkeel/Full_Data/full_data_Y_Meters.h5', 'r')\n",
    "class_of_first_observation = Ys['Y'][0]\n",
    "decoded_first_class = encoders.inverse_transform([argmax( class_of_first_observation   )])\n",
    "\n",
    "if decoded_first_class == row_first_observation_class:\n",
    "    print('NICE, Classes Encoded properly')\n",
    "else:\n",
    "    print('There is a problem in encoding-decoding classes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Taha; This cell is entirly doublicated.\n",
    "    It Encodes the eliminated data classes.\n",
    "'''\n",
    "eliminated_data_label_encoder = LabelEncoder()\n",
    "eliminated_data_integer_encoded = eliminated_data_label_encoder.fit_transform(list(eliminated_filtered_data[\"Bahr\"]))\n",
    "# binary encode\n",
    "eliminated_data_onehot_encoder = OneHotEncoder(sparse=False)\n",
    "eliminated_data_integer_encoded = eliminated_data_integer_encoded.reshape(len(eliminated_data_integer_encoded), 1)\n",
    "eliminated_data_bohor_encoded = eliminated_data_onehot_encoder.fit_transform(eliminated_data_integer_encoded)\n",
    "save_h5('../data/Encoded/8bits/WithoutTashkeel/Eliminated/Eliminated_data_Y_Meters.h5', 'Y', eliminated_data_bohor_encoded)\n",
    "save_h5('../data/Encoded/8bits/WithTashkeel/Eliminated/Eliminated_data_Y_Meters.h5', 'Y', eliminated_data_bohor_encoded)\n",
    "#saving with pickle: https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "print('saving preprocessors....')\n",
    "with open(\"../encoders_eliminated_data.pickle\", 'wb') as pre_file:\n",
    "    pickle.dump(eliminated_data_bohor_encoded, pre_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('saved ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WITH TASHKEEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FULL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayt_len_full_withtashkeel = np.max((full_filtered_data.Bayt_Text.apply(separate_token_with_dicrites).apply(len)))\n",
    "print(\"bayt_len_full_withtashkeel done..\")\n",
    "full_filtered_data_with_tashkeel_cleaned = Clean_data(data_frame = full_filtered_data,max_bayt_len = bayt_len_full_withtashkeel,verse_column_name = 'Bayt_Text')\n",
    "print(\"full_cleaned_data_with_tashkeel cleaned ..\")\n",
    "full_cleaned_data_with_tashkeel_encoded = full_filtered_data_with_tashkeel_cleaned.Bayt_Text.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, bayt_len_full_withtashkeel))\n",
    "print(\"full_cleaned_data_with_tashkeel_encoded encoded ..\")\n",
    "full_cleaned_data_with_tashkeel_encoded_stacked = np.stack(full_cleaned_data_with_tashkeel_encoded,axis=0)\n",
    "# save('../data/Encoded/8bits/WithTashkeel/Full_Data/full_data_matrix_with_tashkeel_8bitsEncoding.h5', 'X', full_cleaned_data_with_tashkeel_encoded_stacked) \n",
    "# Taha\n",
    "save_h5('../data/Encoded/8bits/WithTashkeel/Full_Data/full_data_matrix_with_tashkeel_8bitsEncoding.h5', 'X', full_cleaned_data_with_tashkeel_encoded_stacked) \n",
    "print(\"full_cleaned_data_with_tashkeel_encoded saved ..\")\n",
    "print(full_cleaned_data_with_tashkeel_encoded_stacked.shape)\n",
    "\n",
    "#### ELIMINATED DATA\n",
    "\n",
    "bayt_len_eliminated_withtashkeel = np.max((eliminated_filtered_data.Bayt_Text.apply(separate_token_with_dicrites).apply(len)))\n",
    "print(\"bayt_len_eliminated_withtashkeel done..\")\n",
    "eliminated_filtered_data_with_tashkeel_cleaned = Clean_data(data_frame = eliminated_filtered_data,max_bayt_len = bayt_len_eliminated_withtashkeel  ,verse_column_name = 'Bayt_Text')\n",
    "print(\"eliminated_cleaned_data_with_tashkeel cleaned ..\")\n",
    "eliminated_cleaned_data_with_tashkeel_encoded = eliminated_filtered_data_with_tashkeel_cleaned.Bayt_Text.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, bayt_len_eliminated_withtashkeel))\n",
    "print(\"eliminated_cleaned_data_with_tashkeel_encoded encoded ..\")\n",
    "eliminated_cleaned_data_with_tashkeel_encoded_stacked = np.stack(eliminated_cleaned_data_with_tashkeel_encoded,axis=0)\n",
    "save_h5('../data/Encoded/8bits/WithTashkeel/Eliminated/eliminated_data_matrix_with_tashkeel_8bitsEncoding.h5', 'X', eliminated_cleaned_data_with_tashkeel_encoded_stacked) \n",
    "print(\"full_cleaned_data_with_tashkeel_encoded saved ..\")\n",
    "print(eliminated_cleaned_data_with_tashkeel_encoded_stacked.shape)\n",
    "#encoded_byot = data_with_Tashkeel_cleaned.apply(lambda x: helpers.string_with_tashkeel_vectorizer_OneHot(x, maximum_bayt_len))\n",
    "\n",
    "## WITHOUT TASHKEEL\n",
    "\n",
    "### FULL DATA\n",
    "\n",
    "# RUN HERE\n",
    "full_filtered_data_without_tashkeel = full_filtered_data['Bayt_Text'].apply(pyarabic.araby.strip_tashkeel)\n",
    "bayt_len_full_without_tashkeel = np.max((full_filtered_data_without_tashkeel.apply(len)))\n",
    "print(\"bayt_len_full_without_tashkeel done..\")\n",
    "full_cleaned_data_without_tashkeel_encoded = full_filtered_data_without_tashkeel.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, bayt_len_full_without_tashkeel))\n",
    "print(\"full_filtered_data_without_tashkeel encoded ..\")\n",
    "full_filtered_data_without_tashkeel_staked = np.stack(full_cleaned_data_without_tashkeel_encoded,axis=0)\n",
    "save_h5('../data/Encoded/8bits/WithoutTashkeel/Full_Data/full_data_matrix_without_tashkeel_8bitsEncoding.h5', 'X', full_filtered_data_without_tashkeel_staked) \n",
    "print(\"full_filtered_data_without_tashkeel_staked saved ..\")\n",
    "print(full_filtered_data_without_tashkeel_staked.shape)\n",
    "\n",
    "### ELIMINATE\n",
    "\n",
    "eliminated_filtered_data_without_tashkeel = eliminated_filtered_data['Bayt_Text'].apply(pyarabic.araby.strip_tashkeel)\n",
    "bayt_len_eliminated_without_tashkeel = np.max((eliminated_filtered_data_without_tashkeel.apply(len)))\n",
    "print(\"bayt_len_eliminated_without_tashkeel done..\")\n",
    "eliminated_cleaned_data_without_tashkeel_encoded = eliminated_filtered_data_without_tashkeel.apply(lambda x: helpers.string_with_tashkeel_vectorizer(x, bayt_len_eliminated_without_tashkeel))\n",
    "print(\"eliminated_filtered_data_without_tashkeel encoded ..\")\n",
    "eliminated_filtered_data_without_tashkeel_staked = np.stack(eliminated_cleaned_data_without_tashkeel_encoded,axis=0)\n",
    "save_h5('../data/Encoded/8bits/WithoutTashkeel/Eliminated/eliminated_data_matrix_without_tashkeel_8bitsEncoding.h5', 'X', full_filtered_data_without_tashkeel_staked) \n",
    "print(\"eliminated_filtered_data_without_tashkeel_staked saved ..\")\n",
    "\n",
    "print(eliminated_filtered_data_without_tashkeel_staked.shape)\n",
    "\n",
    "\n",
    "## Congratualtions :) Finished :) :) :)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
